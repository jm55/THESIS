{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def loadDF():\n",
    "    df = pd.read_csv(\"oliveira.csv\")\n",
    "    return df[df['malware'] == 1]\n",
    "\n",
    "def getX(df):\n",
    "    return df.iloc[:, 1:102-1]\n",
    "\n",
    "#Load list of API calls\n",
    "API_LIST = \"api_calls.txt\"\n",
    "DELIMITER = \"NaN\"\n",
    "API_FILE = open(API_LIST,\"r\")\n",
    "APIS = API_FILE.readline().split(',')\n",
    "APIS.append(DELIMITER) #serves as a label for NaN values for Instance-based datasets\n",
    "API_FILE.close()\n",
    "\n",
    "#Inverse Label Encoding\n",
    "def inverse_labeller(item):\n",
    "    global APIS\n",
    "    return item.map(lambda x: APIS[int(x)])\n",
    "def inverse_label(df):\n",
    "    df.iloc[:, 1:df.shape[1]-2] = df.iloc[:, 1:df.shape[1]-2].apply(inverse_labeller, axis=1, result_type='reduce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: https://medium.com/swlh/k-means-clustering-on-high-dimensional-data-d2151e1a4240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchK(parameters, model, X):\n",
    "    # instantiating ParameterGrid, pass number of clusters as input\n",
    "    parameter_grid = ParameterGrid({'n_clusters': parameters})\n",
    "    best_score = -1\n",
    "    best_grid = -1\n",
    "    silhouette_scores = []\n",
    "    # evaluation based on silhouette_score\n",
    "    for p in parameter_grid:\n",
    "        model.set_params(**p)    # set current hyper parameter\n",
    "        model.fit(X)          # fit model on wine dataset, this will find clusters based on parameter p\n",
    "        ss = metrics.silhouette_score(X, model.labels_)   # calculate silhouette_score\n",
    "        silhouette_scores += [ss]       # store all the scores\n",
    "        print('Parameter:', p, 'Score', ss)\n",
    "        # check p which has the best score\n",
    "        if ss > best_score:\n",
    "            best_score = ss\n",
    "            best_grid = p\n",
    "    # plotting silhouette score\n",
    "    plt.bar(range(len(silhouette_scores)), list(silhouette_scores), align='center', color='#722f59', width=0.5)\n",
    "    plt.xticks(range(len(silhouette_scores)), list(parameters))\n",
    "    plt.title('Silhouette Score', fontweight='bold')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.show()\n",
    "    return best_grid['n_clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample hashes for VirusTotal Analysis\n",
    "# For each cluster, it will pick a random sample\n",
    "def getSampleHashes(df, name, clusters, size):\n",
    "    sampleHashes = []\n",
    "    for i in range(0, size):\n",
    "        clusterSize = df[df['cluster']==i].shape[0]\n",
    "        sampleHashes.append([i,df[df['cluster']==i].iloc[random.randint(0,clusterSize-1)][0]])\n",
    "    pd.DataFrame(sampleHashes, columns =['cluster', 'hash']).to_csv(f\"sampleHashes_{name}_{size}.csv\", index=False)\n",
    "    return sampleHashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Microsoft recognizes [15](https://www.microsoft.com/en-us/security/business/security-101/what-is-malware#types-of-malware\n",
    ") common types malwares. This information can be used as a reference for the max size of clusters to be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = loadDF()\n",
    "X = getX(df)\n",
    "\n",
    "clusters = [10,20,30,40,50,60,70,80,90,100] #Let's assume that there are up to 100 clusters that can be derived from the Oliveira dataset.\n",
    "bestClusterSize = searchK(clusters, KMeans(init='k-means++', n_init='auto', verbose=0, random_state=1), X)\n",
    "kmeans = KMeans(n_clusters=bestClusterSize, init='k-means++', n_init='auto', verbose=0, random_state=1)\n",
    "kmeans.fit(X)\n",
    "df['cluster'] = kmeans.predict(X)\n",
    "df = inverse_label(df)\n",
    "df.to_csv(f\"Cluster_KMeans_{bestClusterSize:.0f}.csv\", index=False)\n",
    "getSampleHashes(df, \"KMeans\", clusters, bestClusterSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AgglomerativeClustering is highly unstable and requires a lot of memory for the input dataset. Hence, while ideal, it is not feasible on the current hardware.\n",
    "# df = loadDF()\n",
    "# X = getX(df)\n",
    "\n",
    "# clusters = 13\n",
    "# bestClusterSize = searchK(clusters, AgglomerativeClustering(compute_full_tree='auto', linkage='ward'), X)\n",
    "# ac = AgglomerativeClustering(n_clusters=bestClusterSize, compute_full_tree='auto', linkage='ward')\n",
    "# df['cluster'] = ac.fit_predict(X)\n",
    "# df = inverse_label(df)\n",
    "# df.to_csv(f\"Cluster_AgglomerativeClustering_{bestClusterSize:.0f}.csv\", index=False)\n",
    "# getSampleHashes(df, \"AgglomerativeClustering\", clusters, bestClusterSize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
