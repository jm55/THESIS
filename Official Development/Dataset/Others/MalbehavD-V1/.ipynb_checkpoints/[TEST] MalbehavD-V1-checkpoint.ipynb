{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d753f09",
   "metadata": {},
   "source": [
    "# Evaluation and Comparison of Boosted ML Models in Behavior-Based Malware Detection\n",
    "\n",
    "\n",
    "## Notebook: LightGBM Tuning\n",
    "\n",
    "***\n",
    "\n",
    "**What is the objective of this file?**\n",
    "\n",
    "To process the datasets to make it suitable for use in Model Training and Model Evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93914a5",
   "metadata": {},
   "source": [
    "## Checklist\n",
    "\n",
    "- Ensure that you have installed the necessary libraries needed to execute the training process. \n",
    "- You can view the list of the specific versions in the thesis document or through the `.sh` or `.bat` files in the repository's home directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6a6ab6",
   "metadata": {},
   "source": [
    "# 0. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python Libraries\n",
    "import time, threading, math\n",
    "from datetime import datetime\n",
    "\n",
    "#Data/Dataset libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Split Sampler/Data Splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Oversampler\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "\n",
    "#Label Encoding\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#Timer\n",
    "start = end = 0\n",
    "LOG_FILENAME = \"MalbehavD_Log.txt\"\n",
    "def logging(message):\n",
    "    log = open(LOG_FILENAME, \"a\")\n",
    "    log.write(message)\n",
    "    log.close()\n",
    "def start_time():\n",
    "    global start\n",
    "    start = time.time()\n",
    "def end_time(process):\n",
    "    global start\n",
    "    global DATASET_FILENAME\n",
    "    elapse = time.time()-start\n",
    "    start = 0\n",
    "    printout = f\"{str(datetime.now())}@{DATASET_FILENAME}: {process} - {round(elapse, 6)}s\\n\"\n",
    "    logging(printout)\n",
    "    return round(elapse, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb085e1f",
   "metadata": {},
   "source": [
    "# 1. Import Datasets\n",
    "\n",
    "**Notice:** \n",
    "1. Make sure check the value of line 3 of the block below before running.\n",
    "2. A backup of the processed datasets is already found in `/Processed Datasets` folder. Simply unzip it to replace the contents of `/Processed Datasets/IB` and `/Processed Datasets/TB` folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caad9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filenames\n",
    "filename = [\"malbd.csv\"]\n",
    "DATASET_FILENAME = filename[0] # <== CHANGE THIS ACCORDINGLY\n",
    "API_LIST = \"../../api_calls.txt\"\n",
    "\n",
    "#Load list of API calls\n",
    "DELIMITER = \"NaN\"\n",
    "API_FILE = open(API_LIST,\"r\")\n",
    "APIS = API_FILE.readline().split(',')\n",
    "APIS.append(DELIMITER) #serves as a label for NaN values for Instance-based datasets\n",
    "API_FILE.close()\n",
    "\n",
    "#Importing Datasets\n",
    "oli = pd.read_csv(DATASET_FILENAME)\n",
    "print(\"Oliveira Info:\")\n",
    "oli.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53862f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "APIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fcb67c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Previewing Dataset\n",
    "oli.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29934c19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Oliveira Label Counts\")\n",
    "print(\"0 as Benign, 1 as Malicious\")\n",
    "oli.malware.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62c1fa2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Oliveira Unique API calls list\")\n",
    "oli_unique = pd.Series(oli[list(oli.columns.values)[1:101]].values.ravel())\n",
    "oli_unique.sort_values(inplace=True, ascending=False)\n",
    "oli_unique.value_counts()[0:19].plot(kind='barh', figsize=(7,3.5), title='Top 20 API calls in \\'Oliveira\\'') #Top 20 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6079b877",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "oli_unique.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ccd5fc",
   "metadata": {},
   "source": [
    "# 2. Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428c483c",
   "metadata": {},
   "source": [
    "## 2.1. Dataset Cleaning and Dataset Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cecee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove falsely labelled malicious samples\n",
    "oli = oli[oli['type'] != '_']\n",
    "\n",
    "# Remove specific malware types\n",
    "# removables = ['ransomware', 'miner', 'virus', 'spyware', 'hacktool', 'dropper', 'worm']\n",
    "# for r in removables:\n",
    "#     oli = oli[oli['type'] != r]\n",
    "\n",
    "#Remove type column\n",
    "type_col = oli.pop('type')\n",
    "\n",
    "#Removing hash column\n",
    "hash_col = oli.pop('hash')\n",
    "\n",
    "#Re-arranging column positions\n",
    "label_col = oli.pop('malware')\n",
    "oli = pd.concat([label_col, oli], axis=1)\n",
    "oli = pd.concat([oli, hash_col], axis=1) # <=== This will be retained for the benefit of model evaluation.\n",
    "oli = pd.concat([oli, type_col], axis=1) # <=== This will be retained for the benefit of model evaluation.\n",
    "\n",
    "oli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a957af2",
   "metadata": {},
   "source": [
    "## 2.2. Inverse Data Encoding\n",
    "\n",
    "*Encoded (Ordinal) API calls to String API Calls*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4bc529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(api:str):\n",
    "    return APIS.index(api)\n",
    "\n",
    "for j in range(1,101):\n",
    "    oli.iloc[:,j] = pd.Series(list(map(convert, oli.iloc[:,j].to_list())))\n",
    "\n",
    "oli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b770dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse Label Encoding\n",
    "def inverse_label(item):\n",
    "    global APIS\n",
    "    return item.map(lambda x: APIS[int(x)])\n",
    "start_time()\n",
    "oli.iloc[:, 1:101] = oli.iloc[:, 1:101].apply(inverse_label, axis=1, result_type='reduce')\n",
    "end_time(\"Inverse Data Encode\")\n",
    "\n",
    "oli.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba655c15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Oliveira Unique API calls list\")\n",
    "oli_unique = pd.Series(oli[list(oli.columns.values)[1:101]].values.ravel())\n",
    "oli_unique.sort_values(inplace=True, ascending=False)\n",
    "oli_unique.value_counts()[0:19].plot(kind='barh', figsize=(7,3.5), title='Top 20 API calls in \\'Oliveira\\'') #Top 20 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85567355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oli_unique.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70006c59",
   "metadata": {},
   "source": [
    "## 2.3. Feature Duplicate Processing\n",
    "\n",
    "*Building Time-based and Instance-Based Datasets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b41f792",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TB = oli.copy(deep=True) #Time-based behavior (same as original)\n",
    "IB = oli.copy(deep=True) #Instance-based behavior (to be created)\n",
    "\n",
    "start_time()\n",
    "print(\"Transposing IB...\")\n",
    "IB.transpose()\n",
    "print(\"IB Transposed!\")\n",
    "print(\"Removing duplicates...\")\n",
    "print(\"Row:\", end=\" \")\n",
    "for r in range(oli.shape[0]):\n",
    "    #Per row (sample) removal of duplicates, thus cannot scale into the whole dataframe (which is way faster)\n",
    "    row = IB.iloc[r, 1:101].drop_duplicates(keep='first', inplace=False).to_list()\n",
    "    IB.iloc[r, 1:101] = row + ([DELIMITER]*(100-len(row)))\n",
    "    if r % 100 == 0:\n",
    "        print(r, end=\" \")\n",
    "print(\"\\nDuplicates removed!\")\n",
    "print(\"Retransposing IB (revert)...\")\n",
    "IB.transpose()\n",
    "print(\"IB Retransposed!\")\n",
    "end_time(\"Feature Duplicate Process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dcd895",
   "metadata": {},
   "outputs": [],
   "source": [
    "IB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d74599",
   "metadata": {},
   "source": [
    "# 5. (Manual) Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db45fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert(api:str):\n",
    "    return APIS.index(api)\n",
    "def ordinal_encode(offset):\n",
    "    global APIS\n",
    "    global ENCODED\n",
    "    for j in range(1,101):\n",
    "        ENCODED[offset].iloc[:,j] = pd.Series(list(map(convert, ENCODED[offset].iloc[:,j].to_list())))\n",
    "\n",
    "ENCODED = [TB.copy(deep=True), IB.copy(deep=True)]\n",
    "\n",
    "tb_thread = threading.Thread(target=ordinal_encode, args=(0,))\n",
    "ib_thread = threading.Thread(target=ordinal_encode, args=(1,))\n",
    "\n",
    "start_time()\n",
    "tb_thread.start()\n",
    "ib_thread.start()\n",
    "tb_thread.join()\n",
    "ib_thread.join()\n",
    "end_time(\"Label Encode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ea5b6f",
   "metadata": {},
   "source": [
    "# 6. Saving to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a260453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(dfs, filenames):\n",
    "    for d in range(len(dfs)):\n",
    "        dfs[d].to_csv(index=False, chunksize=100, mode='w', path_or_buf=filenames[d])\n",
    "\n",
    "STR_FILENAMES = ['TB/M-CATB_TB.csv', 'IB/M-CATB_IB.csv', 'TB/M-CATB_TB_Test.csv', 'IB/M-CATB_IB_Test.csv']\n",
    "STR_API = [TB.copy(deep=True), IB.copy(deep=True)]\n",
    "\n",
    "ENC_FILENAMES = ['TB/M-LGBM_TB.csv', 'IB/M-LGBM_IB.csv', 'TB/M-LGBM_TB_Test.csv', 'IB/M-LGBM_IB_Test.csv']\n",
    "ENC_API = ENCODED\n",
    "\n",
    "start_time()\n",
    "save_to_csv(STR_API, STR_FILENAMES)\n",
    "save_to_csv(ENC_API, ENC_FILENAMES)\n",
    "end_time('save_to_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging(\"\\n\") #Adds a spacer for next instance of logs for this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
