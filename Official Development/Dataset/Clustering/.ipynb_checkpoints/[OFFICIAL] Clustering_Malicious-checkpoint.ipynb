{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Demo (Malicious)\n",
    "\n",
    "**Note:** Make sure that you have `oliveira.csv` in the same directory as this notebook.\n",
    "\n",
    "**Clustering Methods not Supported:** GaussianMixture & HDBScan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Preparation\n",
    "\n",
    "Prepare the functions that might be used as the notebook runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ejose\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans,  BisectingKMeans, MiniBatchKMeans, Birch, DBSCAN\n",
    "# from sklearn.cluster import SpectralClustering, AgglomerativeClustering, OPTICS, MeanShift # Not working\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Load list of API calls\n",
    "API_LIST = \"../api_calls.txt\"\n",
    "DELIMITER = \"NaN\"\n",
    "API_FILE = open(API_LIST,\"r\")\n",
    "APIS = API_FILE.readline().split(',')\n",
    "APIS.append(DELIMITER) #Add the label for NaN values.\n",
    "API_FILE.close()\n",
    "\n",
    "#Random Seed\n",
    "seed = 1\n",
    "\n",
    "def list_to_str(ls:list):\n",
    "    '''Convert list to a stringified version (comma delimited).'''\n",
    "    output = \"\"\n",
    "    for l in ls:\n",
    "        output += str(l) + \",\"\n",
    "    return output[0:len(output)-1]\n",
    "\n",
    "def load_df():\n",
    "    '''Load the dataset file (CSV) as DataFrame'''\n",
    "    print(\"Loading DF...\")\n",
    "    df = pd.read_csv(\"../oliveira_labelled.csv\", low_memory=False, memory_map=True) # MAKE SURE THIS IS SET AS `oliveira.csv`\n",
    "    df = df[df['malware'] == 1].copy()\n",
    "    df = df.drop('malware', axis=1)\n",
    "    df = df.drop('type', axis=1)\n",
    "    print(\"\")\n",
    "    return df.reset_index().iloc[:,1:]\n",
    "\n",
    "def get_x(df:pd.DataFrame):\n",
    "    '''Get the feature columns of the DataFrame'''\n",
    "    return df.iloc[:, 1:102-1]\n",
    "\n",
    "#Inverse Label Encoding\n",
    "def inverse_labeller(item):\n",
    "    '''Low Level. Converts encoded API calls to string API calls'''\n",
    "    global APIS\n",
    "    return item.map(lambda x: APIS[int(x)])\n",
    "def inverse_label(df:pd.DataFrame):\n",
    "    '''High Level. Converts encoded API calls to string API calls'''\n",
    "    df2 = df.copy(deep=True)\n",
    "    print(\"Inverse Labelling...\")\n",
    "    df2.iloc[:, 1:101] = df2.iloc[:, 1:101].apply(inverse_labeller, axis=1, result_type='reduce')\n",
    "    print(\"\")\n",
    "    return df2\n",
    "\n",
    "def search_k(parameters, model, X):\n",
    "    '''Search for the best parameter(s) for the model (usually cluster size or K value)'''\n",
    "    paramGrid = ParameterGrid(parameters)\n",
    "    best_score = -1\n",
    "    best_grid = -1\n",
    "    best_clusterer = None\n",
    "    # evaluation based on silhouette_score\n",
    "    print(\"Searching Best Clustering Parameters...\")\n",
    "    for p in paramGrid:\n",
    "        model.set_params(**p)    # set current hyper parameter\n",
    "        start_time = time.time()\n",
    "        model.fit(X)          # fit model on wine dataset, this will find clusters based on parameter p\n",
    "        ss = metrics.silhouette_score(X, model.labels_, random_state=seed)   # calculate silhouette_score\n",
    "        # silhouette_scores.append([p, ss]) # store all the scores\n",
    "        print('Parameter:', p, 'Score', f\"{ss:.4f}\", 'Unique_Labels', len(pd.Series(model.labels_).unique()), \"Time\", f\"{time.time()-start_time:.4f}\")\n",
    "        # check p which has the best score\n",
    "        if ss > best_score:\n",
    "            best_score = ss\n",
    "            best_grid = p\n",
    "            best_clusterer = model\n",
    "    print(\"\")\n",
    "    print(\"BEST PARAM SETUP: \", best_grid, best_score)\n",
    "    print(\"\")\n",
    "    return best_grid, best_clusterer\n",
    "\n",
    "def clustering(inner_df:pd.DataFrame, name:str, clusterer, parameters):\n",
    "    '''Executes the data clustering on the dataset. Produces the same input dataset with the additional column for the cluster #.'''\n",
    "    '''The input dataset must contain integer API calls (except the API Call Pattern strings)'''\n",
    "    X = get_x(inner_df)\n",
    "    bestCluster, bestClusterer = search_k(parameters, clusterer, X) #assumes bestCluster already fitted\n",
    "    #bestClusterer.fit(X)\n",
    "    inner_df['cluster'] = bestClusterer.labels_\n",
    "    inner_df.to_csv(f\"Clustering/Malicious/{name}_Encoded_Clustering.csv\", index=False)\n",
    "    inner_df = inverse_label(inner_df)\n",
    "    inner_df.to_csv(f\"Clustering/Malicious/{name}_Clustering.csv\", index=False)\n",
    "    print(\"\")\n",
    "    return bestCluster\n",
    "\n",
    "def common_api_cluster(inner_df:pd.DataFrame, name:str):\n",
    "    '''Determine the most common API call patterns for each cluster'''\n",
    "    global df\n",
    "    inner_df = df\n",
    "    clusters = inner_df['cluster'].unique()\n",
    "    clusters.sort()\n",
    "    commonAPI = []\n",
    "    print(\"Searching for Common API Patterns per Cluster...\")\n",
    "    print(clusters)\n",
    "    for cluster in clusters:\n",
    "        raw_commonC = inner_df[inner_df['cluster']==cluster]['pattern']#.value_counts()\n",
    "        commonC = raw_commonC.value_counts().to_frame(name='counts').reset_index()\n",
    "        commonAPI.append([cluster, commonC['counts'].iloc[0], round(commonC['counts'].iloc[0]/raw_commonC.shape[0],4), commonC['pattern'].iloc[0]])\n",
    "    commonAPI = pd.DataFrame(commonAPI, columns=['cluster', 'count', 'match_ratio', 'pattern'])\n",
    "    commonAPI.to_csv(f\"Clustering/Malicious/{name}_Common_API_Cluster.csv\", index=False)\n",
    "    print(\"\")\n",
    "    print(\"Average Match Ratio:\", commonAPI['match_ratio'].mean())\n",
    "    return commonAPI\n",
    "\n",
    "def get_samplehash_common(inner_df:pd.DataFrame, common_counts:pd.DataFrame, name:str, samplesize:int):\n",
    "    '''Get sample hashes from each cluster that matches the common API call pattern of the cluster.'''\n",
    "    hashes = []\n",
    "    for i in range(inner_df.shape[0]):\n",
    "        hashes.append([inner_df.iloc[i,:]['cluster'], inner_df.iloc[i,:]['hash'], '_', inner_df.iloc[i,:]['pattern']])\n",
    "    hashes = pd.DataFrame(hashes, columns=['cluster', 'hash', 'Type 1', 'pattern'])\n",
    "    hashes['Type 1'] = inner_df['type']\n",
    "    hashes.to_csv(f\"Clustering/Malicious/Manual_{name}_SampleHash_Common.csv\", index=False)\n",
    "    print(\"\")\n",
    "    return hashes\n",
    "    \n",
    "def inject_patterns(inner_df:pd.DataFrame, inverse_labelled_df:pd.DataFrame):\n",
    "    '''Injects the API call patterns of each sample as its last column'''\n",
    "    patterns = []\n",
    "    print(\"Injecting API patterns...\")\n",
    "    for row in range(inner_df.shape[0]):\n",
    "        patterns.append(list_to_str(inverse_labelled_df.iloc[row,1:101].transpose().to_list()))\n",
    "    inner_df['pattern'] = patterns\n",
    "    print(\"\")\n",
    "    inverse_label(inner_df).to_csv(f\"Clustering/Malicious/API_Patterns.csv\", index=False)\n",
    "    return inner_df\n",
    "\n",
    "if not os.path.exists(os.path.abspath(os.getcwd())+\"\\Clustering\"):\n",
    "    os.makedirs(os.path.abspath(os.getcwd())+\"\\Clustering\\Malicious\")\n",
    "    os.makedirs(os.path.abspath(os.getcwd())+\"\\Clustering\\Benign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DF...\n",
      "\n",
      "Inverse Labelling...\n",
      "\n",
      "Injecting API patterns...\n",
      "\n",
      "Inverse Labelling...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>...</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>pattern</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>071e8c3f8922e186e57548cd4c703a5d</td>\n",
       "      <td>112</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>208</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>RegOpenKeyExA,NtOpenKey,NtQueryValueKey,NtClos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33f8e6d08a6aae939f25a8e0d63dd523</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>GetSystemTimeAsFileTime,NtAllocateVirtualMemor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b68abd064e975e1c6d5f25e748663076</td>\n",
       "      <td>16</td>\n",
       "      <td>110</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>113</td>\n",
       "      <td>112</td>\n",
       "      <td>SetUnhandledExceptionFilter,OleInitialize,LdrL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72049be7bd30ea61297ea624ae198067</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>228</td>\n",
       "      <td>302</td>\n",
       "      <td>GetSystemTimeAsFileTime,NtAllocateVirtualMemor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c9b3700a77facf29172f32df6bc77f48</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>40</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>GetSystemTimeAsFileTime,LdrLoadDll,LdrGetProce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42792</th>\n",
       "      <td>e3d6d58faa040f0f9742c9d0eaf58be4</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>GetSystemTimeAsFileTime,LdrLoadDll,LdrGetProce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42793</th>\n",
       "      <td>9b917bab7f32188ae40c744f2be9aaf8</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>159</td>\n",
       "      <td>224</td>\n",
       "      <td>82</td>\n",
       "      <td>159</td>\n",
       "      <td>224</td>\n",
       "      <td>82</td>\n",
       "      <td>159</td>\n",
       "      <td>224</td>\n",
       "      <td>82</td>\n",
       "      <td>GetSystemTimeAsFileTime,LdrLoadDll,LdrGetProce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42794</th>\n",
       "      <td>35a18ee05f75f04912018d9f462cb990</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>GetSystemTimeAsFileTime,LdrLoadDll,LdrGetProce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42795</th>\n",
       "      <td>654139d715abcf7ecdddbef5a84f224b</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>GetSystemTimeAsFileTime,LdrLoadDll,LdrGetProce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42796</th>\n",
       "      <td>078c9d4e7be4819a06974c6f292a4857</td>\n",
       "      <td>112</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>208</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>RegOpenKeyExA,NtOpenKey,NtQueryValueKey,NtClos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42797 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   hash  t_0  t_1  t_2  t_3  t_4  t_5  t_6  \\\n",
       "0      071e8c3f8922e186e57548cd4c703a5d  112  274  158  215  274  158  215   \n",
       "1      33f8e6d08a6aae939f25a8e0d63dd523   82  208  187  208  172  117  172   \n",
       "2      b68abd064e975e1c6d5f25e748663076   16  110  240  117  240  117  240   \n",
       "3      72049be7bd30ea61297ea624ae198067   82  208  187  208  172  117  172   \n",
       "4      c9b3700a77facf29172f32df6bc77f48   82  240  117  240  117  240  117   \n",
       "...                                 ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "42792  e3d6d58faa040f0f9742c9d0eaf58be4   82  240  117  240  117  240  117   \n",
       "42793  9b917bab7f32188ae40c744f2be9aaf8   82  240  117  240  117  240  117   \n",
       "42794  35a18ee05f75f04912018d9f462cb990   82  240  117  240  117  240  117   \n",
       "42795  654139d715abcf7ecdddbef5a84f224b   82  240  117  240  117  240  117   \n",
       "42796  078c9d4e7be4819a06974c6f292a4857  112  274  158  215  274  158  215   \n",
       "\n",
       "       t_7  t_8  ...  t_91  t_92  t_93  t_94  t_95  t_96  t_97  t_98  t_99  \\\n",
       "0      298   76  ...    71   297   135   171   215    35   208    56    71   \n",
       "1      117  172  ...    81   240   117    71   297   135   171   215    35   \n",
       "2      117  240  ...    65   112   123    65   112   123    65   113   112   \n",
       "3      117  172  ...   208   302   208   302   187   208   302   228   302   \n",
       "4      240  117  ...   209   260    40   209   260   141   260   141   260   \n",
       "...    ...  ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "42792  240  117  ...   141   260   141   260   141   260   141   260   141   \n",
       "42793  240  117  ...   159   224    82   159   224    82   159   224    82   \n",
       "42794  240  117  ...   260   141   260   141   260   141   260   141   260   \n",
       "42795  240  117  ...   141   260   141   260   141   260   141   260   141   \n",
       "42796  298   76  ...    71   297   135   171   215    35   208    56    71   \n",
       "\n",
       "                                                 pattern  \n",
       "0      RegOpenKeyExA,NtOpenKey,NtQueryValueKey,NtClos...  \n",
       "1      GetSystemTimeAsFileTime,NtAllocateVirtualMemor...  \n",
       "2      SetUnhandledExceptionFilter,OleInitialize,LdrL...  \n",
       "3      GetSystemTimeAsFileTime,NtAllocateVirtualMemor...  \n",
       "4      GetSystemTimeAsFileTime,LdrLoadDll,LdrGetProce...  \n",
       "...                                                  ...  \n",
       "42792  GetSystemTimeAsFileTime,LdrLoadDll,LdrGetProce...  \n",
       "42793  GetSystemTimeAsFileTime,LdrLoadDll,LdrGetProce...  \n",
       "42794  GetSystemTimeAsFileTime,LdrLoadDll,LdrGetProce...  \n",
       "42795  GetSystemTimeAsFileTime,LdrLoadDll,LdrGetProce...  \n",
       "42796  RegOpenKeyExA,NtOpenKey,NtQueryValueKey,NtClos...  \n",
       "\n",
       "[42797 rows x 102 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_df()\n",
    "df = inject_patterns(df.copy(), inverse_label(df.copy()))\n",
    "reserve_df = df.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "The process as the following:\n",
    "1. Find the best cluster size (by means of the [Silhouette Score](https://tushar-joshi-89.medium.com/silhouette-score-a9f7d8d78f29))\n",
    "2. Search for the most common API pattern for each cluster\n",
    "3. Sample n hashes per cluster that match the most common API pattern for the same cluster.\n",
    "\n",
    "**Silhouette Score:** How good is the quality of data clustering? *(Higher is better)*\n",
    "\n",
    "**Match Ratio:** How common is the most common API call pattern in each cluster among the samples found in its cluster? *(Higher is better)*\n",
    "\n",
    "**Commonality Ratio**: How many of the matching most common API patterns per cluster are there relative to the dataset size. *(Higher is better)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''UPDATE THESE VALUES AS NEEDED'''\n",
    "\n",
    "clusters = [25,50,75,100,125,150,175,200]    # Place either single or multiple values as long as it is in list format. \n",
    "                    # For multiple values, the program will iterate through every cluster size and will choose the best (usually the biggest value) to be part of the best configuration.\n",
    "                    # It influences the projected total number of samples to verify/analyze.\n",
    "                    \n",
    "samplesize = 50     # Max no. of samples to obtain from a cluster. \n",
    "                    # It influences the projected total number of samples to verify/analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = reserve_df.copy()\n",
    "# print(df.shape, \"\\n\")\n",
    "\n",
    "# algorithm = ['lloyd', 'elkan']\n",
    "\n",
    "# #Search for best clusters size and run clustering\n",
    "# bestClusterParam = clustering(df, \"KMeans\", KMeans(n_init='auto', verbose=0, random_state=seed), \n",
    "#                               {'algorithm':algorithm})\n",
    "\n",
    "# commonHashes = get_samplehash_common(df, commonAPI, \"KMeans\", samplesize)\n",
    "# display(commonHashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BisectingKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = reserve_df.copy()\n",
    "# print(df.shape, \"\\n\")\n",
    "\n",
    "# algorithm = ['lloyd', 'elkan']\n",
    "\n",
    "# #Search for best clusters size and run clustering\n",
    "# bestClusterParam = clustering(df, \"BisectingKMeans\", \n",
    "#                               BisectingKMeans(random_state=seed, verbose=0, copy_x=True), \n",
    "#                               {'algorithm':algorithm})\n",
    "\n",
    "# commonHashes = get_samplehash_common(df, commonAPI, \"BisectingKMeans\", samplesize)\n",
    "# display(commonHashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = reserve_df.copy()\n",
    "# print(df.shape, \"\\n\")\n",
    "\n",
    "# reassignment_ratio = [0.01, 0.1, 0.3]\n",
    "\n",
    "# #Search for best clusters size and run clustering\n",
    "# bestClusterParam = clustering(df, \"MiniBatchKMeans\", \n",
    "#                               MiniBatchKMeans(verbose=0, random_state=seed, n_init='auto', \n",
    "#                                               batch_size=os.cpu_count()*256), \n",
    "#                               {'reassignment_ratio':reassignment_ratio})\n",
    "\n",
    "# commonHashes = get_samplehash_common(df, commonAPI, \"MiniBatchKMeans\", samplesize)\n",
    "# display(commonHashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42797, 102) \n",
      "\n",
      "Searching Best Clustering Parameters...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcityblock\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanhattan\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#Search for best clusters size and run clustering\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m bestClusterParam \u001b[38;5;241m=\u001b[39m clustering(df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDBSCAN\u001b[39m\u001b[38;5;124m\"\u001b[39m, DBSCAN(algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \n\u001b[0;32m     10\u001b[0m                               {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples\u001b[39m\u001b[38;5;124m'\u001b[39m : min_samples, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m: metrics})\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# commonAPI = common_api_cluster(df, \"DBSCAN\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# display(commonAPI)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m commonHashes \u001b[38;5;241m=\u001b[39m get_samplehash_common(df, commonAPI, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDBSCAN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 87\u001b[0m, in \u001b[0;36mclustering\u001b[1;34m(inner_df, name, clusterer, parameters)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''The input dataset must contain integer API calls (except the API Call Pattern strings)'''\u001b[39;00m\n\u001b[0;32m     86\u001b[0m X \u001b[38;5;241m=\u001b[39m get_x(inner_df)\n\u001b[1;32m---> 87\u001b[0m bestCluster, bestClusterer \u001b[38;5;241m=\u001b[39m search_k(parameters, clusterer, X) \u001b[38;5;66;03m#assumes bestCluster already fitted\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m#bestClusterer.fit(X)\u001b[39;00m\n\u001b[0;32m     89\u001b[0m inner_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m bestClusterer\u001b[38;5;241m.\u001b[39mlabels_\n",
      "Cell \u001b[1;32mIn[1], line 69\u001b[0m, in \u001b[0;36msearch_k\u001b[1;34m(parameters, model, X)\u001b[0m\n\u001b[0;32m     67\u001b[0m model\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mp)    \u001b[38;5;66;03m# set current hyper parameter\u001b[39;00m\n\u001b[0;32m     68\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 69\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X)          \u001b[38;5;66;03m# fit model on wine dataset, this will find clusters based on parameter p\u001b[39;00m\n\u001b[0;32m     70\u001b[0m ss \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39msilhouette_score(X, model\u001b[38;5;241m.\u001b[39mlabels_, random_state\u001b[38;5;241m=\u001b[39mseed)   \u001b[38;5;66;03m# calculate silhouette_score\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# silhouette_scores.append([p, ss]) # store all the scores\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_dbscan.py:410\u001b[0m, in \u001b[0;36mDBSCAN.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    408\u001b[0m neighbors_model\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;66;03m# This has worst case O(n^2) memory complexity\u001b[39;00m\n\u001b[1;32m--> 410\u001b[0m neighborhoods \u001b[38;5;241m=\u001b[39m neighbors_model\u001b[38;5;241m.\u001b[39mradius_neighbors(X, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    413\u001b[0m     n_neighbors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mlen\u001b[39m(neighbors) \u001b[38;5;28;01mfor\u001b[39;00m neighbors \u001b[38;5;129;01min\u001b[39;00m neighborhoods])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:1243\u001b[0m, in \u001b[0;36mRadiusNeighborsMixin.radius_neighbors\u001b[1;34m(self, X, radius, return_distance, sort_results)\u001b[0m\n\u001b[0;32m   1241\u001b[0m     results \u001b[38;5;241m=\u001b[39m neigh_dist, neigh_ind\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m     neigh_ind_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(chunked_results, [])\n\u001b[0;32m   1244\u001b[0m     results \u001b[38;5;241m=\u001b[39m _to_object_array(neigh_ind_list)\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sort_results:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2153\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2152\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m D_chunk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 2153\u001b[0m     D_chunk \u001b[38;5;241m=\u001b[39m reduce_func(D_chunk, sl\u001b[38;5;241m.\u001b[39mstart)\n\u001b[0;32m   2154\u001b[0m     _check_chunk_size(D_chunk, chunk_size)\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m D_chunk\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:1074\u001b[0m, in \u001b[0;36mRadiusNeighborsMixin._radius_neighbors_reduce_func\u001b[1;34m(self, dist, start, radius, return_distance)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_radius_neighbors_reduce_func\u001b[39m(\u001b[38;5;28mself\u001b[39m, dist, start, radius, return_distance):\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reduce a chunk of distances to the nearest neighbors.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \n\u001b[0;32m   1050\u001b[0m \u001b[38;5;124;03m    Callback to :func:`sklearn.metrics.pairwise.pairwise_distances_chunked`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;124;03m        The neighbors indices.\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1074\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mwhere(d \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m radius)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dist]\n\u001b[0;32m   1076\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_distance:\n\u001b[0;32m   1077\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:1074\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_radius_neighbors_reduce_func\u001b[39m(\u001b[38;5;28mself\u001b[39m, dist, start, radius, return_distance):\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reduce a chunk of distances to the nearest neighbors.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \n\u001b[0;32m   1050\u001b[0m \u001b[38;5;124;03m    Callback to :func:`sklearn.metrics.pairwise.pairwise_distances_chunked`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;124;03m        The neighbors indices.\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1074\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mwhere(d \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m radius)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dist]\n\u001b[0;32m   1076\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_distance:\n\u001b[0;32m   1077\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\multiarray.py:346\u001b[0m, in \u001b[0;36mwhere\u001b[1;34m(condition, x, y)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    inner(a, b, /)\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    341\u001b[0m \n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b)\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mwhere)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhere\u001b[39m(condition, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    348\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m    where(condition, [x, y], /)\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m           [ 0,  3, -1]])\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (condition, x, y)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = reserve_df.copy()\n",
    "print(df.shape, \"\\n\")\n",
    "\n",
    "#eps = [0.2,0.5,0.8]\n",
    "min_samples = [1,2,3,4,5]\n",
    "metrics = ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']\n",
    "\n",
    "#Search for best clusters size and run clustering\n",
    "bestClusterParam = clustering(df, \"DBSCAN\", DBSCAN(algorithm='auto', n_jobs=-1), \n",
    "                              {'min_samples' : min_samples, 'metric': metrics})\n",
    "\n",
    "commonHashes = get_samplehash_common(df, commonAPI, \"DBSCAN\", 5)\n",
    "display(commonHashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Birch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = reserve_df.copy()\n",
    "# print(df.shape, \"\\n\")\n",
    "\n",
    "# branching_factor = [25,50,75,100]\n",
    "\n",
    "# #Search for best clusters size and run clustering\n",
    "# bestClusterParam = clustering(df, \"Birch\", Birch(compute_labels=True), \n",
    "#                               {'branching_factor':branching_factor})\n",
    "\n",
    "# commonHashes = get_samplehash_common(df, commonAPI, \"Birch\", samplesize)\n",
    "# display(commonHashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpectralClustering\n",
    "\n",
    "*Due to some dataset- or hardware-related limitations, SpectralClustering cannot be executed using the full Oliveira dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = reserve_df.copy()\n",
    "# print(df.shape, \"\\n\")\n",
    "\n",
    "# n_neighbors=[10,20,30,40,50]\n",
    "# assign_labels=['kmeans','discretize']\n",
    "\n",
    "# #Search for best clusters size and run clustering\n",
    "# bestClusterParam = clustering(df, \"SpectralClustering\", \n",
    "#                               SpectralClustering(random_state=seed, affinity='nearest_neighbors', assign_labels='kmeans', n_jobs=-1, verbose=0), \n",
    "#                               {'n_neighbors':n_neighbors, 'assign_labels':assign_labels})\n",
    "\n",
    "# commonHashes = get_samplehash_common(df, commonAPI, \"SpectralClustering\", samplesize)\n",
    "# display(commonHashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AgglomerativeClustering (Ward)\n",
    "\n",
    "*Due to memory-related limitations, AgglomerativeClustering cannot be executed using the full Oliveira dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = reserve_df.copy()\n",
    "# print(df.shape, \"\\n\")\n",
    "\n",
    "# print(\"Computing Connectivity...\")\n",
    "# start_time = time.time()\n",
    "# connectivity = []\n",
    "# connectivity.append(kneighbors_graph(get_x(df:pd.DataFrame).values, 2, mode='connectivity', include_self='auto', n_jobs=-1))\n",
    "# #connectivity.append(kneighbors_graph(get_x(df:pd.DataFrame).values, 4, mode='connectivity', include_self='auto', n_jobs=-1))\n",
    "# #connectivity.append(kneighbors_graph(get_x(df:pd.DataFrame).values, 6, mode='connectivity', include_self='auto', n_jobs=-1))\n",
    "# #connectivity.append(kneighbors_graph(get_x(df:pd.DataFrame).values, 8, mode='connectivity', include_self='auto', n_jobs=-1))\n",
    "# print(f\"Connectivity Computation: {time.time()-start_time:.4f}(s)\\n\")\n",
    "\n",
    "# #Search for best clusters size and run clustering\n",
    "# bestClusterParam = clustering(df, \"AgglomerativeClustering\", AgglomerativeClustering(memory=\".memory/\", \n",
    "#                                                                                      linkage='complete', \n",
    "#                                                                                      connectivity=connectivity), \n",
    "#                               {'connectivity':connectivity})\n",
    "\n",
    "# commonAPI = common_api_cluster(df, \"AgglomerativeClustering\")\n",
    "# display(commonAPI)\n",
    "\n",
    "# commonHashes = get_samplehash_common(df, commonAPI, \"AgglomerativeClustering\", samplesize)\n",
    "# display(commonHashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optics\n",
    "\n",
    "*Due to some dataset- or hardware-related limitations, Optics cannot be executed using the full Oliveira dataset in a reasonable time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = reserve_df.copy()\n",
    "# print(df.shape, \"\\n\")\n",
    "\n",
    "# leaf_size = [10,20,30,40,50] #Let's assume that there are up to 100 clusters that can be derived from the Oliveira dataset. Going beyond that is too much\n",
    "# min_samples = [5,10,15,20]\n",
    "\n",
    "# #Search for best clusters size and run clustering\n",
    "# bestClusterParam = clustering(df, \"OPTICS\", OPTICS(algorithm='auto', memory=\".memory/\", n_jobs=-1), \n",
    "#                               {'leaf_size': leaf_size, 'min_samples':min_samples}, \n",
    "#                               samplesize)\n",
    "\n",
    "# commonAPI = common_api_cluster(df, \"OPTICS\")\n",
    "# display(commonAPI)\n",
    "\n",
    "# commonHashes = get_samplehash_common(df, commonAPI, \"OPTICS\", samplesize)\n",
    "# display(commonHashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MeanShift\n",
    "\n",
    "*Due to some hardware-related limitations, MeanShift cannot be executed using the lite/full Oliveira dataset in a reasonable time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = reserve_df.copy()\n",
    "# print(df.shape, \"\\n\")\n",
    "\n",
    "# max_iter = [300,500] #Let's assume that there are up to 100 clusters that can be derived from the Oliveira dataset. Going beyond that is too much\n",
    "\n",
    "# #Search for best clusters size and run clustering\n",
    "# bestClusterParam = clustering(df, \"MeanShift\", MeanShift(n_jobs=-1), {'max_iter': max_iter})\n",
    "\n",
    "# commonAPI = common_api_cluster(df, \"MeanShift\")\n",
    "# display(commonAPI)\n",
    "\n",
    "# commonHashes = get_samplehash_common(df, commonAPI, \"MeanShift\", samplesize)\n",
    "# display(commonHashes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
