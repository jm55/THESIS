{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Demo (Benign)\n",
    "\n",
    "**Clustering Methods not Supported:** GaussianMixture & HDBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "def cpu_count():\n",
    "    return multiprocessing.cpu_count()\n",
    "\n",
    "def list_to_str(ls):\n",
    "    output = \"\"\n",
    "    for l in ls:\n",
    "        output += str(l) + \",\"\n",
    "    return output[0:len(output)-1]\n",
    "\n",
    "def load_df():\n",
    "    print(\"Loading DF...\")\n",
    "    df = pd.read_csv(\"oliveira_lite.csv\", low_memory=False, memory_map=True)\n",
    "    df = df[df['malware'] == 0].copy()\n",
    "    df = df.drop('malware', axis=1)\n",
    "    print(\"\")\n",
    "    return df.reset_index().iloc[:,1:]\n",
    "\n",
    "def get_x(df):\n",
    "    return df.iloc[:, 1:102-1]\n",
    "\n",
    "#Load list of API calls\n",
    "API_LIST = \"api_calls.txt\"\n",
    "DELIMITER = \"NaN\"\n",
    "API_FILE = open(API_LIST,\"r\")\n",
    "APIS = API_FILE.readline().split(',')\n",
    "APIS.append(DELIMITER) #serves as a label for NaN values for Instance-based datasets\n",
    "API_FILE.close()\n",
    "\n",
    "#Random Seed\n",
    "seed = 1\n",
    "\n",
    "#Inverse Label Encoding\n",
    "def inverse_labeller(item):\n",
    "    global APIS\n",
    "    return item.map(lambda x: APIS[int(x)])\n",
    "def inverse_label(df):\n",
    "    print(\"Inverse Labelling...\")\n",
    "    df.iloc[:, 1:101] = df.iloc[:, 1:101].apply(inverse_labeller, axis=1, result_type='reduce')\n",
    "    print(\"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: https://medium.com/swlh/k-means-clustering-on-high-dimensional-data-d2151e1a4240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_api_cluster(inner_df, name, size):\n",
    "    global df\n",
    "    inner_df = df\n",
    "    clusters = inner_df['cluster'].unique()\n",
    "    clusters.sort()\n",
    "    columns = inner_df.columns[1:102]\n",
    "    commonAPI = []\n",
    "    print(\"Searching for Common API Patterns per Cluster...\")\n",
    "    print(clusters)\n",
    "    for cluster in clusters:\n",
    "        raw_commonC = inner_df[inner_df['cluster']==cluster]['pattern']#.value_counts()\n",
    "        commonC = raw_commonC.value_counts().to_frame(name='counts').reset_index()\n",
    "        commonAPI.append([cluster, commonC['counts'].iloc[0], round(commonC['counts'].iloc[0]/raw_commonC.shape[0],4), commonC['pattern'].iloc[0]])\n",
    "    commonAPI = pd.DataFrame(commonAPI, columns=['cluster', 'count', 'match_ratio', 'pattern'])\n",
    "    commonAPI.to_csv(f\"Clustering/Benign/{name}_Common_API_Cluster.csv\", index=False)\n",
    "    print(\"\")\n",
    "    return commonAPI\n",
    "\n",
    "def get_samplehash_common(inner_df, common_counts, name, size, samplesize):\n",
    "    hashes = []\n",
    "    global seed\n",
    "    random.seed(seed)\n",
    "    matching_samples = 0\n",
    "    print(f\"Random (Seed @ {seed}) Sampling Hashes subset of  Most Common API Patterns...\")\n",
    "    for pattern in range(common_counts.shape[0]):\n",
    "        sub_df = inner_df[inner_df['pattern']==common_counts.iloc[pattern,3]]\n",
    "        subsamples = len(sub_df['hash'].to_list())\n",
    "        matching_samples += subsamples\n",
    "        if subsamples < samplesize:\n",
    "            subsamples = random.sample(sub_df['hash'].to_list(), len(sub_df['hash'].to_list()))\n",
    "        else:\n",
    "            subsamples = random.sample(sub_df['hash'].to_list(), samplesize)\n",
    "        for subsample in subsamples:\n",
    "            hashes.append([common_counts.iloc[pattern,:]['cluster'], subsample, '_', '_', '_', common_counts.iloc[pattern,:]['pattern']])\n",
    "    hashes = pd.DataFrame(hashes, columns=['cluster', 'hash', 'Type 1', 'Type 2', 'Type 3', 'pattern'])\n",
    "    hashes.to_csv(f\"Clustering/Benign/{name}_SampleHash_Common.csv\", index=False)\n",
    "    print(f\"Commonality Ratio: {(matching_samples/inner_df.shape[0])*100:.4f}%\")\n",
    "    print(\"\")\n",
    "    return hashes\n",
    "    \n",
    "def inject_patterns(inner_df, inverse_labelled_df):\n",
    "    patterns = []\n",
    "    print(\"Injecting API patterns...\")\n",
    "    for row in range(inner_df.shape[0]):\n",
    "        patterns.append(list_to_str(inverse_labelled_df.iloc[row,1:101].transpose().to_list()))\n",
    "    inner_df['pattern'] = patterns\n",
    "    print(\"\")\n",
    "    inverse_label(inner_df).to_csv(f\"Clustering/Benign/API_Patterns.csv\", index=False)\n",
    "    return inner_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DF...\n",
      "\n",
      "Inverse Labelling...\n",
      "\n",
      "Injecting API patterns...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = load_df()\n",
    "df = inject_patterns(df.copy(), inverse_label(df.copy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reserve_df = df.copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
