{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "\n",
    "**Objectives**\n",
    "- Take note of the results of previous performance tests that used the Holdout Method (i.e., during Model Robustness Test)\n",
    "- Using `McNemar Test`, determine any differences between GBDT Models (e.g., LGBM Default vs CatBoost Default), between configurations (e.g., LGBM Default vs LGBM Tuned), and between behavior-types (i.e., Time-based LGBM Tuned vs Time-based CatBoost Tuned)\n",
    "- Use whichever dataset is appropriate (probably the Test/Holdout Split).\n",
    "- Take note of the results\n",
    "\n",
    "Assume a `significance level` of **0.05 (5%)** as it was mentioned in RRL relating to Model Comparison (let's just use it the reference no. for significane level).\n",
    "\n",
    "<hr>\n",
    "\n",
    "*Kindly double check the statement(s) that will follow:*\n",
    "\n",
    "Assume null hypothesis as *\"there is a significant difference between the two models\"*. `<== Modify this accordingly depending on which will be compared (whether if GBDT vs GBDT or Default vs Tuned)`\n",
    "\n",
    "If the resulting `p-value` is larger than the `significance level`, the null hypothesis is not rejected. Else if otherwise (`p-value` < `significance level`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.stats.contingency_tables as statsmodels #mcnemar\n",
    "import mlxtend.evaluate as mlxtend #mcnemar_table, mcnemar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgbm\n",
    "import catboost as catb\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_LGBM_TB = pd.read_csv('../Dataset/TB/LGBM_TB_Test.csv', low_memory=False) #<== Point these to the proper Test/Holdout datasets.\n",
    "DF_LGBM_IB = pd.read_csv('../Dataset/IB/LGBM_IB_Test.csv', low_memory=False)\n",
    "DF_CATB_TB = pd.read_csv('../Dataset/TB/CATB_TB_Test.csv', low_memory=False) #<== Point these to the proper Test/Holdout datasets.\n",
    "DF_CATB_IB = pd.read_csv('../Dataset/IB/CATB_IB_Test.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Battle Chart:**\n",
    "\n",
    "**GBDT vs GBDT**\n",
    "- LGBM TB vs CatBoost TB\n",
    "- LGBM IB vs CatBoost IB\n",
    "- Tuned LGBM TB vs Tuned CatBoost TB\n",
    "- Tuned LGBM IB vs Tuned CatBoost IB\n",
    "\n",
    "**Default vs Tuned**\n",
    "- LGBM TB vs Tuned LGBM TB\n",
    "- LGBM IB vs Tuned LGBM IB\n",
    "- CatBoost TB vs Tuned CatBoost TB\n",
    "- CatBoost IB vs Tuned CatBoost IB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: categorical_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ejose\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.2.1 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4328,    8],\n",
       "       [   5,   47]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels.mcnemar:\n",
      "pvalue      0.40538055645894244\n",
      "statistic   0.6923076923076923\n",
      "\n",
      "mlxtend.mcnemar (sanity check):\n",
      "pvalue:\t0.40538055645894244\n",
      "chi2:\t0.6923076923076923\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SAMPLE USE CASE (LGBM TB vs Tuned LGBM TB)\n",
    "\n",
    "default_tb = load('../GBDT_Training/Outputs/Results/Demo//LGBM/Train (Default)/DEMO_LGBM_TB.model') # <== Point these to the respective .model files\n",
    "tuned_tb = load('../GBDT_Training/Outputs/Results/Demo/LGBM/Train (Tuned)/TUNED_DEMO_LGBM_TB.model')\n",
    "\n",
    "# The correct target (class) labels\n",
    "y_target = DF_LGBM_TB['malware'] # <=== Collect labels as list\n",
    "\n",
    "# Class labels predicted by model 1\n",
    "y_model1 = default_tb.predict(X=DF_LGBM_TB.iloc[:,1:101]) # <=== Collect Model A predictions as list\n",
    "\n",
    "# Class labels predicted by model 2\n",
    "y_model2 = tuned_tb.predict(X=DF_LGBM_TB.iloc[:,1:101]) # <=== Collect Model B predictions as list\n",
    "\n",
    "table = mlxtend.mcnemar_table(y_target=y_target, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "display(table)\n",
    "\n",
    "print(\"statsmodels.mcnemar:\")\n",
    "print(statsmodels.mcnemar(table, exact=False, correction=False))\n",
    "print(\"\")\n",
    "\n",
    "chi2, p = mlxtend.mcnemar(table, exact=False, corrected=False)\n",
    "print(\"mlxtend.mcnemar (sanity check):\")\n",
    "print(f\"pvalue:\\t{p}\\nchi2:\\t{chi2}\\n\")\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
