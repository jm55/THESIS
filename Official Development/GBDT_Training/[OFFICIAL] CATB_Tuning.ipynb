{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "696eff13",
   "metadata": {},
   "source": [
    "# Evaluation and Comparison of Boosted ML Models in Behavior-Based Malware Detection\n",
    "\n",
    "\n",
    "## Notebook: CatBoost Tuning\n",
    "\n",
    "***\n",
    "\n",
    "**What is the objective of this file?**\n",
    "\n",
    "To tune the model using the Train Split (Dynamically Split)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d03d82a",
   "metadata": {},
   "source": [
    "# Checklist\n",
    "\n",
    "- Ensure that you have the proper dataset files that you intend to use (i.e., whether the lite dataset or full version). \n",
    "    - The datasets it will use points to `/Official Development/Dataset/IB` & `/Official Development/Dataset/TB`. \n",
    "    - You can run the `/Official Development/Dataset [OFFICIAL] Oliveira Dataset Notebook.ipynb file` or unzip one of the zipped folders in the `/Official Development/Dataset/Processed` towards the two aforementioned folders. \n",
    "- Ensure that you have installed the necessary libraries needed to execute the training process. \n",
    "    - You can view the list of the specific versions in the thesis document or through the `.sh` or `.bat` files in the repository's home directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e57fcc",
   "metadata": {},
   "source": [
    "# 1. CatBoost Tuning Setup\n",
    "\n",
    "Setting tuning environment parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f39e6a",
   "metadata": {},
   "source": [
    "## 1.0. Tuning Settings\n",
    "\n",
    "1. What will the output filename be?\n",
    "2. What is the dictionary of hyperparameter values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d5f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILENAME = \"TEST_TUNE\" # <== Specify the filename of the output hyperparameters text file (can be used during tuned model training).\n",
    "\n",
    "K = 5\n",
    "DYNAMIC_SPLIT = 1/K # <== Expected validation ratio. Training ratio will be 1-(1/K). Just for when needed to view.\n",
    "RANDOM_SEED = 1 # <==  <== Must be the same throughout the entire study (acts as a controlled variable), hence let's just settle with 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d93db1c",
   "metadata": {},
   "source": [
    "**Recommended Parameters as per the paper:**\n",
    "1. grow_policy = SymmetricTree\n",
    "2. objective = Logloss\n",
    "3. bootstrap_type* = Bayesian\n",
    "4. boosting_type = Ordered\n",
    "\n",
    "*As per documentation relative to study's objectives and needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f78714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample (tested) parameter set\n",
    "catb_params = {\n",
    "    #Based-on Study/Documentation (Do not change)\n",
    "    'grow_policy':['SymmetricTree'], #By design, CatBoost is intended to grow symmetrically.\n",
    "    'objective':['Logloss'], #To be same as what's used in Binary LightGBMClassifier\n",
    "    'bootstrap_type':['Bayesian'], #Recommended for classifier models\n",
    "    'boosting_type':['Ordered'], #As per paper \"Ordered Boosting\"\n",
    "    \n",
    "    #Common (Add and experiment with valid values)\n",
    "    'max_depth':[6,16], #As per documentation: min=6, max=16 (Logloss), mean=(max-min)/2\n",
    "    'learning_rate':[0.03, 0.1, 0.3], #Typical learning rate (either 0.01, 0.03, 0.1, 0.3)\n",
    "    'n_estimators':[10,25,50,75,100,250,500,750,1000], #Limited to 50 for prototyping; [50,100,250,500,750,1000,2000,5000,10000]\n",
    "    'l2_leaf_reg':[3], #CatB default=3\n",
    "    'auto_class_weights':['None', 'Balanced', 'SqrtBalanced'], #Specify that classes are weighted equally (i.e., no inherent bias); ['None', 'Balanced', 'SqrtBalanced']\n",
    "    \n",
    "    #Other hyperparameters\n",
    "    \n",
    "    #Others; Not common but can improve accuracy/performance:\n",
    "    'task_type':['CPU'], #CPU/GPU (CUDA)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c808a5",
   "metadata": {},
   "source": [
    "# ⚠️Warning\n",
    "\n",
    "**Be careful of modifying the code beyond this point as it was designed to run autonomously based on the parameters set above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c23aa33",
   "metadata": {},
   "source": [
    "## 1.1. Loading Libraries & Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d526108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python Libraries\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "#Data/Dataset Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Model Selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "#Metrics (for in-tuning testing only)\n",
    "from sklearn.metrics import classification_report, accuracy_score, balanced_accuracy_score, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "\n",
    "#Visualization\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#GBDT Models\n",
    "# import lightgbm\n",
    "import catboost\n",
    "\n",
    "#File Writing Library (exclusive for use on LightGBM)\n",
    "from joblib import dump, load\n",
    "\n",
    "start = end = 0\n",
    "LOG_FILENAME = \"CATB_Tuning_Log.txt\"\n",
    "def logging(message):\n",
    "    log = open(LOG_FILENAME, \"a\")\n",
    "    log.write(message)\n",
    "    log.close()\n",
    "def startTime():\n",
    "    global start\n",
    "    start = time.time()\n",
    "def endTime(process):\n",
    "    global start\n",
    "    elapse = time.time()-start\n",
    "    start = 0\n",
    "    printout = f\"{str(datetime.now())}@{OUTPUT_FILENAME}: {process} - {round(elapse, 6)}s\\n\"\n",
    "    logging(printout)\n",
    "    return round(elapse, 6)\n",
    "def printToFile(label, output):\n",
    "    global OUTPUT_FILENAME\n",
    "    filename = OUTPUT_FILENAME + \"_\" + label + \".tune\"\n",
    "    with open(\"Outputs/\" + filename, 'w') as f:\n",
    "        f.write(str(output))\n",
    "        f.close()\n",
    "def getIndexes(dataset):\n",
    "    indexes = []\n",
    "    for i in range(dataset.shape[1]-1):\n",
    "        indexes.append(\"t_\"+str(i))\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6f3d1",
   "metadata": {},
   "source": [
    "## 1.2. Loading Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db54f53d",
   "metadata": {},
   "source": [
    "### 1.2.1. Setting Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e377c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting filenames of files\n",
    "TB_Train = \"../Dataset/TB/TB_CATB.csv\" # <== Location for Time-based Train Split for LightGBM\n",
    "#TB_Test = \"../Dataset/TB/TB_Test_CATB.csv\" # <== Location for Time-based Test Split for LightGBM\n",
    "IB_Train = \"../Dataset/IB/IB_CATB.csv\" # <== Location for Instance-based Train Split for LightGBM\n",
    "#IB_Test = \"../Dataset/IB/IB_Test_CATB.csv\" # <== Location for Instance-based Test Split for LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c60f6c",
   "metadata": {},
   "source": [
    "### 1.2.2. Loading datasets to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd143938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading datasets to DataFrames\n",
    "tb_train = pd.read_csv(TB_Train, low_memory=False).fillna(\"NaN\")\n",
    "ib_train = pd.read_csv(IB_Train, low_memory=False).fillna(\"NaN\")\n",
    "\n",
    "print(\"Dataset Sizes\")\n",
    "print(\"TB Train Size:\", tb_train.shape)\n",
    "print(\"IB Train Size:\", ib_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1f24ad",
   "metadata": {},
   "source": [
    "### 1.2.3. Previewing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previewing Time-based Dataset\n",
    "tb_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1a597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previewing Instance-based Dataset\n",
    "ib_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfa163a",
   "metadata": {},
   "source": [
    "## 1.3. Preparing Stratified K-Folds for RandomizedSearchCV\n",
    "\n",
    "By default, RandomizedSearchCV uses a normal K-Folds with k value of 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cafce6",
   "metadata": {},
   "source": [
    "### 1.3.1. Underlying Code for Stratified K-Folds\n",
    "\n",
    "Most of the code is for visualizing actually. Only `getStratKFold()` matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05742e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStratKFold():\n",
    "    global K\n",
    "    return StratifiedKFold(n_splits=K, shuffle=True, random_state=1)\n",
    "\n",
    "#K-folds sample visualization\n",
    "def kfolds_vis(dataset):\n",
    "    global K\n",
    "    X = dataset.iloc[:,1:] #All rows, 2nd to last column\n",
    "    y = dataset.iloc[:,0] #All rows, first column only\n",
    "    fig, ax = plt.subplots(figsize=(10,K+1))\n",
    "    train = plot_cv_indices(getStratKFold(), X, y, ax, K)\n",
    "    plt.show()\n",
    "\n",
    "#K-folds sample visualization (inner workings)\n",
    "def plot_cv_indices(cv, X, y, ax, n_splits, lw=25):\n",
    "    #From: https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt], indices[tr] = 1,0\n",
    "        ax.scatter(range(len(indices)), [ii] * len(indices), c=indices, marker=\"_\", \n",
    "                   lw=lw, cmap=plt.cm.Paired, vmin=0, vmax=1)\n",
    "    #ax.scatter(range(len(X)), [ii] * len(X), c=y, marker=\"_\", lw=lw, cmap=plt.cm.Paired)\n",
    "    yticklabels = list(range(n_splits))\n",
    "    ax.set(yticks=np.arange(n_splits), yticklabels=yticklabels, xlabel=\"Dataset Subsample\", ylabel=\"CV iteration\", ylim=[n_splits,-1], xlim=[0, X.shape[0]])\n",
    "    ax.set_title(\"{}\".format(type(cv).__name__))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc4f3c1",
   "metadata": {},
   "source": [
    "### 1.3.2. Visualizing the K-Folds\n",
    "\n",
    "The objective of the visualization is to show that no CV iteration will have the same set of samples (i.e., no overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29154991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Stratified K-Folds Split at\",K,\"splits.\")\n",
    "print(\"TB/TB_Encoded Dataset\")\n",
    "kfolds_vis(tb_train)\n",
    "print(\"IB/IB_Encoded Dataset\")\n",
    "kfolds_vis(ib_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa3a69",
   "metadata": {},
   "source": [
    "### 1.3.3. Plotting Search CV Results\n",
    "\n",
    "As suggested from https://stackoverflow.com/a/57013458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68993e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_search_results(grid, cv_results, scoring=['Accuracy','Precision','Recall']):\n",
    "    ## Results from grid search\n",
    "    results = grid.cv_results_\n",
    "    means_test_acc = results['mean_test_accuracy']\n",
    "    stds_test_acc = results['std_test_accuracy']\n",
    "    # means_train_acc = results['mean_train_accuracy']\n",
    "    # stds_train_acc = results['std_train_accuracy']\n",
    "    means_test_prec = results['mean_test_precision']\n",
    "    stds_test_prec = results['std_test_precision']\n",
    "    # means_train_prec = results['mean_train_precision']\n",
    "    # stds_train_prec = results['std_train_precision']\n",
    "    means_test_rec = results['mean_test_recall']\n",
    "    stds_test_rec = results['std_test_recall']\n",
    "    # means_train_rec = results['mean_train_recall']\n",
    "    # stds_train_rec = results['std_train_recall']\n",
    "    ## Getting indexes of values per hyper-parameter\n",
    "    masks=[]\n",
    "    masks_names= list(grid.best_params_.keys())\n",
    "    for p_k, p_v in grid.best_params_.items():\n",
    "        masks.append(list(results['param_'+p_k].data==p_v))\n",
    "    params = grid.get_params()['param_grid']\n",
    "    ## Ploting results\n",
    "    fig, ax = plt.subplots(int(len(params)/4),4, sharex='none', sharey='none',figsize=(16,9))\n",
    "    fig.suptitle(f'Score per parameter {str(scoring)}')\n",
    "    fig.text(0.04, 0.5, 'MEAN SCORE', va='center', rotation='vertical')\n",
    "    pram_preformace_in_best = {}\n",
    "    row = col = 0\n",
    "    for i, p in enumerate(masks_names):\n",
    "        m = np.stack(masks[:i] + masks[i+1:])\n",
    "        pram_preformace_in_best\n",
    "        best_parms_mask = m.all(axis=0)\n",
    "        best_index = np.where(best_parms_mask)[0]\n",
    "        x = np.array(params[p])\n",
    "        ax[row,col].errorbar(x, np.array(means_test_acc[best_index]), np.array(stds_test_acc[best_index]), linestyle='--', marker='o', label='Accuracy (Test)')\n",
    "        # ax[row,col].errorbar(x, np.array(means_train_acc[best_index]), np.array(stds_train_acc[best_index]), linestyle='-', marker='^',label='Accuracy (Train)')\n",
    "        ax[row,col].errorbar(x, np.array(means_test_prec[best_index]), np.array(stds_test_prec[best_index]), linestyle='--', marker='o', label='Precision (Test)')\n",
    "        # ax[row,col].errorbar(x, np.array(means_train_prec[best_index]), np.array(stds_train_prec[best_index]), linestyle='-', marker='^',label='Precision (Train)')\n",
    "        ax[row,col].errorbar(x, np.array(means_test_rec[best_index]), np.array(stds_test_rec[best_index]), linestyle='--', marker='o', label='Recall (Test)')\n",
    "        # ax[row,col].errorbar(x, np.array(means_train_rec[best_index]), np.array(stds_train_rec[best_index]), linestyle='-', marker='^',label='Recall (Train)')\n",
    "        ax[row,col].set_xlabel(p.upper())\n",
    "        col += 1\n",
    "        if col >= 4:\n",
    "            row += 1\n",
    "            col = 0\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(16,9))\n",
    "    # plt.plot(cv_results.index, cv_results['mean_train_accuracy'], linestyle='-', marker='o', label='Accuracy (Train)')\n",
    "    plt.plot(cv_results.index, cv_results['mean_test_accuracy'], linestyle='-', marker='o', label='Accuracy (Test)')\n",
    "    # plt.plot(cv_results.index, cv_results['mean_train_precision'], linestyle='-', marker='o', label='Precision (Train)')\n",
    "    plt.plot(cv_results.index, cv_results['mean_test_precision'], linestyle='-', marker='o', label='Precision (Test)')\n",
    "    # plt.plot(cv_results.index, cv_results['mean_train_recall'], linestyle='-', marker='o', label='Recall (Train)')\n",
    "    plt.plot(cv_results.index, cv_results['mean_test_recall'], linestyle='-', marker='o', label='Recall (Test)')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.title(f'{str(scoring)} per Iteration')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3086d",
   "metadata": {},
   "source": [
    "## 2. Tuning Proper\n",
    "\n",
    "*Actual Model Tuning Process*\n",
    "\n",
    "**Notes:**\n",
    "1. Not all hyperparameters are compatible with each other.\n",
    "2. Select, test, and prioritize ones that are relevant to the project.\n",
    "3. The example shown below are already tested for compatibility. \n",
    "4. Watch your terminal for outputs.\n",
    "\n",
    "LGBM Hyperparameters: https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "\n",
    "CatBoost Hyperparameters: https://catboost.ai/en/docs/references/training-parameters/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f876f",
   "metadata": {},
   "source": [
    "## 2.1. Tuning Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beea04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Stratified K-Folds\n",
    "stratKFold = getStratKFold()\n",
    "\n",
    "#Specify features (X) and labels (y) for Time-based data\n",
    "X_tb = tb_train.iloc[:,1:] #All rows, 2nd to last column\n",
    "y_tb = tb_train.iloc[:,0] #All rows, first column only\n",
    "\n",
    "#Specify features (X) and labels (y) for Instance-based data\n",
    "X_ib = ib_train.iloc[:,1:] #All rows, 2nd to last column\n",
    "y_ib = ib_train.iloc[:,0] #All rows, first column only\n",
    "\n",
    "def get_threshold(cv_results, target:str):\n",
    "    return cv_results[target].max() - cv_results[target].std()\n",
    "\n",
    "def refit_strategy(cv_results):\n",
    "    cv_results_ = pd.DataFrame(cv_results)\n",
    "    # Filter-out all results below the threshold\n",
    "    high_accuracy_cv_results = cv_results_[cv_results_[\"mean_test_accuracy\"] > .90]\n",
    "    high_precision_cv_results = high_accuracy_cv_results[high_accuracy_cv_results[\"mean_test_precision\"] > get_threshold(high_accuracy_cv_results, 'mean_test_precision')]\n",
    "    high_recall_cv_results = high_precision_cv_results[high_precision_cv_results[\"mean_test_recall\"] > get_threshold(high_precision_cv_results, 'mean_test_recall')]\n",
    "\n",
    "    return high_recall_cv_results[\"mean_score_time\"].idxmin() #Return the one that is fastest\n",
    "\n",
    "def getTuner(model, params, scoring=['accuracy','precision','recall']):\n",
    "    return GridSearchCV(model, params, scoring=scoring, n_jobs=1, refit=refit_strategy, cv=getStratKFold(), verbose=2, pre_dispatch='2*n_jobs', error_score=0, return_train_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5790c5",
   "metadata": {},
   "source": [
    "## 2.2. Time-based Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e0945",
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime()\n",
    "catb_classifier = catboost.CatBoostClassifier(random_state=1, thread_count=os.cpu_count(), verbose=1, cat_features=getIndexes(tb_train), nan_mode='Min')\n",
    "tb_tuner = getTuner(catb_classifier, catb_params)\n",
    "tb_tuner.fit(X_tb, y_tb)\n",
    "endTime(\"TB_CATB\")\n",
    "\n",
    "printToFile(\"TB_CATB\", tb_tuner.best_params_)\n",
    "cv_results = pd.DataFrame.from_dict(tb_tuner.cv_results_)\n",
    "cv_results.to_csv('CATB_TB_Tuning_CV_Results.csv')\n",
    "plot_search_results(tb_tuner, cv_results)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da81325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_search_results(tb_tuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77915e92",
   "metadata": {},
   "source": [
    "## 2.3. Instance-based Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4527766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime()\n",
    "catb_classifier = catboost.CatBoostClassifier(random_state=1, thread_count=os.cpu_count(), verbose=0, cat_features=getIndexes(tb_train), nan_mode='Min')\n",
    "ib_tuner = getTuner(catb_classifier, catb_params)\n",
    "ib_tuner.fit(X_ib, y_ib)\n",
    "endTime(\"IB_CATB\")\n",
    "\n",
    "printToFile(\"IB_CATB\", tb_tuner.best_params_)\n",
    "cv_results = pd.DataFrame.from_dict(tb_tuner.cv_results_)\n",
    "cv_results.to_csv('CATB_IB_Tuning_CV_Results.csv')\n",
    "plot_search_results(tb_tuner, cv_results)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ac8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
