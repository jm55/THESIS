{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ejose\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import lightgbm as lgbm\n",
    "import catboost as catb\n",
    "import sklearn.svm as svc\n",
    "import sklearn.neural_network as mlpc\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv('./Dataset/oliveira_labelled.csv')\n",
    "\n",
    "API_LIST = \"./Dataset/api_calls.txt\"\n",
    "DELIMITER = \"NaN\"\n",
    "API_FILE = open(API_LIST,\"r\")\n",
    "APIS = API_FILE.readline().split(',')\n",
    "APIS.append(DELIMITER) #serves as a label for NaN values for Instance-based datasets\n",
    "API_FILE.close()\n",
    "\n",
    "#Inverse Label Encoding\n",
    "def inverse_label(item:str):\n",
    "    global APIS\n",
    "    return item.map(lambda x: APIS[int(x)])\n",
    "\n",
    "def list_to_str(ls:list):\n",
    "    '''Convert list to a stringified version (comma delimited).'''\n",
    "    output = \"\"\n",
    "    for l in ls:\n",
    "        output += str(l) + \",\"\n",
    "    return output[0:len(output)-1]\n",
    "\n",
    "def inject_patterns(inner_df:pd.DataFrame):\n",
    "    '''Injects the API call patterns of each sample as its last column'''\n",
    "    patterns = []\n",
    "    print(\"Injecting API patterns...\")\n",
    "    for row in range(inner_df.shape[0]):\n",
    "        patterns.append(list_to_str(inner_df.iloc[row,1:101].transpose().to_list()))\n",
    "    inner_df['pattern'] = patterns\n",
    "    return inner_df # DBSCAN requires only the numeric label encoded version of the API Calls\n",
    "\n",
    "def ib_convert(input_df:pd.DataFrame):\n",
    "    print(\"Transposing IB...\")\n",
    "    input_df.transpose()\n",
    "    print(\"IB Transposed!\")\n",
    "    print(\"Removing duplicates...\")\n",
    "    print(\"Row:\", end=\" \")\n",
    "    for r in range(input_df.shape[0]):\n",
    "        row = input_df.iloc[r, 1:101].drop_duplicates(keep='first', inplace=False).to_list()\n",
    "        input_df.iloc[r, 1:101] = row + ([307]*(100-len(row)))\n",
    "        if r % 100 == 0:\n",
    "            print(r, end=\" \")\n",
    "    print(\"\\nDuplicates removed!\")\n",
    "    print(\"Retransposing IB (revert)...\")\n",
    "    input_df.transpose()\n",
    "    print(\"IB Retransposed!\")\n",
    "    return input_df\n",
    "\n",
    "# Remove falsely labelled malicious samples\n",
    "df = df[df['type'] != '_']\n",
    "\n",
    "# Remove specific malware types\n",
    "# removables = ['ransomware', 'miner', 'virus', 'spyware', 'hacktool', 'dropper', 'worm']\n",
    "# for r in removables:\n",
    "#     df = df[df['type'] != r]\n",
    "\n",
    "#Remove type column\n",
    "type_col = df.pop('type')\n",
    "\n",
    "#Removing hash column\n",
    "hash_col = df.pop('hash')\n",
    "\n",
    "#Re-arranging column positions\n",
    "label_col = df.pop('malware')\n",
    "df = pd.concat([label_col, df], axis=1)\n",
    "df = pd.concat([df, hash_col], axis=1) # <=== This will be retained for the benefit of model evaluation.\n",
    "df = pd.concat([df, type_col], axis=1) # <=== This will be retained for the benefit of model evaluation.\n",
    "\n",
    "#df.iloc[:, 1:101] = df.iloc[:, 1:101].apply(inverse_label, axis=1, result_type='reduce')\n",
    "#df = inject_patterns(df)\n",
    "\n",
    "mal_df = df[df['malware'] == 1]\n",
    "ben_df = df[df['malware'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign for Training: 969\n",
      "Bening for Test:  108\n"
     ]
    }
   ],
   "source": [
    "X = ben_df.iloc[:,1:] #Features\n",
    "y = ben_df.iloc[:,0] #Labels\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.10, random_state=1, shuffle=True)\n",
    "\n",
    "ben_train = pd.concat([train_labels,train_features], axis=1)\n",
    "ben_test = pd.concat([test_labels,test_features], axis=1)\n",
    "\n",
    "print(\"Benign for Training:\", ben_train['type'].value_counts().sum())\n",
    "print(\"Bening for Test: \", ben_test['type'].value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malicious for Training: 3213\n",
      "Malicious for Testing: 36946\n"
     ]
    }
   ],
   "source": [
    "X = mal_df.iloc[:,1:] #Features\n",
    "y = mal_df.iloc[:,0] #Labels\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.08, random_state=1, shuffle=True)\n",
    "\n",
    "mal_test = pd.concat([train_labels,train_features], axis=1)\n",
    "mal_train = pd.concat([test_labels,test_features], axis=1)\n",
    "\n",
    "print(\"Malicious for Training:\", mal_train['type'].value_counts().sum())\n",
    "print(\"Malicious for Testing:\", mal_test['type'].value_counts().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mal_test + ben_test\n",
    "\n",
    "To not undergo SMOTETonek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37054, 103)\n",
      "malware\n",
      "1    36946\n",
      "0      108\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>malware</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>...</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>hash</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>ba21b9378d594b044470e1eb89e846db</td>\n",
       "      <td>trojan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>16</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>141</td>\n",
       "      <td>65</td>\n",
       "      <td>260</td>\n",
       "      <td>65</td>\n",
       "      <td>141</td>\n",
       "      <td>65</td>\n",
       "      <td>117</td>\n",
       "      <td>215</td>\n",
       "      <td>5d883b9aabe16c16c97c6e5d04b333e2</td>\n",
       "      <td>trojan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>215</td>\n",
       "      <td>117</td>\n",
       "      <td>208</td>\n",
       "      <td>117</td>\n",
       "      <td>208</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>260</td>\n",
       "      <td>65</td>\n",
       "      <td>141</td>\n",
       "      <td>65</td>\n",
       "      <td>260</td>\n",
       "      <td>65</td>\n",
       "      <td>141</td>\n",
       "      <td>23455429246e698971b4d9fdbe1ce2fd</td>\n",
       "      <td>trojan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>0f25b6e10708d379c09eb06bb01bb077</td>\n",
       "      <td>trojan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>16</td>\n",
       "      <td>81</td>\n",
       "      <td>252</td>\n",
       "      <td>81</td>\n",
       "      <td>208</td>\n",
       "      <td>257</td>\n",
       "      <td>...</td>\n",
       "      <td>303</td>\n",
       "      <td>39</td>\n",
       "      <td>303</td>\n",
       "      <td>39</td>\n",
       "      <td>303</td>\n",
       "      <td>39</td>\n",
       "      <td>303</td>\n",
       "      <td>39</td>\n",
       "      <td>d42963113be901a2fd140eb2f505fc73</td>\n",
       "      <td>trojan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   malware  t_0  t_1  t_2  t_3  t_4  t_5  t_6  t_7  t_8  ...  t_92  t_93  \\\n",
       "0        1   82  240  117  240  117  240  117  240  117  ...   260   141   \n",
       "1        1   82  172  117   16  240  117  240  117   99  ...   141    65   \n",
       "2        1  215  117  208  117  208  117  240  117  240  ...   117   260   \n",
       "3        1   82  240  117  240  117  240  117  240  117  ...   260   141   \n",
       "4        1   82  172  117   16   81  252   81  208  257  ...   303    39   \n",
       "\n",
       "   t_94  t_95  t_96  t_97  t_98  t_99                              hash  \\\n",
       "0   260   141   260   141   260   141  ba21b9378d594b044470e1eb89e846db   \n",
       "1   260    65   141    65   117   215  5d883b9aabe16c16c97c6e5d04b333e2   \n",
       "2    65   141    65   260    65   141  23455429246e698971b4d9fdbe1ce2fd   \n",
       "3   260   141   260   141   260   141  0f25b6e10708d379c09eb06bb01bb077   \n",
       "4   303    39   303    39   303    39  d42963113be901a2fd140eb2f505fc73   \n",
       "\n",
       "     type  \n",
       "0  trojan  \n",
       "1  trojan  \n",
       "2  trojan  \n",
       "3  trojan  \n",
       "4  trojan  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>malware</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>...</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>hash</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36946</th>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>16</td>\n",
       "      <td>208</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>...</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>59147b8b8abf9768ca96badfd91d7bb9</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36947</th>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>35</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>208</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>483b022e6f2805d0cdf4e1db7d1237af</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36948</th>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "      <td>76457240c1640a0812a3ef57159708b4</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36949</th>\n",
       "      <td>0</td>\n",
       "      <td>286</td>\n",
       "      <td>110</td>\n",
       "      <td>172</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>114</td>\n",
       "      <td>215</td>\n",
       "      <td>117</td>\n",
       "      <td>71</td>\n",
       "      <td>25</td>\n",
       "      <td>71</td>\n",
       "      <td>275</td>\n",
       "      <td>260</td>\n",
       "      <td>25a904a73a9c6548c39351f3bbfac641</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36950</th>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>86</td>\n",
       "      <td>208</td>\n",
       "      <td>86</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>25</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>25</td>\n",
       "      <td>60</td>\n",
       "      <td>39dfc1401b7db273933b5fb08e8394f8</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37049</th>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>274</td>\n",
       "      <td>215</td>\n",
       "      <td>106</td>\n",
       "      <td>171</td>\n",
       "      <td>...</td>\n",
       "      <td>89</td>\n",
       "      <td>215</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>46691ecd93d1ba38de8eb68ab281603e</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37050</th>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>16</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>194</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>215</td>\n",
       "      <td>187</td>\n",
       "      <td>215</td>\n",
       "      <td>50</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>283</td>\n",
       "      <td>824e84ac88ac9f82d772960657e094d1</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37051</th>\n",
       "      <td>0</td>\n",
       "      <td>297</td>\n",
       "      <td>8</td>\n",
       "      <td>135</td>\n",
       "      <td>215</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>208</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>215</td>\n",
       "      <td>208</td>\n",
       "      <td>297</td>\n",
       "      <td>93</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>215</td>\n",
       "      <td>9b7a7f7e6df8ae601c75adb56f0ba994</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37052</th>\n",
       "      <td>0</td>\n",
       "      <td>286</td>\n",
       "      <td>110</td>\n",
       "      <td>172</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>114</td>\n",
       "      <td>215</td>\n",
       "      <td>117</td>\n",
       "      <td>261</td>\n",
       "      <td>106</td>\n",
       "      <td>144</td>\n",
       "      <td>297</td>\n",
       "      <td>117</td>\n",
       "      <td>223d7689bbf3fbf0dc2ead33ad704689</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37053</th>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>228</td>\n",
       "      <td>16</td>\n",
       "      <td>172</td>\n",
       "      <td>208</td>\n",
       "      <td>215</td>\n",
       "      <td>89</td>\n",
       "      <td>215</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>da2ed5d190fd57188abbb44e0f591f80</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       malware  t_0  t_1  t_2  t_3  t_4  t_5  t_6  t_7  t_8  ...  t_92  t_93  \\\n",
       "36946        0   82   16  208  240  117  215  274  158  215  ...   172   117   \n",
       "36947        0  240  117  240  117  240  117  240  117  240  ...   117    35   \n",
       "36948        0  240  117  240  117  240  117  240  117  240  ...   172   117   \n",
       "36949        0  286  110  172  240  117  240  117  240  117  ...   114   215   \n",
       "36950        0   82   16   35  240  117   86  208   86   31  ...    25    60   \n",
       "...        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "37049        0   82   16   31  172  117  274  215  106  171  ...    89   215   \n",
       "37050        0   82   16  172  117  194  240  117  172  117  ...   215   187   \n",
       "37051        0  297    8  135  215  171  215  172  117  208  ...   117   215   \n",
       "37052        0  286  110  172  240  117  240  117  240  117  ...   114   215   \n",
       "37053        0   82  228   16  172  208  215   89  215  172  ...   117   172   \n",
       "\n",
       "       t_94  t_95  t_96  t_97  t_98  t_99                              hash  \\\n",
       "36946   172   117   172   117   172   117  59147b8b8abf9768ca96badfd91d7bb9   \n",
       "36947    60    81   208    60    81    60  483b022e6f2805d0cdf4e1db7d1237af   \n",
       "36948   240   117   172   117    29    25  76457240c1640a0812a3ef57159708b4   \n",
       "36949   117    71    25    71   275   260  25a904a73a9c6548c39351f3bbfac641   \n",
       "36950    81    25    60    81    25    60  39dfc1401b7db273933b5fb08e8394f8   \n",
       "...     ...   ...   ...   ...   ...   ...                               ...   \n",
       "37049   172   117   172   117   172   117  46691ecd93d1ba38de8eb68ab281603e   \n",
       "37050   215    50   274   158   215   283  824e84ac88ac9f82d772960657e094d1   \n",
       "37051   208   297    93   240   117   215  9b7a7f7e6df8ae601c75adb56f0ba994   \n",
       "37052   117   261   106   144   297   117  223d7689bbf3fbf0dc2ead33ad704689   \n",
       "37053   117   172   117   172   117   172  da2ed5d190fd57188abbb44e0f591f80   \n",
       "\n",
       "         type  \n",
       "36946  benign  \n",
       "36947  benign  \n",
       "36948  benign  \n",
       "36949  benign  \n",
       "36950  benign  \n",
       "...       ...  \n",
       "37049  benign  \n",
       "37050  benign  \n",
       "37051  benign  \n",
       "37052  benign  \n",
       "37053  benign  \n",
       "\n",
       "[108 rows x 103 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_tb = pd.concat([mal_test, ben_test], axis=0, ignore_index=True)\n",
    "print(test_tb.shape)\n",
    "print(test_tb['malware'].value_counts())\n",
    "display(test_tb.head())\n",
    "display(test_tb[test_tb['malware']==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mal_train + ben_train\n",
    "\n",
    "To undergo *SMOTETomek*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4182, 103)\n",
      "malware\n",
      "1    3213\n",
      "0     969\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_tb = pd.concat([mal_train, ben_train], axis=0, ignore_index=True)\n",
    "print(train_tb.shape)\n",
    "print(train_tb['malware'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4484, 101)\n",
      "malware\n",
      "1    3206\n",
      "0    1278\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>malware</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>...</th>\n",
       "      <th>t_90</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>93</td>\n",
       "      <td>208</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>215</td>\n",
       "      <td>108</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>286</td>\n",
       "      <td>110</td>\n",
       "      <td>172</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>202</td>\n",
       "      <td>65</td>\n",
       "      <td>117</td>\n",
       "      <td>260</td>\n",
       "      <td>297</td>\n",
       "      <td>215</td>\n",
       "      <td>114</td>\n",
       "      <td>215</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>208</td>\n",
       "      <td>228</td>\n",
       "      <td>117</td>\n",
       "      <td>228</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>261</td>\n",
       "      <td>208</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>260</td>\n",
       "      <td>40</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>40</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   malware  t_0  t_1  t_2  t_3  t_4  t_5  t_6  t_7  t_8  ...  t_90  t_91  \\\n",
       "0        1   82  240  117  240  117  240  117  240  117  ...   172   117   \n",
       "1        1  286  110  172  240  117  240  117  240  117  ...    65   202   \n",
       "2        1   82  172  117   16   29  208  228  117  228  ...   117   172   \n",
       "3        1   82  240  117  240  117  240  117  240  117  ...   261   208   \n",
       "4        1   82  240  117  240  117  240  117  240  117  ...   260   141   \n",
       "\n",
       "   t_92  t_93  t_94  t_95  t_96  t_97  t_98  t_99  \n",
       "0    93   208    16    31   215   108   215    35  \n",
       "1    65   117   260   297   215   114   215    71  \n",
       "2   117   172   117   172   117   172   117   172  \n",
       "3   240   117   260    40   209   260    40   209  \n",
       "4   260   141   260   141   260   141   260   141  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>malware</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>...</th>\n",
       "      <th>t_90</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3206</th>\n",
       "      <td>0</td>\n",
       "      <td>286</td>\n",
       "      <td>110</td>\n",
       "      <td>172</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>297</td>\n",
       "      <td>215</td>\n",
       "      <td>114</td>\n",
       "      <td>215</td>\n",
       "      <td>117</td>\n",
       "      <td>261</td>\n",
       "      <td>106</td>\n",
       "      <td>144</td>\n",
       "      <td>297</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3207</th>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>86</td>\n",
       "      <td>82</td>\n",
       "      <td>37</td>\n",
       "      <td>70</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>297</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3208</th>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3209</th>\n",
       "      <td>0</td>\n",
       "      <td>286</td>\n",
       "      <td>110</td>\n",
       "      <td>172</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>297</td>\n",
       "      <td>215</td>\n",
       "      <td>114</td>\n",
       "      <td>215</td>\n",
       "      <td>117</td>\n",
       "      <td>261</td>\n",
       "      <td>106</td>\n",
       "      <td>144</td>\n",
       "      <td>297</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3210</th>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>82</td>\n",
       "      <td>16</td>\n",
       "      <td>240</td>\n",
       "      <td>194</td>\n",
       "      <td>31</td>\n",
       "      <td>208</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>252</td>\n",
       "      <td>112</td>\n",
       "      <td>20</td>\n",
       "      <td>215</td>\n",
       "      <td>297</td>\n",
       "      <td>35</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>0</td>\n",
       "      <td>286</td>\n",
       "      <td>110</td>\n",
       "      <td>172</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>297</td>\n",
       "      <td>215</td>\n",
       "      <td>114</td>\n",
       "      <td>215</td>\n",
       "      <td>117</td>\n",
       "      <td>261</td>\n",
       "      <td>106</td>\n",
       "      <td>144</td>\n",
       "      <td>297</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4480</th>\n",
       "      <td>0</td>\n",
       "      <td>221</td>\n",
       "      <td>117</td>\n",
       "      <td>221</td>\n",
       "      <td>117</td>\n",
       "      <td>221</td>\n",
       "      <td>117</td>\n",
       "      <td>228</td>\n",
       "      <td>141</td>\n",
       "      <td>245</td>\n",
       "      <td>...</td>\n",
       "      <td>157</td>\n",
       "      <td>135</td>\n",
       "      <td>181</td>\n",
       "      <td>135</td>\n",
       "      <td>173</td>\n",
       "      <td>143</td>\n",
       "      <td>177</td>\n",
       "      <td>124</td>\n",
       "      <td>177</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4481</th>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>16</td>\n",
       "      <td>274</td>\n",
       "      <td>41</td>\n",
       "      <td>178</td>\n",
       "      <td>242</td>\n",
       "      <td>60</td>\n",
       "      <td>216</td>\n",
       "      <td>208</td>\n",
       "      <td>...</td>\n",
       "      <td>124</td>\n",
       "      <td>156</td>\n",
       "      <td>102</td>\n",
       "      <td>148</td>\n",
       "      <td>102</td>\n",
       "      <td>148</td>\n",
       "      <td>115</td>\n",
       "      <td>143</td>\n",
       "      <td>115</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4482</th>\n",
       "      <td>0</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>94</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>187</td>\n",
       "      <td>72</td>\n",
       "      <td>101</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4483</th>\n",
       "      <td>0</td>\n",
       "      <td>286</td>\n",
       "      <td>110</td>\n",
       "      <td>172</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>297</td>\n",
       "      <td>215</td>\n",
       "      <td>114</td>\n",
       "      <td>215</td>\n",
       "      <td>117</td>\n",
       "      <td>261</td>\n",
       "      <td>106</td>\n",
       "      <td>144</td>\n",
       "      <td>297</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1278 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      malware  t_0  t_1  t_2  t_3  t_4  t_5  t_6  t_7  t_8  ...  t_90  t_91  \\\n",
       "3206        0  286  110  172  240  117  240  117  240  117  ...   297   215   \n",
       "3207        0   82  274  158  215   86   82   37   70   37  ...   158   215   \n",
       "3208        0  240  117  240  117  240  117  240  117  240  ...   158   215   \n",
       "3209        0  286  110  172  240  117  240  117  240  117  ...   297   215   \n",
       "3210        0   82  172  117   82   16  240  194   31  208  ...   144   252   \n",
       "...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "4479        0  286  110  172  240  117  240  117  240  117  ...   297   215   \n",
       "4480        0  221  117  221  117  221  117  228  141  245  ...   157   135   \n",
       "4481        0   82   16  274   41  178  242   60  216  208  ...   124   156   \n",
       "4482        0  215  274  158  215  274  158  215  172  117  ...    94   240   \n",
       "4483        0  286  110  172  240  117  240  117  240  117  ...   297   215   \n",
       "\n",
       "      t_92  t_93  t_94  t_95  t_96  t_97  t_98  t_99  \n",
       "3206   114   215   117   261   106   144   297   117  \n",
       "3207    82   240   117    82   240   117   297     8  \n",
       "3208   274   158   215   274   158   215   274   158  \n",
       "3209   114   215   117   261   106   144   297    71  \n",
       "3210   112    20   215   297    35   240   117    35  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "4479   114   215   117   261   106   144   297   117  \n",
       "4480   181   135   173   143   177   124   177   165  \n",
       "4481   102   148   102   148   115   143   115   157  \n",
       "4482   117   240   117   187    72   101    60    81  \n",
       "4483   114   215   117   261   106   144   297   117  \n",
       "\n",
       "[1278 rows x 101 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "smt = SMOTETomek(random_state=1, n_jobs=8, sampling_strategy=0.4)\n",
    "\n",
    "X = train_tb.iloc[:,1:101]\n",
    "y = train_tb.iloc[:,0]\n",
    "\n",
    "X_res, y_res = smt.fit_resample(X, y)\n",
    "train_tb = pd.concat([y_res,X_res], axis=1)\n",
    "print(train_tb.shape)\n",
    "print(train_tb['malware'].value_counts())\n",
    "display(train_tb.head())\n",
    "display(train_tb[train_tb['malware']==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating IB versions of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposing IB...\n",
      "IB Transposed!\n",
      "Removing duplicates...\n",
      "Row: 0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 3400 3500 3600 3700 3800 3900 4000 4100 4200 4300 4400 \n",
      "Duplicates removed!\n",
      "Retransposing IB (revert)...\n",
      "IB Retransposed!\n",
      "\n",
      "\n",
      "Transposing IB...\n",
      "IB Transposed!\n",
      "Removing duplicates...\n",
      "Row: 0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 3400 3500 3600 3700 3800 3900 4000 4100 4200 4300 4400 4500 4600 4700 4800 4900 5000 5100 5200 5300 5400 5500 5600 5700 5800 5900 6000 6100 6200 6300 6400 6500 6600 6700 6800 6900 7000 7100 7200 7300 7400 7500 7600 7700 7800 7900 8000 8100 8200 8300 8400 8500 8600 8700 8800 8900 9000 9100 9200 9300 9400 9500 9600 9700 9800 9900 10000 10100 10200 10300 10400 10500 10600 10700 10800 10900 11000 11100 11200 11300 11400 11500 11600 11700 11800 11900 12000 12100 12200 12300 12400 12500 12600 12700 12800 12900 13000 13100 13200 13300 13400 13500 13600 13700 13800 13900 14000 14100 14200 14300 14400 14500 14600 14700 14800 14900 15000 15100 15200 15300 15400 15500 15600 15700 15800 15900 16000 16100 16200 16300 16400 16500 16600 16700 16800 16900 17000 17100 17200 17300 17400 17500 17600 17700 17800 17900 18000 18100 18200 18300 18400 18500 18600 18700 18800 18900 19000 19100 19200 19300 19400 19500 19600 19700 19800 19900 20000 20100 20200 20300 20400 20500 20600 20700 20800 20900 21000 21100 21200 21300 21400 21500 21600 21700 21800 21900 22000 22100 22200 22300 22400 22500 22600 22700 22800 22900 23000 23100 23200 23300 23400 23500 23600 23700 23800 23900 24000 24100 24200 24300 24400 24500 24600 24700 24800 24900 25000 25100 25200 25300 25400 25500 25600 25700 25800 25900 26000 26100 26200 26300 26400 26500 26600 26700 26800 26900 27000 27100 27200 27300 27400 27500 27600 27700 27800 27900 28000 28100 28200 28300 28400 28500 28600 28700 28800 28900 29000 29100 29200 29300 29400 29500 29600 29700 29800 29900 30000 30100 30200 30300 30400 30500 30600 30700 30800 30900 31000 31100 31200 31300 31400 31500 31600 31700 31800 31900 32000 32100 32200 32300 32400 32500 32600 32700 32800 32900 33000 33100 33200 33300 33400 33500 33600 33700 33800 33900 34000 34100 34200 34300 34400 34500 34600 34700 34800 34900 35000 35100 35200 35300 35400 35500 35600 35700 35800 35900 36000 36100 36200 36300 36400 36500 36600 36700 36800 36900 37000 \n",
      "Duplicates removed!\n",
      "Retransposing IB (revert)...\n",
      "IB Retransposed!\n"
     ]
    }
   ],
   "source": [
    "train_ib = train_tb.copy(deep=True)\n",
    "test_ib = test_tb.copy(deep=True)\n",
    "\n",
    "train_ib = ib_convert(train_ib).copy(deep=True)\n",
    "print(\"\\n\")\n",
    "test_ib = ib_convert(test_ib).copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ib.iloc[:,1:101] = train_ib.iloc[:,1:101].astype('str')\n",
    "# train_ib.replace(\"nan\", \"NaN\", inplace=True)\n",
    "# test_ib.iloc[:,1:101] = test_ib.iloc[:,1:101].astype('str')\n",
    "# test_ib.replace(\"nan\", \"NaN\", inplace=True)\n",
    "# display(train_ib.head())\n",
    "# display(test_ib.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting results to usable for models\n",
    "\n",
    "Encoded APIs to Unencoded/String APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_tb_enc = train_tb.copy(deep=True)\n",
    "# test_tb_enc = test_tb.copy(deep=True)\n",
    "# train_ib_enc = train_ib.copy(deep=True)\n",
    "# test_ib_enc = test_ib.copy(deep=True)\n",
    "\n",
    "# train_tb_enc.iloc[:, 1:101] = train_tb.iloc[:, 1:101].apply(inverse_label, axis=1, result_type='reduce')\n",
    "# test_tb_enc.iloc[:, 1:101] = test_tb.iloc[:, 1:101].apply(inverse_label, axis=1, result_type='reduce')\n",
    "# train_ib_enc.iloc[:, 1:101] = train_ib.iloc[:, 1:101].apply(inverse_label, axis=1, result_type='reduce')\n",
    "# test_ib_enc.iloc[:, 1:101] = test_ib.iloc[:, 1:101].apply(inverse_label, axis=1, result_type='reduce')\n",
    "\n",
    "# display(train_tb_enc.head())\n",
    "# display(test_tb_enc.head())\n",
    "# display(train_ib_enc.head())\n",
    "# display(test_ib_enc.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying it out on LightGBM, CatBoost, and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indexes():\n",
    "    indexes = []\n",
    "    for i in range(100):\n",
    "        indexes.append(f\"t_{i}\")\n",
    "    return indexes\n",
    "\n",
    "def train_test(train, test, model, model_str:str):\n",
    "    X = train.iloc[:,1:101]\n",
    "    y = train.iloc[:,0]\n",
    "    X_test = test.iloc[:,1:101]\n",
    "    y_test = test.iloc[:,0]\n",
    "    print(f\"Model: {model_str}\")\n",
    "    #MODEL ROBUSTNESS\n",
    "    model.fit(X,y)\n",
    "    y_pred = model.predict(X_test)\n",
    "    #print(\"\\nModel, Fold, Accuracy, Precision, F1-Score, Recall, ROC-AUC\")\n",
    "    #print(f\"{model_str}, {'T'}, {metrics.accuracy_score(y_test, y_pred):.4f}, {metrics.average_precision_score(y_test, y_pred):.4f}, {metrics.f1_score(y_test, y_pred):.4f}, {metrics.recall_score(y_test, y_pred):.4f}, {metrics.roc_auc_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Fold: T\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    print(f\"ROC-AUC: {metrics.roc_auc_score(y_test, y_pred):.4f}\")\n",
    "    #STRATIFIED K-FOLDS\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
    "    ctr = 0\n",
    "    for train_idx, test_idx in skf.split(train.iloc[:,1:101], train.iloc[:,0]):\n",
    "        X_train = train.iloc[train_idx, 1:101]\n",
    "        y_train = train.iloc[train_idx, 0]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(train.iloc[test_idx, 1:101])\n",
    "        y_test = train.iloc[test_idx, 0]\n",
    "        #print(f\"{model_str}, {ctr}, {metrics.accuracy_score(y_test, y_pred):.4f}, {metrics.average_precision_score(y_test, y_pred):.4f}, {metrics.f1_score(y_test, y_pred):.4f}, {metrics.recall_score(y_test, y_pred):.4f}, {metrics.roc_auc_score(y_test, y_pred):.4f}\")\n",
    "        print(f\"Fold: {ctr}\")\n",
    "        print(classification_report(y_test, y_pred, digits=4))\n",
    "        ctr += 1\n",
    "    print('-------------------------------------------------------')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LGBM TB\n",
      "Fold: T\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0952    0.8333    0.1709       108\n",
      "           1     0.9995    0.9769    0.9881     36946\n",
      "\n",
      "    accuracy                         0.9764     37054\n",
      "   macro avg     0.5474    0.9051    0.5795     37054\n",
      "weighted avg     0.9969    0.9764    0.9857     37054\n",
      "\n",
      "ROC-AUC: 0.9051\n",
      "Fold: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9342    0.8353    0.8820       255\n",
      "           1     0.9372    0.9766    0.9565       642\n",
      "\n",
      "    accuracy                         0.9365       897\n",
      "   macro avg     0.9357    0.9060    0.9193       897\n",
      "weighted avg     0.9364    0.9365    0.9353       897\n",
      "\n",
      "Fold: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9076    0.8828    0.8950       256\n",
      "           1     0.9537    0.9641    0.9589       641\n",
      "\n",
      "    accuracy                         0.9409       897\n",
      "   macro avg     0.9307    0.9235    0.9270       897\n",
      "weighted avg     0.9406    0.9409    0.9407       897\n",
      "\n",
      "Fold: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9160    0.8945    0.9051       256\n",
      "           1     0.9583    0.9672    0.9627       641\n",
      "\n",
      "    accuracy                         0.9465       897\n",
      "   macro avg     0.9371    0.9309    0.9339       897\n",
      "weighted avg     0.9462    0.9465    0.9463       897\n",
      "\n",
      "Fold: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9048    0.8906    0.8976       256\n",
      "           1     0.9566    0.9626    0.9596       641\n",
      "\n",
      "    accuracy                         0.9420       897\n",
      "   macro avg     0.9307    0.9266    0.9286       897\n",
      "weighted avg     0.9418    0.9420    0.9419       897\n",
      "\n",
      "Fold: 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9303    0.8902    0.9098       255\n",
      "           1     0.9571    0.9735    0.9652       641\n",
      "\n",
      "    accuracy                         0.9498       896\n",
      "   macro avg     0.9437    0.9318    0.9375       896\n",
      "weighted avg     0.9494    0.9498    0.9494       896\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "Model: CATB TB\n",
      "Fold: T\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1086    0.7870    0.1908       108\n",
      "           1     0.9994    0.9811    0.9902     36946\n",
      "\n",
      "    accuracy                         0.9805     37054\n",
      "   macro avg     0.5540    0.8841    0.5905     37054\n",
      "weighted avg     0.9968    0.9805    0.9878     37054\n",
      "\n",
      "ROC-AUC: 0.8841\n",
      "Fold: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9220    0.7412    0.8217       255\n",
      "           1     0.9046    0.9751    0.9385       642\n",
      "\n",
      "    accuracy                         0.9086       897\n",
      "   macro avg     0.9133    0.8581    0.8801       897\n",
      "weighted avg     0.9095    0.9086    0.9053       897\n",
      "\n",
      "Fold: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9414    0.8164    0.8745       256\n",
      "           1     0.9304    0.9797    0.9544       641\n",
      "\n",
      "    accuracy                         0.9331       897\n",
      "   macro avg     0.9359    0.8981    0.9144       897\n",
      "weighted avg     0.9335    0.9331    0.9316       897\n",
      "\n",
      "Fold: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9083    0.8125    0.8577       256\n",
      "           1     0.9281    0.9672    0.9473       641\n",
      "\n",
      "    accuracy                         0.9231       897\n",
      "   macro avg     0.9182    0.8899    0.9025       897\n",
      "weighted avg     0.9225    0.9231    0.9217       897\n",
      "\n",
      "Fold: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9283    0.8086    0.8643       256\n",
      "           1     0.9273    0.9750    0.9506       641\n",
      "\n",
      "    accuracy                         0.9275       897\n",
      "   macro avg     0.9278    0.8918    0.9074       897\n",
      "weighted avg     0.9276    0.9275    0.9259       897\n",
      "\n",
      "Fold: 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9452    0.8118    0.8734       255\n",
      "           1     0.9291    0.9813    0.9545       641\n",
      "\n",
      "    accuracy                         0.9330       896\n",
      "   macro avg     0.9372    0.8965    0.9139       896\n",
      "weighted avg     0.9337    0.9330    0.9314       896\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_test(train_tb, test_tb, lgbm.LGBMClassifier(random_state=1, n_jobs=0, verbose=0), \"LGBM TB\")\n",
    "train_test(train_tb, test_tb, catb.CatBoostClassifier(random_state=1, thread_count=-1, verbose=0, cat_features=get_indexes(), \n",
    "                                                    nan_mode='Min', custom_metric=['Logloss', 'AUC', 'Precision'], one_hot_max_size=256), \"CATB TB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LGBM IB\n",
      "Fold: T\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0706    0.8148    0.1300       108\n",
      "           1     0.9994    0.9687    0.9838     36946\n",
      "\n",
      "    accuracy                         0.9682     37054\n",
      "   macro avg     0.5350    0.8917    0.5569     37054\n",
      "weighted avg     0.9967    0.9682    0.9813     37054\n",
      "\n",
      "ROC-AUC: 0.8917\n",
      "Fold: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9359    0.8588    0.8957       255\n",
      "           1     0.9457    0.9766    0.9609       642\n",
      "\n",
      "    accuracy                         0.9431       897\n",
      "   macro avg     0.9408    0.9177    0.9283       897\n",
      "weighted avg     0.9429    0.9431    0.9424       897\n",
      "\n",
      "Fold: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9012    0.8555    0.8778       256\n",
      "           1     0.9434    0.9626    0.9529       641\n",
      "\n",
      "    accuracy                         0.9320       897\n",
      "   macro avg     0.9223    0.9090    0.9153       897\n",
      "weighted avg     0.9314    0.9320    0.9315       897\n",
      "\n",
      "Fold: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9309    0.8945    0.9124       256\n",
      "           1     0.9585    0.9735    0.9659       641\n",
      "\n",
      "    accuracy                         0.9509       897\n",
      "   macro avg     0.9447    0.9340    0.9391       897\n",
      "weighted avg     0.9506    0.9509    0.9506       897\n",
      "\n",
      "Fold: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8726    0.8828    0.8777       256\n",
      "           1     0.9530    0.9485    0.9507       641\n",
      "\n",
      "    accuracy                         0.9298       897\n",
      "   macro avg     0.9128    0.9157    0.9142       897\n",
      "weighted avg     0.9300    0.9298    0.9299       897\n",
      "\n",
      "Fold: 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9367    0.8706    0.9024       255\n",
      "           1     0.9499    0.9766    0.9631       641\n",
      "\n",
      "    accuracy                         0.9464       896\n",
      "   macro avg     0.9433    0.9236    0.9328       896\n",
      "weighted avg     0.9462    0.9464    0.9458       896\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "Model: CATB IB\n",
      "Fold: T\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1117    0.7870    0.1956       108\n",
      "           1     0.9994    0.9817    0.9905     36946\n",
      "\n",
      "    accuracy                         0.9811     37054\n",
      "   macro avg     0.5555    0.8844    0.5930     37054\n",
      "weighted avg     0.9968    0.9811    0.9881     37054\n",
      "\n",
      "ROC-AUC: 0.8844\n",
      "Fold: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9526    0.7882    0.8627       255\n",
      "           1     0.9213    0.9844    0.9518       642\n",
      "\n",
      "    accuracy                         0.9287       897\n",
      "   macro avg     0.9369    0.8863    0.9072       897\n",
      "weighted avg     0.9302    0.9287    0.9265       897\n",
      "\n",
      "Fold: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9442    0.7930    0.8620       256\n",
      "           1     0.9223    0.9813    0.9509       641\n",
      "\n",
      "    accuracy                         0.9275       897\n",
      "   macro avg     0.9332    0.8871    0.9064       897\n",
      "weighted avg     0.9285    0.9275    0.9255       897\n",
      "\n",
      "Fold: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9353    0.8477    0.8893       256\n",
      "           1     0.9414    0.9766    0.9587       641\n",
      "\n",
      "    accuracy                         0.9398       897\n",
      "   macro avg     0.9383    0.9121    0.9240       897\n",
      "weighted avg     0.9396    0.9398    0.9389       897\n",
      "\n",
      "Fold: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9087    0.8555    0.8813       256\n",
      "           1     0.9436    0.9657    0.9545       641\n",
      "\n",
      "    accuracy                         0.9342       897\n",
      "   macro avg     0.9262    0.9106    0.9179       897\n",
      "weighted avg     0.9336    0.9342    0.9336       897\n",
      "\n",
      "Fold: 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9495    0.8118    0.8753       255\n",
      "           1     0.9292    0.9828    0.9553       641\n",
      "\n",
      "    accuracy                         0.9342       896\n",
      "   macro avg     0.9394    0.8973    0.9153       896\n",
      "weighted avg     0.9350    0.9342    0.9325       896\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_test(train_ib, test_ib, lgbm.LGBMClassifier(random_state=1, n_jobs=0,verbose=0), \"LGBM IB\")\n",
    "train_test(train_ib, test_ib, catb.CatBoostClassifier(random_state=1, thread_count=-1, verbose=0, cat_features=get_indexes(), \n",
    "                                                    nan_mode='Min', custom_metric=['Logloss', 'AUC', 'Precision'], one_hot_max_size=256), \"CATB IB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SVM TB\n",
      "Fold: T\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0731    0.8056    0.1341       108\n",
      "           1     0.9994    0.9701    0.9846     36946\n",
      "\n",
      "    accuracy                         0.9697     37054\n",
      "   macro avg     0.5363    0.8879    0.5593     37054\n",
      "weighted avg     0.9967    0.9697    0.9821     37054\n",
      "\n",
      "ROC-AUC: 0.8879\n",
      "Fold: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9067    0.8000    0.8500       255\n",
      "           1     0.9241    0.9673    0.9452       642\n",
      "\n",
      "    accuracy                         0.9197       897\n",
      "   macro avg     0.9154    0.8836    0.8976       897\n",
      "weighted avg     0.9191    0.9197    0.9181       897\n",
      "\n",
      "Fold: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9016    0.8594    0.8800       256\n",
      "           1     0.9449    0.9626    0.9536       641\n",
      "\n",
      "    accuracy                         0.9331       897\n",
      "   macro avg     0.9233    0.9110    0.9168       897\n",
      "weighted avg     0.9325    0.9331    0.9326       897\n",
      "\n",
      "Fold: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9076    0.8828    0.8950       256\n",
      "           1     0.9537    0.9641    0.9589       641\n",
      "\n",
      "    accuracy                         0.9409       897\n",
      "   macro avg     0.9307    0.9235    0.9270       897\n",
      "weighted avg     0.9406    0.9409    0.9407       897\n",
      "\n",
      "Fold: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8956    0.8711    0.8832       256\n",
      "           1     0.9491    0.9594    0.9542       641\n",
      "\n",
      "    accuracy                         0.9342       897\n",
      "   macro avg     0.9223    0.9153    0.9187       897\n",
      "weighted avg     0.9338    0.9342    0.9339       897\n",
      "\n",
      "Fold: 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9394    0.8510    0.8930       255\n",
      "           1     0.9429    0.9782    0.9602       641\n",
      "\n",
      "    accuracy                         0.9420       896\n",
      "   macro avg     0.9411    0.9146    0.9266       896\n",
      "weighted avg     0.9419    0.9420    0.9411       896\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "Model: MLPC TB\n",
      "Fold: T\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0312    0.8611    0.0602       108\n",
      "           1     0.9996    0.9218    0.9591     36946\n",
      "\n",
      "    accuracy                         0.9216     37054\n",
      "   macro avg     0.5154    0.8914    0.5096     37054\n",
      "weighted avg     0.9967    0.9216    0.9565     37054\n",
      "\n",
      "ROC-AUC: 0.8914\n",
      "Fold: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8282    0.8510    0.8395       255\n",
      "           1     0.9402    0.9299    0.9350       642\n",
      "\n",
      "    accuracy                         0.9075       897\n",
      "   macro avg     0.8842    0.8904    0.8872       897\n",
      "weighted avg     0.9083    0.9075    0.9078       897\n",
      "\n",
      "Fold: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8201    0.8906    0.8539       256\n",
      "           1     0.9548    0.9220    0.9381       641\n",
      "\n",
      "    accuracy                         0.9130       897\n",
      "   macro avg     0.8875    0.9063    0.8960       897\n",
      "weighted avg     0.9163    0.9130    0.9141       897\n",
      "\n",
      "Fold: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8303    0.8789    0.8539       256\n",
      "           1     0.9505    0.9282    0.9392       641\n",
      "\n",
      "    accuracy                         0.9142       897\n",
      "   macro avg     0.8904    0.9036    0.8966       897\n",
      "weighted avg     0.9162    0.9142    0.9149       897\n",
      "\n",
      "Fold: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8388    0.7930    0.8153       256\n",
      "           1     0.9191    0.9392    0.9290       641\n",
      "\n",
      "    accuracy                         0.8974       897\n",
      "   macro avg     0.8790    0.8661    0.8721       897\n",
      "weighted avg     0.8962    0.8974    0.8965       897\n",
      "\n",
      "Fold: 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8092    0.8314    0.8201       255\n",
      "           1     0.9322    0.9220    0.9271       641\n",
      "\n",
      "    accuracy                         0.8962       896\n",
      "   macro avg     0.8707    0.8767    0.8736       896\n",
      "weighted avg     0.8972    0.8962    0.8966       896\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_test(train_tb, test_tb, svc.SVC(random_state=1, verbose=0, cache_size=1024), \"SVM TB\")\n",
    "train_test(train_tb, test_tb, mlpc.MLPClassifier(random_state=1, verbose=0), \"MLPC TB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SVM IB\n",
      "[LibSVM]Fold: T\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0529    0.7222    0.0986       108\n",
      "           1     0.9992    0.9622    0.9803     36946\n",
      "\n",
      "    accuracy                         0.9615     37054\n",
      "   macro avg     0.5260    0.8422    0.5395     37054\n",
      "weighted avg     0.9964    0.9615    0.9778     37054\n",
      "\n",
      "ROC-AUC: 0.8422\n",
      "[LibSVM]Fold: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9034    0.7333    0.8095       255\n",
      "           1     0.9014    0.9688    0.9339       642\n",
      "\n",
      "    accuracy                         0.9019       897\n",
      "   macro avg     0.9024    0.8511    0.8717       897\n",
      "weighted avg     0.9020    0.9019    0.8986       897\n",
      "\n",
      "[LibSVM]Fold: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8929    0.7812    0.8333       256\n",
      "           1     0.9168    0.9626    0.9391       641\n",
      "\n",
      "    accuracy                         0.9108       897\n",
      "   macro avg     0.9048    0.8719    0.8862       897\n",
      "weighted avg     0.9100    0.9108    0.9089       897\n",
      "\n",
      "[LibSVM]Fold: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8889    0.7812    0.8316       256\n",
      "           1     0.9167    0.9610    0.9383       641\n",
      "\n",
      "    accuracy                         0.9097       897\n",
      "   macro avg     0.9028    0.8711    0.8850       897\n",
      "weighted avg     0.9087    0.9097    0.9079       897\n",
      "\n",
      "[LibSVM]Fold: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8410    0.7852    0.8121       256\n",
      "           1     0.9164    0.9407    0.9284       641\n",
      "\n",
      "    accuracy                         0.8963       897\n",
      "   macro avg     0.8787    0.8629    0.8703       897\n",
      "weighted avg     0.8949    0.8963    0.8952       897\n",
      "\n",
      "[LibSVM]Fold: 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9190    0.7569    0.8301       255\n",
      "           1     0.9096    0.9735    0.9405       641\n",
      "\n",
      "    accuracy                         0.9118       896\n",
      "   macro avg     0.9143    0.8652    0.8853       896\n",
      "weighted avg     0.9123    0.9118    0.9091       896\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "Model: MLPC IB\n",
      "Iteration 1, loss = 12.53048791\n",
      "Iteration 2, loss = 7.89847706\n",
      "Iteration 3, loss = 5.72900197\n",
      "Iteration 4, loss = 4.55309532\n",
      "Iteration 5, loss = 3.70128969\n",
      "Iteration 6, loss = 2.77328653\n",
      "Iteration 7, loss = 2.05900085\n",
      "Iteration 8, loss = 1.84298028\n",
      "Iteration 9, loss = 1.42271555\n",
      "Iteration 10, loss = 1.29548122\n",
      "Iteration 11, loss = 3.97432175\n",
      "Iteration 12, loss = 1.80070981\n",
      "Iteration 13, loss = 1.47368519\n",
      "Iteration 14, loss = 1.58074311\n",
      "Iteration 15, loss = 1.27993120\n",
      "Iteration 16, loss = 2.14237155\n",
      "Iteration 17, loss = 1.38789286\n",
      "Iteration 18, loss = 1.33048558\n",
      "Iteration 19, loss = 1.93987046\n",
      "Iteration 20, loss = 1.26854683\n",
      "Iteration 21, loss = 1.38050850\n",
      "Iteration 22, loss = 1.16704160\n",
      "Iteration 23, loss = 1.29441396\n",
      "Iteration 24, loss = 1.99468473\n",
      "Iteration 25, loss = 0.97995369\n",
      "Iteration 26, loss = 0.97050963\n",
      "Iteration 27, loss = 1.56836043\n",
      "Iteration 28, loss = 1.76547589\n",
      "Iteration 29, loss = 0.85121665\n",
      "Iteration 30, loss = 0.92403336\n",
      "Iteration 31, loss = 2.51437781\n",
      "Iteration 32, loss = 1.07751169\n",
      "Iteration 33, loss = 0.76443628\n",
      "Iteration 34, loss = 1.07280065\n",
      "Iteration 35, loss = 2.14742512\n",
      "Iteration 36, loss = 1.14006824\n",
      "Iteration 37, loss = 1.08666307\n",
      "Iteration 38, loss = 0.87540029\n",
      "Iteration 39, loss = 1.28443394\n",
      "Iteration 40, loss = 0.79856984\n",
      "Iteration 41, loss = 2.01516436\n",
      "Iteration 42, loss = 2.66290966\n",
      "Iteration 43, loss = 1.70219292\n",
      "Iteration 44, loss = 1.29404192\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fold: T\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0069    0.9444    0.0136       108\n",
      "           1     0.9997    0.6000    0.7499     36946\n",
      "\n",
      "    accuracy                         0.6010     37054\n",
      "   macro avg     0.5033    0.7722    0.3818     37054\n",
      "weighted avg     0.9968    0.6010    0.7478     37054\n",
      "\n",
      "ROC-AUC: 0.7722\n",
      "Iteration 1, loss = 12.51390165\n",
      "Iteration 2, loss = 7.97833488\n",
      "Iteration 3, loss = 5.99032504\n",
      "Iteration 4, loss = 5.15315894\n",
      "Iteration 5, loss = 4.50033659\n",
      "Iteration 6, loss = 3.61483137\n",
      "Iteration 7, loss = 4.16173701\n",
      "Iteration 8, loss = 2.95127199\n",
      "Iteration 9, loss = 3.03139239\n",
      "Iteration 10, loss = 4.23684757\n",
      "Iteration 11, loss = 2.79696888\n",
      "Iteration 12, loss = 1.81173395\n",
      "Iteration 13, loss = 1.55767468\n",
      "Iteration 14, loss = 1.30565038\n",
      "Iteration 15, loss = 1.44414294\n",
      "Iteration 16, loss = 2.31793267\n",
      "Iteration 17, loss = 2.30271655\n",
      "Iteration 18, loss = 1.41874875\n",
      "Iteration 19, loss = 1.15089001\n",
      "Iteration 20, loss = 1.48805994\n",
      "Iteration 21, loss = 1.28829788\n",
      "Iteration 22, loss = 1.19260594\n",
      "Iteration 23, loss = 1.77251561\n",
      "Iteration 24, loss = 1.99094755\n",
      "Iteration 25, loss = 1.48206196\n",
      "Iteration 26, loss = 1.62101399\n",
      "Iteration 27, loss = 2.45797605\n",
      "Iteration 28, loss = 1.16290936\n",
      "Iteration 29, loss = 1.16634521\n",
      "Iteration 30, loss = 1.08202175\n",
      "Iteration 31, loss = 2.27386325\n",
      "Iteration 32, loss = 1.17294494\n",
      "Iteration 33, loss = 0.97208309\n",
      "Iteration 34, loss = 1.05752902\n",
      "Iteration 35, loss = 0.86215746\n",
      "Iteration 36, loss = 1.61863886\n",
      "Iteration 37, loss = 1.48671733\n",
      "Iteration 38, loss = 0.82993257\n",
      "Iteration 39, loss = 0.74936856\n",
      "Iteration 40, loss = 2.10078228\n",
      "Iteration 41, loss = 1.28085889\n",
      "Iteration 42, loss = 1.22430482\n",
      "Iteration 43, loss = 0.76034292\n",
      "Iteration 44, loss = 2.16765302\n",
      "Iteration 45, loss = 1.09527809\n",
      "Iteration 46, loss = 0.85675359\n",
      "Iteration 47, loss = 1.71754800\n",
      "Iteration 48, loss = 1.12519192\n",
      "Iteration 49, loss = 0.84066200\n",
      "Iteration 50, loss = 0.99406120\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fold: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9512    0.1529    0.2635       255\n",
      "           1     0.7477    0.9969    0.8545       642\n",
      "\n",
      "    accuracy                         0.7570       897\n",
      "   macro avg     0.8494    0.5749    0.5590       897\n",
      "weighted avg     0.8055    0.7570    0.6865       897\n",
      "\n",
      "Iteration 1, loss = 12.82599540\n",
      "Iteration 2, loss = 8.22837582\n",
      "Iteration 3, loss = 6.47117801\n",
      "Iteration 4, loss = 5.69505951\n",
      "Iteration 5, loss = 4.53145164\n",
      "Iteration 6, loss = 4.30031212\n",
      "Iteration 7, loss = 3.94612207\n",
      "Iteration 8, loss = 3.34823970\n",
      "Iteration 9, loss = 2.85656183\n",
      "Iteration 10, loss = 2.48471458\n",
      "Iteration 11, loss = 2.22800403\n",
      "Iteration 12, loss = 1.82234294\n",
      "Iteration 13, loss = 1.39960472\n",
      "Iteration 14, loss = 1.20476466\n",
      "Iteration 15, loss = 2.94217838\n",
      "Iteration 16, loss = 3.39167379\n",
      "Iteration 17, loss = 1.75462759\n",
      "Iteration 18, loss = 1.16289617\n",
      "Iteration 19, loss = 1.01419175\n",
      "Iteration 20, loss = 0.97153478\n",
      "Iteration 21, loss = 1.95791838\n",
      "Iteration 22, loss = 1.03076631\n",
      "Iteration 23, loss = 1.14756869\n",
      "Iteration 24, loss = 2.32254900\n",
      "Iteration 25, loss = 1.56708840\n",
      "Iteration 26, loss = 0.94066512\n",
      "Iteration 27, loss = 0.89646076\n",
      "Iteration 28, loss = 0.77519057\n",
      "Iteration 29, loss = 0.64989494\n",
      "Iteration 30, loss = 2.37205182\n",
      "Iteration 31, loss = 1.11893652\n",
      "Iteration 32, loss = 1.10204580\n",
      "Iteration 33, loss = 0.72271878\n",
      "Iteration 34, loss = 2.82455367\n",
      "Iteration 35, loss = 3.19492624\n",
      "Iteration 36, loss = 2.32383026\n",
      "Iteration 37, loss = 1.18760927\n",
      "Iteration 38, loss = 0.86280947\n",
      "Iteration 39, loss = 0.78464260\n",
      "Iteration 40, loss = 0.85849600\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fold: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5468    0.8672    0.6707       256\n",
      "           1     0.9308    0.7129    0.8074       641\n",
      "\n",
      "    accuracy                         0.7570       897\n",
      "   macro avg     0.7388    0.7901    0.7391       897\n",
      "weighted avg     0.8212    0.7570    0.7684       897\n",
      "\n",
      "Iteration 1, loss = 13.29466749\n",
      "Iteration 2, loss = 9.40001873\n",
      "Iteration 3, loss = 6.44086834\n",
      "Iteration 4, loss = 5.35031395\n",
      "Iteration 5, loss = 4.89771248\n",
      "Iteration 6, loss = 4.06404651\n",
      "Iteration 7, loss = 3.49943246\n",
      "Iteration 8, loss = 3.02976791\n",
      "Iteration 9, loss = 2.76771128\n",
      "Iteration 10, loss = 2.46408044\n",
      "Iteration 11, loss = 2.43420368\n",
      "Iteration 12, loss = 3.08254248\n",
      "Iteration 13, loss = 3.06487840\n",
      "Iteration 14, loss = 1.90540923\n",
      "Iteration 15, loss = 1.39071048\n",
      "Iteration 16, loss = 1.24561061\n",
      "Iteration 17, loss = 1.31789460\n",
      "Iteration 18, loss = 1.11733255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 2.08192081\n",
      "Iteration 20, loss = 1.24423275\n",
      "Iteration 21, loss = 2.10109005\n",
      "Iteration 22, loss = 1.32670208\n",
      "Iteration 23, loss = 1.81257575\n",
      "Iteration 24, loss = 3.18810423\n",
      "Iteration 25, loss = 1.82342044\n",
      "Iteration 26, loss = 1.29381376\n",
      "Iteration 27, loss = 0.94627603\n",
      "Iteration 28, loss = 1.80393742\n",
      "Iteration 29, loss = 1.48657776\n",
      "Iteration 30, loss = 1.70156785\n",
      "Iteration 31, loss = 1.15759754\n",
      "Iteration 32, loss = 0.93845218\n",
      "Iteration 33, loss = 0.83327712\n",
      "Iteration 34, loss = 1.42879940\n",
      "Iteration 35, loss = 1.38326137\n",
      "Iteration 36, loss = 1.09581872\n",
      "Iteration 37, loss = 2.56156738\n",
      "Iteration 38, loss = 1.76684163\n",
      "Iteration 39, loss = 1.20721791\n",
      "Iteration 40, loss = 0.93970602\n",
      "Iteration 41, loss = 0.82319035\n",
      "Iteration 42, loss = 0.85411620\n",
      "Iteration 43, loss = 0.79968709\n",
      "Iteration 44, loss = 2.15939858\n",
      "Iteration 45, loss = 3.07796638\n",
      "Iteration 46, loss = 2.01663914\n",
      "Iteration 47, loss = 1.45037934\n",
      "Iteration 48, loss = 1.38462760\n",
      "Iteration 49, loss = 1.05780217\n",
      "Iteration 50, loss = 0.86115630\n",
      "Iteration 51, loss = 0.97381643\n",
      "Iteration 52, loss = 0.96145282\n",
      "Iteration 53, loss = 0.91868031\n",
      "Iteration 54, loss = 0.61315750\n",
      "Iteration 55, loss = 1.32682540\n",
      "Iteration 56, loss = 1.35671452\n",
      "Iteration 57, loss = 0.92897281\n",
      "Iteration 58, loss = 0.93390415\n",
      "Iteration 59, loss = 0.90987670\n",
      "Iteration 60, loss = 1.64277990\n",
      "Iteration 61, loss = 3.30968667\n",
      "Iteration 62, loss = 1.37397374\n",
      "Iteration 63, loss = 1.09679613\n",
      "Iteration 64, loss = 1.41339722\n",
      "Iteration 65, loss = 1.16077988\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fold: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8779    0.7305    0.7974       256\n",
      "           1     0.8991    0.9594    0.9283       641\n",
      "\n",
      "    accuracy                         0.8941       897\n",
      "   macro avg     0.8885    0.8450    0.8629       897\n",
      "weighted avg     0.8931    0.8941    0.8910       897\n",
      "\n",
      "Iteration 1, loss = 13.47431156\n",
      "Iteration 2, loss = 8.98250453\n",
      "Iteration 3, loss = 6.17200126\n",
      "Iteration 4, loss = 4.90390163\n",
      "Iteration 5, loss = 4.06109588\n",
      "Iteration 6, loss = 3.74632493\n",
      "Iteration 7, loss = 3.53522365\n",
      "Iteration 8, loss = 2.78483326\n",
      "Iteration 9, loss = 2.15903334\n",
      "Iteration 10, loss = 2.05625253\n",
      "Iteration 11, loss = 2.32031093\n",
      "Iteration 12, loss = 1.59033946\n",
      "Iteration 13, loss = 1.48412331\n",
      "Iteration 14, loss = 1.18476640\n",
      "Iteration 15, loss = 1.14264695\n",
      "Iteration 16, loss = 1.06505610\n",
      "Iteration 17, loss = 1.31068796\n",
      "Iteration 18, loss = 2.33183213\n",
      "Iteration 19, loss = 1.35658150\n",
      "Iteration 20, loss = 0.99062386\n",
      "Iteration 21, loss = 0.81262649\n",
      "Iteration 22, loss = 1.90827381\n",
      "Iteration 23, loss = 1.62148151\n",
      "Iteration 24, loss = 1.08927427\n",
      "Iteration 25, loss = 0.88195012\n",
      "Iteration 26, loss = 0.84800971\n",
      "Iteration 27, loss = 0.82575103\n",
      "Iteration 28, loss = 1.79818264\n",
      "Iteration 29, loss = 2.41000384\n",
      "Iteration 30, loss = 2.21504335\n",
      "Iteration 31, loss = 1.64279732\n",
      "Iteration 32, loss = 0.91017507\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fold: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6297    0.7773    0.6958       256\n",
      "           1     0.9019    0.8175    0.8576       641\n",
      "\n",
      "    accuracy                         0.8060       897\n",
      "   macro avg     0.7658    0.7974    0.7767       897\n",
      "weighted avg     0.8242    0.8060    0.8114       897\n",
      "\n",
      "Iteration 1, loss = 13.67450436\n",
      "Iteration 2, loss = 9.12557425\n",
      "Iteration 3, loss = 6.30537039\n",
      "Iteration 4, loss = 5.30068098\n",
      "Iteration 5, loss = 4.42537179\n",
      "Iteration 6, loss = 3.56223775\n",
      "Iteration 7, loss = 3.40230683\n",
      "Iteration 8, loss = 2.80326131\n",
      "Iteration 9, loss = 2.86024690\n",
      "Iteration 10, loss = 2.00606863\n",
      "Iteration 11, loss = 1.82451375\n",
      "Iteration 12, loss = 1.41243471\n",
      "Iteration 13, loss = 1.29448972\n",
      "Iteration 14, loss = 1.18797695\n",
      "Iteration 15, loss = 1.61405213\n",
      "Iteration 16, loss = 1.11708464\n",
      "Iteration 17, loss = 1.67978244\n",
      "Iteration 18, loss = 4.09840713\n",
      "Iteration 19, loss = 2.23561061\n",
      "Iteration 20, loss = 2.12456071\n",
      "Iteration 21, loss = 1.20321589\n",
      "Iteration 22, loss = 1.04195389\n",
      "Iteration 23, loss = 1.42330371\n",
      "Iteration 24, loss = 2.49326889\n",
      "Iteration 25, loss = 1.18231581\n",
      "Iteration 26, loss = 0.86919681\n",
      "Iteration 27, loss = 1.31755392\n",
      "Iteration 28, loss = 2.16794065\n",
      "Iteration 29, loss = 1.12471584\n",
      "Iteration 30, loss = 0.87231256\n",
      "Iteration 31, loss = 0.78101386\n",
      "Iteration 32, loss = 0.87159598\n",
      "Iteration 33, loss = 2.79240662\n",
      "Iteration 34, loss = 1.98003561\n",
      "Iteration 35, loss = 1.07050436\n",
      "Iteration 36, loss = 1.46239102\n",
      "Iteration 37, loss = 0.92627431\n",
      "Iteration 38, loss = 0.80952362\n",
      "Iteration 39, loss = 0.69723325\n",
      "Iteration 40, loss = 1.13611424\n",
      "Iteration 41, loss = 1.83315133\n",
      "Iteration 42, loss = 1.20105155\n",
      "Iteration 43, loss = 1.76100056\n",
      "Iteration 44, loss = 2.08039443\n",
      "Iteration 45, loss = 1.22732971\n",
      "Iteration 46, loss = 1.05502608\n",
      "Iteration 47, loss = 0.86373913\n",
      "Iteration 48, loss = 0.75799280\n",
      "Iteration 49, loss = 1.60184170\n",
      "Iteration 50, loss = 1.14745074\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fold: 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9524    0.3137    0.4720       255\n",
      "           1     0.7845    0.9938    0.8768       641\n",
      "\n",
      "    accuracy                         0.8002       896\n",
      "   macro avg     0.8684    0.6537    0.6744       896\n",
      "weighted avg     0.8323    0.8002    0.7616       896\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_test(train_ib, test_ib, svc.SVC(random_state=1, verbose=2, cache_size=1024), \"SVM IB\")\n",
    "train_test(train_ib, test_ib, mlpc.MLPClassifier(random_state=1, verbose=2), \"MLPC IB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
