{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n",
    "\n",
    "## This notebooks is used for screenshots of code in the documents using `carbon.now.sh`.\n",
    "\n",
    "The code shown in this notebooks are the simplified ones used in the actual notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def inject_patterns():\n",
    "    return None\n",
    "\n",
    "def get_unique_clusters():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_df = pd.read_csv('./Verified_Samples.csv', low_memory=False, index_col=False)\n",
    "\n",
    "#Inject pattern (i.e., summarized malware type pattern) to the DataFrame\n",
    "malicious_df = inject_patterns(malicious_df)\n",
    "\n",
    "#Drop row that is falsely labelled. (i.e. '_' on all popularity levels of VirusTotal)\n",
    "malicious_df.drop(malicious_df[(malicious_df['Type 1']=='_')].index, inplace=True)\n",
    "\n",
    "#Identify malware types (incl. counts) in the dataset\n",
    "malware_type_count = malicious_df['Type 1'].value_counts()\n",
    "\n",
    "#Identify the overall list of types each cluster is as designated by VirusTotal.\n",
    "unique_clusters = get_unique_clusters(malicious_df)\n",
    "summary = []\n",
    "for u in unique_clusters:\n",
    "    cluster_sublist = [u]\n",
    "    copy = malicious_df[malicious_df['cluster'] == u].copy(deep=True)\n",
    "    idx = list(copy['Type 1'].value_counts().index)\n",
    "    counts = list(copy['Type 1'].value_counts())\n",
    "    type_sublist = []\n",
    "    for i in range(len(idx)):\n",
    "        type_sublist.append([idx[i], counts[i]])\n",
    "    cluster_sublist.append(type_sublist)\n",
    "    summary.append(cluster_sublist)\n",
    "\n",
    "#Identify the counts of Malware Types per Cluster\n",
    "str_output = \"\"\n",
    "for s in summary:\n",
    "    print(f\"CLUSTER {s[0]}: \", end=\"\")\n",
    "    for t in s[1]:\n",
    "        str_output += f\"{t[0]} ({t[1]}); \"\n",
    "    print(str_output)\n",
    "print(str_output)\n",
    "\n",
    "# Summarize Clusters that have the same malware types as per VirusTotal.\n",
    "print(f\"# of Unique Malware Type: {len(malicious_df['Type 1'].unique())}\\n\")\n",
    "cluster_instance_summary = [0] * len(list(malicious_df['cluster'].unique()))\n",
    "count_summary = []\n",
    "for i, u in enumerate(list(malicious_df['Type 1'].unique())):\n",
    "    matching = malicious_df[malicious_df['Type 1'] == u]['cluster']\n",
    "    count_summary.append([u, len(list(matching)), \n",
    "                          len(list(matching.unique())),\n",
    "                          str(list(matching.unique()))])\n",
    "    print(f\"Unique Malware Type: {i+1}\\n\".upper() + \n",
    "          f\"Malware Type: {u}\\n\" + \n",
    "          f\"Matching Clusters Count: {len(list(matching.unique()))}\\n\" + \n",
    "          f\"Matching Clusters: {list(matching.unique())}\\n\")\n",
    "count_summary.sort(key=lambda x: x[1])\n",
    "count_summary = pd.DataFrame(count_summary, \n",
    "                             columns=['Malware Type', \n",
    "                                      'No. of Matching Verified Samples', \n",
    "                                      'No. of Matching Clusters', \n",
    "                                      \"Matching Clusters\"])\n",
    "count_summary.sort_values(by='No. of Matching Clusters',ascending=False, inplace=True)\n",
    "count_summary = count_summary[['Malware Type','No. of Matching Clusters', 'Matching Clusters']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FalseLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_df = pd.read_csv('./Verified_Samples.csv', low_memory=False)\n",
    "benign_df = pd.read_csv('./API_Patterns.csv', low_memory=False)\n",
    "\n",
    "# How many are falsely labelled samples from the verified samples?\n",
    "false_labelled = malicious_df[(malicious_df['Type 1']=='_')].copy(deep=True)\n",
    "print(f\"No. of falsely labelled samples from verified samples: \n",
    "      {false_labelled.shape[0]} ({false_labelled.shape[0]/malicious_df.shape[0]*100:.4f}%)\\n\")\n",
    "print(\"Counts of Falsely Labelled Samples in each Cluster\")\n",
    "display(false_labelled['cluster'].value_counts())\n",
    "\n",
    "# Does the presented API Call Patterns match those \n",
    "# from the API Call Patterns of those Benign samples?\n",
    "unique_false_patterns = list(false_labelled['pattern'])\n",
    "ctr = 1\n",
    "same = []\n",
    "print(\"Falsely Labelled Malicious Samples that Match API Call Patterns of Benign Samples\")\n",
    "for f in unique_false_patterns:\n",
    "    if benign_df[benign_df['pattern']==f].shape[0]>0 and f not in same:\n",
    "        print(f\"\\nPATTERN: {ctr}\\nAPI Call Pattern: {f}\\n\")\n",
    "        print(\"Hashes of Benign Samples with Matching API Call Patterns:\\n\")\n",
    "        for p in range(benign_df[benign_df['pattern']==f].shape[0]):\n",
    "            print(f\"\\t{benign_df[benign_df['pattern']==f]['hash'].iloc[p]}\\n\")\n",
    "        same.append(f)\n",
    "        ctr+=1\n",
    "same_api_calls = {len(same)} # No. of API Call Patterns of \n",
    "                             # Falsely-Labelled Malicious Samples == Benign Samples\n",
    "same_api_calls_per = {len(same)/benign_df.shape[0]*100} \n",
    "for i, s in enumerate(same):\n",
    "    print(f\"PATTERN: {i+1}\\n{list(pd.Series(s.split(',')).unique())}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PatternCompare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare API Patterns\n",
    "def print_comparison(types:str, ratios:list, max:int):\n",
    "    if max > len(ratios):\n",
    "        print(f\"The specified `max` value ({max}) exceeds available ratios to select.\")\n",
    "        max = len(ratios)\n",
    "    states = []\n",
    "    output = \"\"\n",
    "    for r in range(0,max):\n",
    "        print(f\"MATCH {r+1}\")\n",
    "        print(f\"Malicious Hashes ({len(ratios[r]['malicious_hash'])}):\")\n",
    "        for t in range(len(ratios[r]['malicious_hash'])): \n",
    "          \t# All malicious hashes that have the same API Call\n",
    "            print(f\"\\t{ratios[r]['malicious_hash'][t]} - {ratios[r]['Type 1'][t]}\")\n",
    "        print(f\"Benign Hashes ({len(ratios[r]['benign_hash'])}):\")\n",
    "        for b in range(len(ratios[r]['benign_hash'])): \n",
    "          \t# All benign hashes that have the same API Call Pattern\n",
    "            print(f\"\\t{ratios[r]['benign_hash'][b]}\")\n",
    "        print(f\"Score: {ratios[r]['ratio']:.4f}\") # Similarity Ratio\n",
    "        print(f\"Malicious API Call Pattern: {ratios[r]['malicious_pattern']}\")\n",
    "        print(f\"Benign API Call Pattern: {ratios[r]['benign_pattern']}\\n\")\n",
    "        for t in range(len(ratios[r]['malicious_hash'])): \n",
    "          \t# Malware Types of matching Malicious Samples\n",
    "            states.append(ratios[r]['Type 1'][t]) \n",
    "        print(\"================================================\\n\")\n",
    "    common_states = pd.Series(states).sort_values()\n",
    "    print(f\"\\nTop {max} Most Matching API Call Patterns to Benign Samples:\")\n",
    "    print(str(pd.Series(common_states).sort_values().value_counts()))\n",
    "    print(output)\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "malicious_df = pd.read_csv('./Verified_Samples.csv', low_memory=False)\n",
    "benign_df = pd.read_csv('./API_Patterns.csv')\n",
    "\n",
    "#Remove falsely labelled samples\n",
    "malicious_df.drop(malicious_df[(malicious_df['Type 1']=='_')].index, inplace=True)\n",
    "\n",
    "#Extract API Patterns (malicious & benign)\n",
    "malicious_patterns = malicious_df['pattern'].to_list()\n",
    "benign_patterns = benign_df['pattern'].to_list()\n",
    "\n",
    "#Compare API Call Patterns\n",
    "print(\"Comparing API Call Patterns...\")\n",
    "ratio = 0\n",
    "ratios = []\n",
    "unique_malicious = list(malicious_df['pattern'].unique())\n",
    "unique_benign = list(benign_df['pattern'].unique())\n",
    "for m,ma in enumerate(unique_malicious):\n",
    "    mal_df = malicious_df[malicious_df['pattern']== ma]\n",
    "    for b,be in enumerate(unique_benign):\n",
    "        ratios.append({'ratio': SequenceMatcher(None, ma, be).ratio(),\n",
    "                       'benign_pattern':be, 'malicious_pattern': ma,\n",
    "                       'Type 1':mal_df['Type 1'].to_list(), \n",
    "                       'malicious_hash':mal_df['hash'].to_list(), \n",
    "                       'benign_hash':benign_df[benign_df['pattern'] == be]['hash'].to_list()})\n",
    "        \n",
    "# MOST SIMILAR/DIFFERENT API CALL PATTERNS TO BOTH MALICIOUS AND BENIGN SAMPLES\n",
    "top = 20\n",
    "ratios.sort(reverse=True,key=lambda ratio: ratio['ratio'])\n",
    "print_comparison(\"HighMatching_Similar\", ratios, top) \n",
    "ratios.sort(reverse=False,key=lambda ratio: ratio['ratio'])\n",
    "print_comparison(\"LowMatching_Different\",ratios, top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InstanceCompare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_df = pd.read_csv('./Verified_Samples.csv', low_memory=False)\n",
    "benign_df = pd.read_csv('./API_Patterns.csv')\n",
    "\n",
    "#Extract Unique API Calls\n",
    "malicious_apis = []\n",
    "for i in range(malicious_df.shape[0]): \n",
    "    if not (malicious_df['Type 1'].iloc[i] == '_'):\n",
    "        malicious_apis += malicious_df['pattern'].iloc[i].split(',')\n",
    "malicious_apis = list(pd.Series(malicious_apis).unique())\n",
    "print(f\"# of Unique API Calls in Verified Malicious Samples: {len(malicious_apis)}\")\n",
    "print(str(malicious_apis) + \"\\n\")\n",
    "\n",
    "benign_apis = []\n",
    "for i in range(benign_df.shape[0]): #Only allow those with \n",
    "    benign_apis += benign_df['pattern'].iloc[i].split(',')\n",
    "benign_apis = list(pd.Series(benign_apis).unique())\n",
    "print(f\"# of Unique API Calls in Benign Samples: {len(benign_apis)}\")\n",
    "print(str(benign_apis)) + \"\\n\"\n",
    "\n",
    "## Identify the Unique API Calls only found in Malicious API Calls.\n",
    "unique = []\n",
    "for m in malicious_apis:\n",
    "    if m not in benign_apis:\n",
    "        unique.append(m)\n",
    "print(f\"No. of truly unique API Calls only found in Malicious Samples: {len(unique)} ({len(unique)/len(benign_apis)*100:.2f}% Matches API Calls of Benign Samples)\")\n",
    "print(f\"Coverage of 'Malicious-only' API Calls to Official API Calls Oliveira.csv: {(len(unique)/len(APIS))*100:.4f}%\")\n",
    "print(\"Unique API Calls to Verified Malicious Samples only: \"+ str(unique))\n",
    "\n",
    "# Identify the Same API Calls found in both Malicious and Benign Samples.\n",
    "same = []\n",
    "for m in malicious_apis:\n",
    "    if m in benign_apis:\n",
    "        same.append(m)\n",
    "print(f\"No. of API Calls in Malicious Samples that is found in API Calls in Benign Samples: {len(same)} ({len(same)/len(benign_apis)*100:.2f}% Matches API Calls of Benign Samples)\")\n",
    "print(f\"Coverage of 'Same-to-Malicious-Benign-Samples' API Calls to Official API Calls Oliveira.csv: {(len(same)/len(APIS))*100:.4f}%\")\n",
    "print(\"Unique API Calls to both Verified Malicious and Benign Samples: \"+ str(same) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oli = pd.read_csv('oliviera.csv')\n",
    "\n",
    "# Dataset Cleaning & Reformatting\n",
    "hash_col = oli.pop('hash') \n",
    "label_col = oli.pop('malware')\n",
    "oli = pd.concat([label_col, oli], axis=1)\n",
    "oli = pd.concat([oli, hash_col], axis=1)\n",
    "\n",
    "# Inverse Label Encoding\n",
    "def inverse_label(item):\n",
    "    global APIS\n",
    "    return item.map(lambda x: APIS[int(x)])\n",
    "oli.iloc[:, 1:101] = oli.iloc[:, 1:101].apply(inverse_label, axis=1, result_type='reduce')\n",
    "\n",
    "# Feature Duplicate Processing\n",
    "TB = oli.copy(deep=True) #Time-based behavior (same as original)\n",
    "IB = oli.copy(deep=True) #Instance-based behavior (to be created)\n",
    "IB.transpose()\n",
    "for r in range(oli.shape[0]):\n",
    "    row = IB.iloc[r, 1:101].drop_duplicates(keep='first', inplace=False).to_list()\n",
    "    IB.iloc[r, 1:101] = row + (['NaN']*(100-len(row)))\n",
    "    if r % 100 == 0:\n",
    "        print(r, end=\" \")\n",
    "print(\"\\nDuplicates removed!\")\n",
    "IB.transpose()\n",
    "\n",
    "# Divide to Train and Holdout/Test (i.e., 90:10)\n",
    "def firstSplit(dataset):\n",
    "    X = dataset.iloc[:,1:102] #Features\n",
    "    y = dataset.iloc[:,0] #Labels\n",
    "    return train_test_split(X, y, test_size=0.1, random_state=1, shuffle=True)\n",
    "TB_Features, TB_Reserve_Features, TB_Labels, TB_Reserve_Labels = firstSplit(TB)\n",
    "TB = pd.concat([TB_Labels,TB_Features], axis=1)\n",
    "TB.pop('hash')\n",
    "TB_Reserve = pd.concat([TB_Reserve_Labels, TB_Reserve_Features], axis=1)\n",
    "IB_Features, IB_Reserve_Features, IB_Labels, IB_Reserve_Labels = firstSplit(IB)\n",
    "IB = pd.concat([IB_Labels,IB_Features], axis=1)\n",
    "IB.pop('hash')\n",
    "IB_Reserve = pd.concat([IB_Reserve_Labels, IB_Reserve_Features], axis=1)\n",
    "\n",
    "# SMOTE\n",
    "balancer = SMOTEN(sampling_strategy='minority', random_state=1, \n",
    "                  k_neighbors=math.ceil(math.sqrt(TB.shape[0])))\n",
    "X,y = TB.iloc[:,1:101],TB.iloc[:,0]\n",
    "X,y = balancer.fit_resample(X, y)\n",
    "print(\"TB Rebalancing Finished!\")\n",
    "TB = pd.concat([y, X], axis=1)\n",
    "balancer = SMOTEN(sampling_strategy='minority', random_state=1, \n",
    "                  k_neighbors=math.ceil(math.sqrt(TB.shape[0])))\n",
    "print(\"IB Rebalance...\")\n",
    "X,y = IB.iloc[:,1:101],IB.iloc[:,0]\n",
    "X,y = balancer.fit_resample(X, y)\n",
    "print(\"IB Rebalancing Finished!\")\n",
    "IB = pd.concat([y, X], axis=1)\n",
    "\n",
    "# LabelEncoding\n",
    "ENCODED = [TB.copy(deep=True), IB.copy(deep=True), \n",
    "           TB_Reserve.copy(deep=True), IB_Reserve.copy(deep=True)]\n",
    "le = LabelEncoder()\n",
    "le.fit(APIS) # List of API Calls; Including 'NaN'.\n",
    "LGBM_TB_Train = TB.copy(deep=True).iloc[:,1:101].apply(le.transform)\n",
    "le = LabelEncoder()\n",
    "le.fit(APIS) # List of API Calls; Including 'NaN'.\n",
    "LGBM_IB_Train = IB.copy(deep=True).iloc[:,1:101].apply(le.transform)\n",
    "le = LabelEncoder()\n",
    "le.fit(APIS) # List of API Calls; Including 'NaN'.\n",
    "LGBM_TB_Holdout = TB_Reserve.copy(deep=True).iloc[:,1:101].apply(le.transform)\n",
    "le = LabelEncoder()\n",
    "le.fit(APIS) # List of API Calls; Including 'NaN'.\n",
    "LGBM_IB_Holdout = IB_Reserve.copy(deep=True).iloc[:,1:101].apply(le.transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Tranche Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5 # 80:20\n",
    "\n",
    "# K-folds sample visualization\n",
    "def kfolds_vis(dataset):\n",
    "    global K\n",
    "    X = dataset.iloc[:,1:] #All rows, 2nd to last column\n",
    "    y = dataset.iloc[:,0] #All rows, first column only\n",
    "    fig, ax = plt.subplots(figsize=(10,K+1), dpi=300)\n",
    "    train = plot_cv_indices(get_strat_kfold(), X, y, ax, K)\n",
    "    plt.show()\n",
    "\n",
    "def get_strat_kfold():\n",
    "    global K\n",
    "    return StratifiedKFold(n_splits=K, shuffle=True, random_state=1)\n",
    "\n",
    "# Render K-folds sample visualization (inner workings)\n",
    "def plot_cv_indices(cv, X, y, ax, n_splits, lw=25):\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt], indices[tr] = 1,0\n",
    "        ax.scatter(range(len(indices)), [ii] * len(indices), c=indices, marker=\"_\", \n",
    "                   lw=lw, cmap=plt.cm.Paired, vmin=0, vmax=1)\n",
    "    yticklabels = list(range(n_splits))\n",
    "    ax.set(yticks=np.arange(n_splits), yticklabels=yticklabels, \n",
    "           xlabel=\"Dataset Subsample\", ylabel=\"CV iteration\", \n",
    "           ylim=[n_splits,-1], xlim=[0, X.shape[0]])\n",
    "    ax.set_title(\"{}\".format(type(cv).__name__))\n",
    "    return ax\n",
    "\n",
    "# Render K-Folds Visualization for LGBM\n",
    "tb_train = pd.read_csv(\"../Dataset/TB/LGBM_TB.csv\", low_memory=False).fillna(\"NaN\")\n",
    "ib_train = pd.read_csv(\"../Dataset/IB/LGBM_IB.csv\", low_memory=False).fillna(\"NaN\")\n",
    "print(\"Stratified K-Folds Split at\",K,\"splits.\")\n",
    "print(\"LGBM TB/TB_Encoded Dataset\")\n",
    "kfolds_vis(tb_train)\n",
    "print(\"LGBM IB/IB_Encoded Dataset\")\n",
    "kfolds_vis(ib_train)\n",
    "\n",
    "# Render K-Folds Visualization for CATB\n",
    "tb_train = pd.read_csv(\"../Dataset/TB/CATB_TB.csv\", low_memory=False).fillna(\"NaN\")\n",
    "ib_train = pd.read_csv(\"../Dataset/IB/CATB_IB.csv\", low_memory=False).fillna(\"NaN\")\n",
    "print(\"Stratified K-Folds Split at\",K,\"splits.\")\n",
    "print(\"CATB TB/TB_Encoded Dataset\")\n",
    "kfolds_vis(tb_train)\n",
    "print(\"CATB IB/IB_Encoded Dataset\")\n",
    "kfolds_vis(ib_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Generic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indexes():\n",
    "    indexes = []\n",
    "    for i in range(100):\n",
    "        indexes.append(f\"t_{i}\")\n",
    "    return indexes\n",
    "\n",
    "def get_hyperparams():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting filenames of files\n",
    "TB_Train = \"../Dataset/TB/LGBM_TB.csv\"\n",
    "IB_Train = \"../Dataset/IB/LGBM_IB.csv\"\n",
    "#Load Dataframe\n",
    "tb_train = pd.read_csv(TB_Train, low_memory=False).fillna(\"NaN\")\n",
    "ib_train = pd.read_csv(IB_Train, low_memory=False).fillna(\"NaN\")\n",
    "#Static splitting of Train Split of Time-based (70:30)\n",
    "X_tb = tb_train.iloc[:,1:101]\n",
    "y_tb = tb_train.iloc[:,0]\n",
    "X_tb_train, X_tb_vali, y_tb_train, y_tb_vali = train_test_split(X_tb, y_tb, test_size=0.3, \n",
    "                                                                shuffle=True, random_state=1)\n",
    "#Static splitting of Train Split of Instance-based (70:30)\n",
    "X_ib = ib_train.iloc[:,1:101]\n",
    "y_ib = ib_train.iloc[:,0]\n",
    "X_ib_train, X_ib_vali, y_ib_train, y_ib_vali = train_test_split(X_ib, y_ib, test_size=0.3, \n",
    "                                                                shuffle=True, random_state=1)\n",
    "\n",
    "#Get hyperparams if tuned model to be trained\n",
    "HYPERPARAMS = None\n",
    "\n",
    "#Training Model\n",
    "tb_lgbm = None\n",
    "tb_lgbm = lightgbm.LGBMClassifier(random_state=1, n_jobs=-1, \n",
    "                                  verbose=1, categorical_data=get_indexes())\n",
    "tb_lgbm.fit(X_tb_train, y_tb_train, \n",
    "            eval_set=[(X_tb_vali, y_tb_vali), (X_tb_train, y_tb_train)], \n",
    "            eval_metric=['binary_logloss', 'average_precision', 'auc'])\n",
    "#Training Model\n",
    "ib_lgbm = lightgbm.LGBMClassifier(random_state=1, n_jobs=-1, \n",
    "                                  verbose=1, categorical_data=get_indexes())\n",
    "ib_lgbm.fit(X_ib_train, y_ib_train, \n",
    "            eval_set=[(X_ib_vali, y_ib_vali), (X_ib_train, y_ib_train)], \n",
    "            eval_metric=['binary_logloss', 'average_precision', 'auc'])\n",
    "#Saving Model as file\n",
    "dump(tb_lgbm, \"TB.model\")\n",
    "dump(ib_lgbm, \"IB.model\")\n",
    "\n",
    "#Training Model\n",
    "tb_lgbm = None\n",
    "tb_lgbm = lightgbm.LGBMClassifier(**TB_HYPERPARAMS, random_state=1, n_jobs=-1, \n",
    "                                  verbose=1, categorical_data=get_indexes())\n",
    "tb_lgbm.fit(X_tb_train, y_tb_train, \n",
    "            eval_set=[(X_tb_vali, y_tb_vali), (X_tb_train, y_tb_train)], \n",
    "            eval_metric=['binary_logloss', 'average_precision', 'auc'])\n",
    "#Training Model\n",
    "ib_lgbm = lightgbm.LGBMClassifier(**IB_HYPERPARAMS, random_state=1, n_jobs=-1, \n",
    "                                  verbose=1, categorical_data=get_indexes())\n",
    "ib_lgbm.fit(X_ib_train, y_ib_train, \n",
    "            eval_set=[(X_ib_vali, y_ib_vali), (X_ib_train, y_ib_train)], \n",
    "            eval_metric=['binary_logloss', 'average_precision', 'auc'])\n",
    "#Saving Model as file\n",
    "dump(tb_lgbm, \"TB.model\")\n",
    "dump(ib_lgbm, \"IB.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning\n",
    "\n",
    "Generic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold(cv_results, target:str):\n",
    "    return cv_results[target].max() - cv_results[target].std()\n",
    "\n",
    "def refit_strategy(cv_results):\n",
    "    cv_results_ = pd.DataFrame(cv_results)\n",
    "    # Filter-out all results below 80% score on acc, prec, & recall\n",
    "    cv_results_ = cv_results_[cv_results_[\"mean_test_accuracy\"] >= .80]\n",
    "    cv_results_ = cv_results_[cv_results_[\"mean_test_precision\"] >= .80]\n",
    "    cv_results_ = cv_results_[cv_results_[\"mean_test_recall\"] >= .80]\n",
    "    # Filter-out all results below max-std threshold score on acc, prec, & recall\n",
    "    cv_results_ = cv_results_[cv_results_[\"mean_test_accuracy\"] >= get_threshold(cv_results_, 'mean_test_accuracy')]\n",
    "    cv_results_ = cv_results_[cv_results_[\"mean_test_precision\"] >= get_threshold(cv_results_, 'mean_test_precision')]\n",
    "    cv_results_ = cv_results_[cv_results_[\"mean_test_recall\"] >= get_threshold(cv_results_, 'mean_test_recall')]\n",
    "    return cv_results_['mean_fit_time'].idxmin()\n",
    "\n",
    "def get_tuner(model, params, scoring=['accuracy','precision','recall', 'roc_auc']):\n",
    "    return GridSearchCV(model, params, scoring=scoring, n_jobs=1, refit=refit_strategy, \n",
    "                        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=1), \n",
    "                        verbose=2, pre_dispatch='2*n_jobs', error_score=0, \n",
    "                        return_train_score=False)\n",
    "\n",
    "#Specify features (X) and labels (y) for Time-based data\n",
    "X_tb = tb_train.iloc[:,1:101] #All rows, 2nd to last column\n",
    "y_tb = tb_train.iloc[:,0] #All rows, first column only\n",
    "\n",
    "#Specify features (X) and labels (y) for Instance-based data\n",
    "X_ib = ib_train.iloc[:,1:101] #All rows, 2nd to last column\n",
    "y_ib = ib_train.iloc[:,0] #All rows, first column only\n",
    "\n",
    "lgbm_classifier = lightgbm.LGBMClassifier(random_state=1, \n",
    "                                          n_jobs=int(os.cpu_count()), verbose=-1)\n",
    "tb_tuner = get_tuner(lgbm_classifier, lgbm_params)\n",
    "tb_tuner.fit(X_tb, y_tb)\n",
    "\n",
    "print_to_file(\"LGBM_TB\", tb_tuner.best_params_)\n",
    "cv_results = pd.DataFrame.from_dict(tb_tuner.cv_results_)\n",
    "cv_results.to_csv(f\"./Outputs/LGBM/{OUTPUT_FILENAME}_LGBM_TB_Tune_CVRes.csv\")\n",
    "\n",
    "display(cv_results)\n",
    "plot_search_results(tb_tuner, cv_results, 'TB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
