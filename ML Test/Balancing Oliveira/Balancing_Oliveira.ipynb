{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c031dbf4",
   "metadata": {},
   "source": [
    "# Balancing Oliveira\n",
    "\n",
    "The objective of this notebook is to explore the viability of conducting dataset sample balancing through two techniques:\n",
    "\n",
    "1. Resampling (specifically RandomOverSampler)\n",
    "2. SMOTE (specifically SMOTEN)\n",
    "\n",
    "It turns out, there is a dedicated library called \"[Imbalanced Learn\"](https://imbalanced-learn.org/stable/index.html) (denoted as `imblearn`) which can be used to handle imbalanced datasets such that it can perform the two techniques.\n",
    "\n",
    "**It is assumed (unless otherwise stated) that other data pre-processing (i.e., removing duplicates, data cleaning, label encoding, and shuffling/renaming of columns) will be done after this. Hence, this notebook only explores time-based behaviors (i.e., with duplicates).**\n",
    "\n",
    "This notebook will also look into implementing/testing the proposal of Tustin where she suggested to use part of Oliveira for Model Robustness Testing instead of a third-party dataset as originally proposed. The overview of its steps are as follows:\n",
    "\n",
    "1. Get the raw dataset (Oliveira).\n",
    "2. Split it into train (70%) and test (30%)\n",
    "3. The train split will undergo smote/resampling.\n",
    "4. Then the test split will be used to determine if the model really works well and robustly\n",
    "\n",
    "*Note that the hyperparameter `random_state` was set to `1` instead of `None` for test repeatability.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "271f21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = 0\n",
    "def start():\n",
    "    global start_time\n",
    "    start_time = time.time()\n",
    "def end():\n",
    "    print(\"Elapsed time:\", time.time()-start_time, \"seconds\")\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier #Nearest to LightGBM and XGBoost\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def hgbt_classifier(dataset, cat):\n",
    "    X = dataset.iloc[:,1:101]\n",
    "    y = dataset.iloc[:,101]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)\n",
    "    hgbt = HistGradientBoostingClassifier(loss='log_loss', learning_rate=0.1, max_iter=300, max_leaf_nodes=2, \n",
    "                                        max_depth=None, min_samples_leaf=20, l2_regularization=0.0, max_bins=255, \n",
    "                                        categorical_features=cat, monotonic_cst=None, interaction_cst=None, \n",
    "                                        warm_start=False, early_stopping='auto', scoring='loss', validation_fraction=0.1, \n",
    "                                        n_iter_no_change=10, tol=1e-07, verbose=0, random_state=1, class_weight=None)\n",
    "    start()\n",
    "    hgbt.fit(X_train, y_train)\n",
    "    end()\n",
    "    y_pred = hgbt.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edba5b0",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b603141",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.39312052726745605 seconds\n",
      "(43876, 102)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>...</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>malware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>071e8c3f8922e186e57548cd4c703a5d</td>\n",
       "      <td>112</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>208</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33f8e6d08a6aae939f25a8e0d63dd523</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b68abd064e975e1c6d5f25e748663076</td>\n",
       "      <td>16</td>\n",
       "      <td>110</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>113</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72049be7bd30ea61297ea624ae198067</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>228</td>\n",
       "      <td>302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c9b3700a77facf29172f32df6bc77f48</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>40</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               hash  t_0  t_1  t_2  t_3  t_4  t_5  t_6  t_7  \\\n",
       "0  071e8c3f8922e186e57548cd4c703a5d  112  274  158  215  274  158  215  298   \n",
       "1  33f8e6d08a6aae939f25a8e0d63dd523   82  208  187  208  172  117  172  117   \n",
       "2  b68abd064e975e1c6d5f25e748663076   16  110  240  117  240  117  240  117   \n",
       "3  72049be7bd30ea61297ea624ae198067   82  208  187  208  172  117  172  117   \n",
       "4  c9b3700a77facf29172f32df6bc77f48   82  240  117  240  117  240  117  240   \n",
       "\n",
       "   t_8  ...  t_91  t_92  t_93  t_94  t_95  t_96  t_97  t_98  t_99  malware  \n",
       "0   76  ...    71   297   135   171   215    35   208    56    71        1  \n",
       "1  172  ...    81   240   117    71   297   135   171   215    35        1  \n",
       "2  240  ...    65   112   123    65   112   123    65   113   112        1  \n",
       "3  172  ...   208   302   208   302   187   208   302   228   302        1  \n",
       "4  117  ...   209   260    40   209   260   141   260   141   260        1  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "start()\n",
    "oli = pd.read_csv(\"oliveira.csv\")\n",
    "end()\n",
    "print(oli.shape)\n",
    "oli.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb017f",
   "metadata": {},
   "source": [
    "# Check Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "707392ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    42797\n",
       "0     1079\n",
       "Name: malware, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oli[\"malware\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e84fb",
   "metadata": {},
   "source": [
    "# Obtain Features (X)\n",
    "\n",
    "This includes the hash as part *feature* for now. \n",
    "\n",
    "However, for model training/fitting, the hash will not be considered as a feature anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "081807cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>...</th>\n",
       "      <th>t_90</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>071e8c3f8922e186e57548cd4c703a5d</td>\n",
       "      <td>112</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>208</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33f8e6d08a6aae939f25a8e0d63dd523</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b68abd064e975e1c6d5f25e748663076</td>\n",
       "      <td>16</td>\n",
       "      <td>110</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>113</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72049be7bd30ea61297ea624ae198067</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>215</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>228</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c9b3700a77facf29172f32df6bc77f48</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>40</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               hash  t_0  t_1  t_2  t_3  t_4  t_5  t_6  t_7  \\\n",
       "0  071e8c3f8922e186e57548cd4c703a5d  112  274  158  215  274  158  215  298   \n",
       "1  33f8e6d08a6aae939f25a8e0d63dd523   82  208  187  208  172  117  172  117   \n",
       "2  b68abd064e975e1c6d5f25e748663076   16  110  240  117  240  117  240  117   \n",
       "3  72049be7bd30ea61297ea624ae198067   82  208  187  208  172  117  172  117   \n",
       "4  c9b3700a77facf29172f32df6bc77f48   82  240  117  240  117  240  117  240   \n",
       "\n",
       "   t_8  ...  t_90  t_91  t_92  t_93  t_94  t_95  t_96  t_97  t_98  t_99  \n",
       "0   76  ...   117    71   297   135   171   215    35   208    56    71  \n",
       "1  172  ...    60    81   240   117    71   297   135   171   215    35  \n",
       "2  240  ...   123    65   112   123    65   112   123    65   113   112  \n",
       "3  172  ...   215   208   302   208   302   187   208   302   228   302  \n",
       "4  117  ...    40   209   260    40   209   260   141   260   141   260  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = oli.iloc[:,:101]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1efce4",
   "metadata": {},
   "source": [
    "# Obtain Labels (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61fee06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: malware, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Obtain labels y\n",
    "y = oli.iloc[:,101]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddcfe85",
   "metadata": {},
   "source": [
    "# Baseline Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8801ff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1.5115134716033936 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.45      0.60       329\n",
      "           1       0.99      1.00      0.99     12834\n",
      "\n",
      "    accuracy                           0.98     13163\n",
      "   macro avg       0.94      0.72      0.79     13163\n",
      "weighted avg       0.98      0.98      0.98     13163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hgbt_classifier(oli, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d998cfad",
   "metadata": {},
   "source": [
    "# 1. Resampling\n",
    "\n",
    "For this, imblearn's RandomOverSampler will be used as a resampling technique.\n",
    "\n",
    "Further details can be found [here](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4c910f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.6906061172485352 seconds\n",
      "X_resampled: (85594, 101)\n",
      "y_resampled: (85594,)\n",
      "Oliveira (Resampled): (85594, 102) \n",
      "\n",
      "Class Count:\n",
      "1    42797\n",
      "0    42797\n",
      "Name: malware, dtype: int64 \n",
      "\n",
      "Top 20 Most Repeated Samples:\n",
      "hash                              malware\n",
      "03384ab6368b68ed16ecb9e6352539af  0          90\n",
      "0822ec2ba98d291e5bfc836bc3686096  0          90\n",
      "f78ea80cec007b2c32fb10f9c6c82f39  0          88\n",
      "075323e77815ee8bcc7854ce23955a15  0          79\n",
      "79b78bb3d583748040c41ded09555fd3  0          72\n",
      "bdaaac3fa3f6796825a51ef1c0e5b3fd  0          71\n",
      "3d8a7a97ea954dd4fe66279df2b445e0  0          70\n",
      "d0b42a077320d2ab2d2a80bcbcae02cb  0          60\n",
      "3daa3068ea8bf5d2e65820c42af62227  0          59\n",
      "7cc90abc007d2efc476930137899cfda  0          58\n",
      "484b5a0e5782535e1091412d24198afc  0          58\n",
      "0566db6153dc8f7bdbef9552a6852139  0          57\n",
      "9976cd22e18868887eacb927616d5e41  0          57\n",
      "3a056dfba8365bc058ac05634cb22818  0          56\n",
      "74db62f95ab558326cc79e4001726832  0          56\n",
      "c6f4acf3988a1486795283cc6e1de56a  0          56\n",
      "64442d3c360ed766e10fb4638a298005  0          55\n",
      "202816ccf78265f3ca37e38e3014e14d  0          55\n",
      "bd9d375f79e14b07541236ac4ebc5675  0          55\n",
      "75aa9a661ce16a6b36f8d8f654b3d86c  0          55\n",
      "dtype: int64 \n",
      "\n",
      "Unique Values: 43867\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>...</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>malware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>071e8c3f8922e186e57548cd4c703a5d</td>\n",
       "      <td>112</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>208</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33f8e6d08a6aae939f25a8e0d63dd523</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b68abd064e975e1c6d5f25e748663076</td>\n",
       "      <td>16</td>\n",
       "      <td>110</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>113</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72049be7bd30ea61297ea624ae198067</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>228</td>\n",
       "      <td>302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c9b3700a77facf29172f32df6bc77f48</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>40</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85589</th>\n",
       "      <td>e1efa1385daabfec74fe877f63f7daf1</td>\n",
       "      <td>286</td>\n",
       "      <td>110</td>\n",
       "      <td>172</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>215</td>\n",
       "      <td>114</td>\n",
       "      <td>215</td>\n",
       "      <td>117</td>\n",
       "      <td>71</td>\n",
       "      <td>25</td>\n",
       "      <td>71</td>\n",
       "      <td>275</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85590</th>\n",
       "      <td>892262603e040080d04a9e5f72e3165b</td>\n",
       "      <td>82</td>\n",
       "      <td>228</td>\n",
       "      <td>16</td>\n",
       "      <td>215</td>\n",
       "      <td>89</td>\n",
       "      <td>208</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>194</td>\n",
       "      <td>82</td>\n",
       "      <td>194</td>\n",
       "      <td>297</td>\n",
       "      <td>194</td>\n",
       "      <td>297</td>\n",
       "      <td>194</td>\n",
       "      <td>82</td>\n",
       "      <td>194</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85591</th>\n",
       "      <td>0b2a5e7d55bc9ca38c5c736d85b1195f</td>\n",
       "      <td>82</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>16</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>198</td>\n",
       "      <td>...</td>\n",
       "      <td>187</td>\n",
       "      <td>215</td>\n",
       "      <td>73</td>\n",
       "      <td>29</td>\n",
       "      <td>73</td>\n",
       "      <td>29</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85592</th>\n",
       "      <td>1206af04cdc528613fc12471419ebf01</td>\n",
       "      <td>16</td>\n",
       "      <td>172</td>\n",
       "      <td>194</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>...</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85593</th>\n",
       "      <td>730f1d918acb1d8334ad3b156f7121d7</td>\n",
       "      <td>16</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>275</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>275</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>275</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>275</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>275</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85594 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   hash  t_0  t_1  t_2  t_3  t_4  t_5  t_6  \\\n",
       "0      071e8c3f8922e186e57548cd4c703a5d  112  274  158  215  274  158  215   \n",
       "1      33f8e6d08a6aae939f25a8e0d63dd523   82  208  187  208  172  117  172   \n",
       "2      b68abd064e975e1c6d5f25e748663076   16  110  240  117  240  117  240   \n",
       "3      72049be7bd30ea61297ea624ae198067   82  208  187  208  172  117  172   \n",
       "4      c9b3700a77facf29172f32df6bc77f48   82  240  117  240  117  240  117   \n",
       "...                                 ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "85589  e1efa1385daabfec74fe877f63f7daf1  286  110  172  240  117  240  117   \n",
       "85590  892262603e040080d04a9e5f72e3165b   82  228   16  215   89  208  215   \n",
       "85591  0b2a5e7d55bc9ca38c5c736d85b1195f   82  172  117   16  172  117  172   \n",
       "85592  1206af04cdc528613fc12471419ebf01   16  172  194  274  158  215  274   \n",
       "85593  730f1d918acb1d8334ad3b156f7121d7   16  172  117  275  240  117  275   \n",
       "\n",
       "       t_7  t_8  ...  t_91  t_92  t_93  t_94  t_95  t_96  t_97  t_98  t_99  \\\n",
       "0      298   76  ...    71   297   135   171   215    35   208    56    71   \n",
       "1      117  172  ...    81   240   117    71   297   135   171   215    35   \n",
       "2      117  240  ...    65   112   123    65   112   123    65   113   112   \n",
       "3      117  172  ...   208   302   208   302   187   208   302   228   302   \n",
       "4      240  117  ...   209   260    40   209   260   141   260   141   260   \n",
       "...    ...  ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "85589  240  117  ...   215   114   215   117    71    25    71   275   260   \n",
       "85590  274  158  ...   194    82   194   297   194   297   194    82   194   \n",
       "85591  117  198  ...   187   215    73    29    73    29    82   240   117   \n",
       "85592  158  215  ...   158   215   274   158   215   274   158   215   274   \n",
       "85593  240  117  ...   117   275   240   117   275   240   117   275   240   \n",
       "\n",
       "       malware  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "...        ...  \n",
       "85589        0  \n",
       "85590        0  \n",
       "85591        0  \n",
       "85592        0  \n",
       "85593        0  \n",
       "\n",
       "[85594 rows x 102 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "start()\n",
    "ros = RandomOverSampler(random_state=1, sampling_strategy='minority') #random_state is arbitrary; setting a value makes its output repeatable\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y) #Resampling of minority classes (i.e., Benign/0 for Oliveira)\n",
    "end()\n",
    "\n",
    "oli_resampled = pd.concat([X_resampled,y_resampled.copy()], axis=1)\n",
    "print(\"X_resampled:\", X_resampled.shape)\n",
    "print(\"y_resampled:\", y_resampled.shape)\n",
    "print(\"Oliveira (Resampled):\", oli_resampled.shape, \"\\n\")\n",
    "print(\"Class Count:\\n\" + str(oli_resampled['malware'].value_counts()), \"\\n\")\n",
    "print(\"Top 20 Most Repeated Samples:\\n\" + str(oli_resampled[['hash','malware']].value_counts()[0:20]), \"\\n\")\n",
    "print(\"Unique Values:\", len(oli_resampled['hash'].unique()))\n",
    "oli_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "994bad80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 2.53947114944458 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.88     12827\n",
      "           1       0.87      0.92      0.89     12852\n",
      "\n",
      "    accuracy                           0.89     25679\n",
      "   macro avg       0.89      0.89      0.89     25679\n",
      "weighted avg       0.89      0.89      0.89     25679\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hgbt_classifier(oli_resampled, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3251356a",
   "metadata": {},
   "source": [
    "# 2. SMOTE (specifically SMOTEN)\n",
    "\n",
    "For this, imblearn's SMOTEN will be used as the SMOTE technique. SMOTEN (Synthetic Minority Over-sampling Technique for Nominal) which is described as \n",
    "\n",
    "`This method is referred as SMOTEN in [1]. It expects that the data to resample are only made of categorical features.`\n",
    "\n",
    "Further details can be found [here](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTEN.html).\n",
    "\n",
    "For this example, it is assumed that the numerical values will act as in-place of categorical data since HGBT does not support categorical data. Apart from that, the hashes are considered still as categorical data despite its irrelevance in the training and prediction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76ed8eda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 176.6128706932068 seconds\n",
      "X_smoten (85594, 101)\n",
      "y_smoten (85594,)\n",
      "Oliveira (SMOTEN): (85594, 102) \n",
      "\n",
      "Class Count:\n",
      "1    42797\n",
      "0    42797\n",
      "Name: malware, dtype: int64 \n",
      "\n",
      "Top 10 Most Repeated Samples:\n",
      "hash                              malware\n",
      "3cedd98ea184c22ee3b024c72a96e075  0          5965\n",
      "0fbe9eac4ff5af1a392d92881c70c559  0          3728\n",
      "0b7e7bc7598abe9cfdc594e17e795cf0  0          1895\n",
      "125d4cdb14dbe86841037e5bbfc6a0bc  0           895\n",
      "35dd2f5d51ba224735732424f8ab6398  0           860\n",
      "05d2a956ac82d30fef9b807e9746b339  0           833\n",
      "0d77f98fafb6c34a5861e61de06b4b0f  0           718\n",
      "0d2ab02c993ea29a1989b442bf7150c7  0           622\n",
      "06b43cb00b61be55b6d100b15edfbc39  0           568\n",
      "07613e49400add94324002a019d3a9f5  0           566\n",
      "0bdf4fee812ba46472f51d1ae2ef5e04  0           554\n",
      "0822ec2ba98d291e5bfc836bc3686096  0           529\n",
      "3506356c329758e4f703cd2103d7daab  0           505\n",
      "501e961feebbde040fb836cb5de122c2  0           498\n",
      "0fca2620b9f96936b7594fc650b1d8ca  0           446\n",
      "1e3cb879f87db59d8cd3b636740f7e7f  0           432\n",
      "0a9ffc46b9bb734ecf71232dcfbd327b  0           418\n",
      "4f3227d63aa6da5a3e25caade8c0a7b3  0           417\n",
      "272c3b485f1db3199df480814b5265d6  0           415\n",
      "15cf85c3d904a7d8650164b0b831a318  0           414\n",
      "dtype: int64 \n",
      "\n",
      "Unique Values: 43867\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>...</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>malware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>071e8c3f8922e186e57548cd4c703a5d</td>\n",
       "      <td>112</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>208</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33f8e6d08a6aae939f25a8e0d63dd523</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b68abd064e975e1c6d5f25e748663076</td>\n",
       "      <td>16</td>\n",
       "      <td>110</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>113</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72049be7bd30ea61297ea624ae198067</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>228</td>\n",
       "      <td>302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c9b3700a77facf29172f32df6bc77f48</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>40</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85589</th>\n",
       "      <td>0fbe9eac4ff5af1a392d92881c70c559</td>\n",
       "      <td>286</td>\n",
       "      <td>110</td>\n",
       "      <td>172</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>215</td>\n",
       "      <td>114</td>\n",
       "      <td>215</td>\n",
       "      <td>117</td>\n",
       "      <td>71</td>\n",
       "      <td>25</td>\n",
       "      <td>71</td>\n",
       "      <td>275</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85590</th>\n",
       "      <td>0d77f98fafb6c34a5861e61de06b4b0f</td>\n",
       "      <td>82</td>\n",
       "      <td>172</td>\n",
       "      <td>16</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>117</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85591</th>\n",
       "      <td>0b2a5e7d55bc9ca38c5c736d85b1195f</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>117</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>208</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>117</td>\n",
       "      <td>73</td>\n",
       "      <td>215</td>\n",
       "      <td>73</td>\n",
       "      <td>29</td>\n",
       "      <td>82</td>\n",
       "      <td>117</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85592</th>\n",
       "      <td>0d77f98fafb6c34a5861e61de06b4b0f</td>\n",
       "      <td>82</td>\n",
       "      <td>172</td>\n",
       "      <td>16</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>...</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85593</th>\n",
       "      <td>08c91397a60a012b4d3c263d203164ce</td>\n",
       "      <td>16</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>35</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85594 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   hash  t_0  t_1  t_2  t_3  t_4  t_5  t_6  \\\n",
       "0      071e8c3f8922e186e57548cd4c703a5d  112  274  158  215  274  158  215   \n",
       "1      33f8e6d08a6aae939f25a8e0d63dd523   82  208  187  208  172  117  172   \n",
       "2      b68abd064e975e1c6d5f25e748663076   16  110  240  117  240  117  240   \n",
       "3      72049be7bd30ea61297ea624ae198067   82  208  187  208  172  117  172   \n",
       "4      c9b3700a77facf29172f32df6bc77f48   82  240  117  240  117  240  117   \n",
       "...                                 ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "85589  0fbe9eac4ff5af1a392d92881c70c559  286  110  172  240  117  240  117   \n",
       "85590  0d77f98fafb6c34a5861e61de06b4b0f   82  172   16  274  158  215  117   \n",
       "85591  0b2a5e7d55bc9ca38c5c736d85b1195f   82  208  117  208  172  117  172   \n",
       "85592  0d77f98fafb6c34a5861e61de06b4b0f   82  172   16  274  158  215  274   \n",
       "85593  08c91397a60a012b4d3c263d203164ce   16  117  117  117  172  117  172   \n",
       "\n",
       "       t_7  t_8  ...  t_91  t_92  t_93  t_94  t_95  t_96  t_97  t_98  t_99  \\\n",
       "0      298   76  ...    71   297   135   171   215    35   208    56    71   \n",
       "1      117  172  ...    81   240   117    71   297   135   171   215    35   \n",
       "2      117  240  ...    65   112   123    65   112   123    65   113   112   \n",
       "3      117  172  ...   208   302   208   302   187   208   302   228   302   \n",
       "4      240  117  ...   209   260    40   209   260   141   260   141   260   \n",
       "...    ...  ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "85589  240  117  ...   215   114   215   117    71    25    71   275   260   \n",
       "85590  158  215  ...   117   215   274   158   215   274   158   215   274   \n",
       "85591  208   16  ...    31   117    73   215    73    29    82   117    85   \n",
       "85592  158  215  ...   158   215   274   158   215   274   158   215   274   \n",
       "85593  117   16  ...   117    35   117   117   117    35     2    81   240   \n",
       "\n",
       "       malware  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "...        ...  \n",
       "85589        0  \n",
       "85590        0  \n",
       "85591        0  \n",
       "85592        0  \n",
       "85593        0  \n",
       "\n",
       "[85594 rows x 102 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTEN\n",
    "\n",
    "start()\n",
    "smoten = SMOTEN(random_state=1, sampling_strategy='minority') #random_state is arbitrary; setting a value makes its output repeatable\n",
    "X_smoten, y_smoten = smoten.fit_resample(X, y) #Resampling of minority classes (i.e., Benign/0 for Oliveira)\n",
    "end()\n",
    "\n",
    "oli_smoten = pd.concat([X_smoten,y_smoten.copy()], axis=1)\n",
    "print(\"X_smoten\", X_smoten.shape)\n",
    "print(\"y_smoten\", y_smoten.shape)\n",
    "print(\"Oliveira (SMOTEN):\", oli_smoten.shape, \"\\n\")\n",
    "print(\"Class Count:\\n\" + str(oli_smoten['malware'].value_counts()), \"\\n\")\n",
    "print(\"Top 10 Most Repeated Samples:\\n\" + str(oli_smoten[['hash','malware']].value_counts()[0:20]),\"\\n\")\n",
    "print(\"Unique Values:\", len(oli_smoten['hash'].unique()))\n",
    "oli_smoten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61f128ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 2.951122999191284 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94     12827\n",
      "           1       0.95      0.94      0.94     12852\n",
      "\n",
      "    accuracy                           0.94     25679\n",
      "   macro avg       0.94      0.94      0.94     25679\n",
      "weighted avg       0.94      0.94      0.94     25679\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hgbt_classifier(oli_smoten,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8a80b1",
   "metadata": {},
   "source": [
    "# 3. Proposed solution to allow for Model Robustness Testing\n",
    "\n",
    "This implementation of the proposed solution assumes that the split will occur at data preparation.\n",
    "\n",
    "Taking note of the overview of steps:\n",
    "\n",
    "1. Get the raw dataset (Oliveira).\n",
    "2. Split it into train (70%) and test (30%)\n",
    "3. The train split will undergo smote/resampling.\n",
    "4. Then the test split will be used to determine if the model really works well and robustly\n",
    "\n",
    "The Oliveira dataset will be divided into train and test splits at a 70:30 ratio. \n",
    "\n",
    "The train split will undergo balancing (either Resampling or SMOTE) while the test split will be as is.\n",
    "\n",
    "The train split will be used in training where it will undergo further splitting as part of the cross-validation processes (i.e., holdout and repeated holdout) during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123bc9e",
   "metadata": {},
   "source": [
    "## 3.1. Loading Raw Oliveira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80ca78ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43876, 102)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>...</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>malware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>071e8c3f8922e186e57548cd4c703a5d</td>\n",
       "      <td>112</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>208</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33f8e6d08a6aae939f25a8e0d63dd523</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b68abd064e975e1c6d5f25e748663076</td>\n",
       "      <td>16</td>\n",
       "      <td>110</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>113</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72049be7bd30ea61297ea624ae198067</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>228</td>\n",
       "      <td>302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c9b3700a77facf29172f32df6bc77f48</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>40</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               hash  t_0  t_1  t_2  t_3  t_4  t_5  t_6  t_7  \\\n",
       "0  071e8c3f8922e186e57548cd4c703a5d  112  274  158  215  274  158  215  298   \n",
       "1  33f8e6d08a6aae939f25a8e0d63dd523   82  208  187  208  172  117  172  117   \n",
       "2  b68abd064e975e1c6d5f25e748663076   16  110  240  117  240  117  240  117   \n",
       "3  72049be7bd30ea61297ea624ae198067   82  208  187  208  172  117  172  117   \n",
       "4  c9b3700a77facf29172f32df6bc77f48   82  240  117  240  117  240  117  240   \n",
       "\n",
       "   t_8  ...  t_91  t_92  t_93  t_94  t_95  t_96  t_97  t_98  t_99  malware  \n",
       "0   76  ...    71   297   135   171   215    35   208    56    71        1  \n",
       "1  172  ...    81   240   117    71   297   135   171   215    35        1  \n",
       "2  240  ...    65   112   123    65   112   123    65   113   112        1  \n",
       "3  172  ...   208   302   208   302   187   208   302   228   302        1  \n",
       "4  117  ...   209   260    40   209   260   141   260   141   260        1  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "oli = pd.read_csv(\"oliveira.csv\")\n",
    "print(oli.shape)\n",
    "oli.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37116d4e",
   "metadata": {},
   "source": [
    "## 3.2. Splitting it into train and test at 70:30 ratio.\n",
    "*Both will be logically named as train_split and test_split respectively.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6ba836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = oli.iloc[:,0:101] #features\n",
    "y = oli.iloc[:,101] #label\n",
    "\n",
    "#Splitting to train_split and test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)\n",
    "\n",
    "#Building train_split dataframe.\n",
    "train_split = pd.concat([X_train,y_train.copy()], axis=1)\n",
    "\n",
    "#Building test_split dataframe.\n",
    "test_split = pd.concat([X_test,y_test.copy()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "700271e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_split shape: (30713, 102)\n",
      "train_split value_counts:\n",
      "1    29963\n",
      "0      750\n",
      "Name: malware, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>...</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>malware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14432</th>\n",
       "      <td>38d19f81159e526a6d410af78f74ccf9</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>240</td>\n",
       "      <td>89</td>\n",
       "      <td>117</td>\n",
       "      <td>31</td>\n",
       "      <td>117</td>\n",
       "      <td>215</td>\n",
       "      <td>260</td>\n",
       "      <td>192</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39388</th>\n",
       "      <td>a04c5d83384df33c48db5b21501aa336</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>25</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23274</th>\n",
       "      <td>1ddbb2bac5055ed0bfad44b24d4442ae</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>25</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28198</th>\n",
       "      <td>a6f3478cd06844ac9da84103b117a862</td>\n",
       "      <td>112</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>208</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23969</th>\n",
       "      <td>8fc79157568b69638820ec31e25072ed</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   hash  t_0  t_1  t_2  t_3  t_4  t_5  t_6  \\\n",
       "14432  38d19f81159e526a6d410af78f74ccf9   82  240  117  240  117  240  117   \n",
       "39388  a04c5d83384df33c48db5b21501aa336  215  274  158  215  274  158  215   \n",
       "23274  1ddbb2bac5055ed0bfad44b24d4442ae  215  274  158  215  274  158  215   \n",
       "28198  a6f3478cd06844ac9da84103b117a862  112  274  158  215  274  158  215   \n",
       "23969  8fc79157568b69638820ec31e25072ed  240  117  240  117  240  117  240   \n",
       "\n",
       "       t_7  t_8  ...  t_91  t_92  t_93  t_94  t_95  t_96  t_97  t_98  t_99  \\\n",
       "14432  240  117  ...   240    89   117    31   117   215   260   192    89   \n",
       "39388  172  117  ...    60    81    60    81   172   117    25   172   117   \n",
       "23274  172  117  ...    60    81    60    81   172   117    25   172   117   \n",
       "28198  298   76  ...    71   297   135   171   215    35   208    56    71   \n",
       "23969  117  240  ...    60    81    60    81    60    81    60    81    60   \n",
       "\n",
       "       malware  \n",
       "14432        1  \n",
       "39388        1  \n",
       "23274        1  \n",
       "28198        1  \n",
       "23969        1  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"train_split shape:\", train_split.shape)\n",
    "print(\"train_split value_counts:\")\n",
    "print(train_split['malware'].value_counts())\n",
    "train_split.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41dcc636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_split shape: (13163, 102)\n",
      "1    12834\n",
      "0      329\n",
      "Name: malware, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>...</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>malware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24659</th>\n",
       "      <td>f0f8ba4c3d750a4ce2deea48152a33d4</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>15</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34393</th>\n",
       "      <td>39b2d87c1adb582fbcacc3a56e274d48</td>\n",
       "      <td>286</td>\n",
       "      <td>110</td>\n",
       "      <td>172</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>275</td>\n",
       "      <td>260</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>141</td>\n",
       "      <td>65</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20301</th>\n",
       "      <td>429236cdeb63d68bf48a3b48b0a34612</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>208</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>100</td>\n",
       "      <td>215</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20025</th>\n",
       "      <td>46079cbf0bcfe8fab9894b4ec88bece3</td>\n",
       "      <td>112</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>208</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19747</th>\n",
       "      <td>303ceda3f52afa9b69ed4f97fec2c895</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   hash  t_0  t_1  t_2  t_3  t_4  t_5  t_6  \\\n",
       "24659  f0f8ba4c3d750a4ce2deea48152a33d4  215  274  158  215  274  158  215   \n",
       "34393  39b2d87c1adb582fbcacc3a56e274d48  286  110  172  240  117  240  117   \n",
       "20301  429236cdeb63d68bf48a3b48b0a34612   82  208  187  208  172  117  172   \n",
       "20025  46079cbf0bcfe8fab9894b4ec88bece3  112  274  158  215  274  158  215   \n",
       "19747  303ceda3f52afa9b69ed4f97fec2c895   82  240  117  240  117  240  117   \n",
       "\n",
       "       t_7  t_8  ...  t_91  t_92  t_93  t_94  t_95  t_96  t_97  t_98  t_99  \\\n",
       "24659  172  117  ...   117    15   240   117   240   117   240   117   172   \n",
       "34393  240  117  ...    71   275   260   240   117   141    65   260   141   \n",
       "20301  208   16  ...   172   117   172   117   208   172   117   100   215   \n",
       "20025  298   76  ...    71   297   135   171   215    35   208    56    71   \n",
       "19747  240  117  ...    82   260   141   260   141   260   141   260   141   \n",
       "\n",
       "       malware  \n",
       "24659        1  \n",
       "34393        1  \n",
       "20301        1  \n",
       "20025        1  \n",
       "19747        1  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"test_split shape:\", test_split.shape)\n",
    "print(test_split['malware'].value_counts())\n",
    "test_split.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe445347",
   "metadata": {},
   "source": [
    "## 3.3. Balancing train_split\n",
    "\n",
    "*For this normal resampling will be selected as it is more 'balanced' than SMOTEN is.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "339e8c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [1 0]\n",
      "Label Value Counts:\n",
      "1    29963\n",
      "0      750\n",
      "Name: malware, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = train_split.iloc[:,0:101]\n",
    "y = train_split.iloc[:,101]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1) #For use in training and testing model\n",
    "print(\"Labels:\", y.unique())\n",
    "print(\"Label Value Counts:\\n\" + str(y.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "844da8ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_resampled (59926, 101)\n",
      "y_resampled (59926,)\n",
      "Oliveira (Resampling): (59926, 102) \n",
      "\n",
      "Class Count:\n",
      "1    29963\n",
      "0    29963\n",
      "Name: malware, dtype: int64 \n",
      "\n",
      "Top 10 Most Repeated Samples:\n",
      "hash                              malware\n",
      "f78ea80cec007b2c32fb10f9c6c82f39  0          91\n",
      "bdaaac3fa3f6796825a51ef1c0e5b3fd  0          80\n",
      "03384ab6368b68ed16ecb9e6352539af  0          79\n",
      "0822ec2ba98d291e5bfc836bc3686096  0          66\n",
      "ec34862bd722c903bfc1f4788ee2ff72  0          63\n",
      "22cc743a96c926817719872e07c351cd  0          61\n",
      "34d0c34237b09bd1c650eedba9bc5fa0  0          60\n",
      "c6ac1a2aeb8c89a599fbc30924e5f743  0          60\n",
      "c0344331f17180acd42ef1a30a482f7f  0          59\n",
      "c11887896f7972ac8d467235ab1b655e  0          58\n",
      "5dc7d52aeb20e15bf064eadea48ba178  0          58\n",
      "f2e6e397c0a2a605c791c63466c9d5ae  0          57\n",
      "32d3f7a71fd8c1da168be1d79868546b  0          56\n",
      "6a7186b9550640068161ed3bb970a508  0          56\n",
      "b03995fa325d3e25ce582e052914cf51  0          56\n",
      "a83d4048ea61b5bcf61dced8fe05394e  0          55\n",
      "63ca0afe173b8e55b9c733a54d5c37b3  0          55\n",
      "3c9d38123129964c0a8415dbc62d10ff  0          55\n",
      "081f6f52f4254f500a8bb168c5a43cc8  0          54\n",
      "6f2e54ee762fadd665579b60268b478a  0          54\n",
      "dtype: int64 \n",
      "\n",
      "Unique Values: 30708\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>...</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>malware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38d19f81159e526a6d410af78f74ccf9</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>240</td>\n",
       "      <td>89</td>\n",
       "      <td>117</td>\n",
       "      <td>31</td>\n",
       "      <td>117</td>\n",
       "      <td>215</td>\n",
       "      <td>260</td>\n",
       "      <td>192</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a04c5d83384df33c48db5b21501aa336</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>25</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1ddbb2bac5055ed0bfad44b24d4442ae</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>25</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a6f3478cd06844ac9da84103b117a862</td>\n",
       "      <td>112</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>208</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8fc79157568b69638820ec31e25072ed</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59921</th>\n",
       "      <td>32dee5d6b7e38027723972192ceede88</td>\n",
       "      <td>82</td>\n",
       "      <td>16</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>65</td>\n",
       "      <td>215</td>\n",
       "      <td>260</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>65</td>\n",
       "      <td>215</td>\n",
       "      <td>65</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59922</th>\n",
       "      <td>64c2395ed30bb2e13af1bc2ad27809c0</td>\n",
       "      <td>82</td>\n",
       "      <td>16</td>\n",
       "      <td>172</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>235</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>235</td>\n",
       "      <td>...</td>\n",
       "      <td>297</td>\n",
       "      <td>93</td>\n",
       "      <td>10</td>\n",
       "      <td>199</td>\n",
       "      <td>93</td>\n",
       "      <td>10</td>\n",
       "      <td>264</td>\n",
       "      <td>215</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59923</th>\n",
       "      <td>849a4f51bd705d66e89777461a387bec</td>\n",
       "      <td>82</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>86</td>\n",
       "      <td>82</td>\n",
       "      <td>37</td>\n",
       "      <td>70</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>215</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>297</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59924</th>\n",
       "      <td>84088a0399f5d33d00295400d09a54f3</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>93</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>82</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>215</td>\n",
       "      <td>260</td>\n",
       "      <td>275</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>31</td>\n",
       "      <td>117</td>\n",
       "      <td>215</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59925</th>\n",
       "      <td>245f176fe695b3b117e54b39e81bde02</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>208</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>117</td>\n",
       "      <td>35</td>\n",
       "      <td>56</td>\n",
       "      <td>240</td>\n",
       "      <td>178</td>\n",
       "      <td>215</td>\n",
       "      <td>117</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59926 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   hash  t_0  t_1  t_2  t_3  t_4  t_5  t_6  \\\n",
       "0      38d19f81159e526a6d410af78f74ccf9   82  240  117  240  117  240  117   \n",
       "1      a04c5d83384df33c48db5b21501aa336  215  274  158  215  274  158  215   \n",
       "2      1ddbb2bac5055ed0bfad44b24d4442ae  215  274  158  215  274  158  215   \n",
       "3      a6f3478cd06844ac9da84103b117a862  112  274  158  215  274  158  215   \n",
       "4      8fc79157568b69638820ec31e25072ed  240  117  240  117  240  117  240   \n",
       "...                                 ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "59921  32dee5d6b7e38027723972192ceede88   82   16  240  117  260  141   65   \n",
       "59922  64c2395ed30bb2e13af1bc2ad27809c0   82   16  172  240  117  235  172   \n",
       "59923  849a4f51bd705d66e89777461a387bec   82  274  158  215   86   82   37   \n",
       "59924  84088a0399f5d33d00295400d09a54f3  208  187  208   93  208  172  117   \n",
       "59925  245f176fe695b3b117e54b39e81bde02   82  208  187  208  172  117  172   \n",
       "\n",
       "       t_7  t_8  ...  t_91  t_92  t_93  t_94  t_95  t_96  t_97  t_98  t_99  \\\n",
       "0      240  117  ...   240    89   117    31   117   215   260   192    89   \n",
       "1      172  117  ...    60    81    60    81   172   117    25   172   117   \n",
       "2      172  117  ...    60    81    60    81   172   117    25   172   117   \n",
       "3      298   76  ...    71   297   135   171   215    35   208    56    71   \n",
       "4      117  240  ...    60    81    60    81    60    81    60    81    60   \n",
       "...    ...  ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "59921  215  260  ...   117   240   117   260   141    65   215    65   260   \n",
       "59922  117  235  ...   297    93    10   199    93    10   264   215   208   \n",
       "59923   70   37  ...   215    82   240   117    82   240   117   297     8   \n",
       "59924   82   60  ...   215   260   275   240   117    31   117   215   260   \n",
       "59925  208   16  ...    35   117    35    56   240   178   215   117   208   \n",
       "\n",
       "       malware  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "...        ...  \n",
       "59921        0  \n",
       "59922        0  \n",
       "59923        0  \n",
       "59924        0  \n",
       "59925        0  \n",
       "\n",
       "[59926 rows x 102 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ros = RandomOverSampler(random_state=1, sampling_strategy='minority') #random_state is arbitrary; setting a value makes its output repeatable\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y) #Resampling of minority classes (i.e., Benign/0 for Oliveira)\n",
    "\n",
    "train_split = pd.concat([X_resampled,y_resampled.copy()], axis=1)\n",
    "print(\"X_resampled\", X_resampled.shape)\n",
    "print(\"y_resampled\", y_resampled.shape)\n",
    "print(\"Oliveira (Resampling):\", train_split.shape, \"\\n\")\n",
    "print(\"Class Count:\\n\" + str(train_split['malware'].value_counts()), \"\\n\")\n",
    "print(\"Top 10 Most Repeated Samples:\\n\" + str(train_split[['hash','malware']].value_counts()[0:20]),\"\\n\")\n",
    "print(\"Unique Values:\", len(train_split['hash'].unique()))\n",
    "train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca9ba7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>t_9</th>\n",
       "      <th>...</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>malware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>93</td>\n",
       "      <td>...</td>\n",
       "      <td>240</td>\n",
       "      <td>89</td>\n",
       "      <td>117</td>\n",
       "      <td>31</td>\n",
       "      <td>117</td>\n",
       "      <td>215</td>\n",
       "      <td>260</td>\n",
       "      <td>192</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>25</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>25</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>112</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>76</td>\n",
       "      <td>208</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>208</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59921</th>\n",
       "      <td>82</td>\n",
       "      <td>16</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>65</td>\n",
       "      <td>215</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>65</td>\n",
       "      <td>215</td>\n",
       "      <td>65</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59922</th>\n",
       "      <td>82</td>\n",
       "      <td>16</td>\n",
       "      <td>172</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>235</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>235</td>\n",
       "      <td>297</td>\n",
       "      <td>...</td>\n",
       "      <td>297</td>\n",
       "      <td>93</td>\n",
       "      <td>10</td>\n",
       "      <td>199</td>\n",
       "      <td>93</td>\n",
       "      <td>10</td>\n",
       "      <td>264</td>\n",
       "      <td>215</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59923</th>\n",
       "      <td>82</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>86</td>\n",
       "      <td>82</td>\n",
       "      <td>37</td>\n",
       "      <td>70</td>\n",
       "      <td>37</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>215</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>297</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59924</th>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>93</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>82</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>215</td>\n",
       "      <td>260</td>\n",
       "      <td>275</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>31</td>\n",
       "      <td>117</td>\n",
       "      <td>215</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59925</th>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>208</td>\n",
       "      <td>16</td>\n",
       "      <td>110</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>117</td>\n",
       "      <td>35</td>\n",
       "      <td>56</td>\n",
       "      <td>240</td>\n",
       "      <td>178</td>\n",
       "      <td>215</td>\n",
       "      <td>117</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59926 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       t_0  t_1  t_2  t_3  t_4  t_5  t_6  t_7  t_8  t_9  ...  t_91  t_92  \\\n",
       "0       82  240  117  240  117  240  117  240  117   93  ...   240    89   \n",
       "1      215  274  158  215  274  158  215  172  117  172  ...    60    81   \n",
       "2      215  274  158  215  274  158  215  172  117  172  ...    60    81   \n",
       "3      112  274  158  215  274  158  215  298   76  208  ...    71   297   \n",
       "4      240  117  240  117  240  117  240  117  240  117  ...    60    81   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "59921   82   16  240  117  260  141   65  215  260  141  ...   117   240   \n",
       "59922   82   16  172  240  117  235  172  117  235  297  ...   297    93   \n",
       "59923   82  274  158  215   86   82   37   70   37  240  ...   215    82   \n",
       "59924  208  187  208   93  208  172  117   82   60   81  ...   215   260   \n",
       "59925   82  208  187  208  172  117  172  208   16  110  ...    35   117   \n",
       "\n",
       "       t_93  t_94  t_95  t_96  t_97  t_98  t_99  malware  \n",
       "0       117    31   117   215   260   192    89        1  \n",
       "1        60    81   172   117    25   172   117        1  \n",
       "2        60    81   172   117    25   172   117        1  \n",
       "3       135   171   215    35   208    56    71        1  \n",
       "4        60    81    60    81    60    81    60        1  \n",
       "...     ...   ...   ...   ...   ...   ...   ...      ...  \n",
       "59921   117   260   141    65   215    65   260        0  \n",
       "59922    10   199    93    10   264   215   208        0  \n",
       "59923   240   117    82   240   117   297     8        0  \n",
       "59924   275   240   117    31   117   215   260        0  \n",
       "59925    35    56   240   178   215   117   208        0  \n",
       "\n",
       "[59926 rows x 101 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split.drop('hash', axis=1, inplace=True)\n",
    "X = train_split.iloc[:,0:100]\n",
    "y = train_split.iloc[:,100]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)\n",
    "train_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aef131",
   "metadata": {},
   "source": [
    "## 3.4. Training on Train_Split\n",
    "\n",
    "*A simple HGBT with K-Cross CV for testing and tuning (RandomizedSearchCV) will be executed for this example.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3353746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All hyperparameters are defaults except for random_state\n",
    "hgbt = HistGradientBoostingClassifier(loss='log_loss', learning_rate=0.1, max_iter=300, max_leaf_nodes=31, max_depth=None, \n",
    "            min_samples_leaf=20, l2_regularization=0.0, max_bins=255, categorical_features=None, \n",
    "            monotonic_cst=None, interaction_cst=None, warm_start=False, early_stopping='auto', scoring='loss', \n",
    "            validation_fraction=0.1, n_iter_no_change=10, tol=1e-07, verbose=0, random_state=1, class_weight=None)\n",
    "\n",
    "def kfolds(X,y,model):\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    sublist = [[\"accuracy\", \"f1_score\",\"precision\",\"recall\",\"roc-auc\",\"time\"]]\n",
    "    axis = 0\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X,y)):\n",
    "        print(\"Fold: \" + str(i), end=\"\") \n",
    "        start = time.time()\n",
    "        training_set = np.take(X, train_index, axis)\n",
    "        training_set_labels = np.take(y, train_index, axis)\n",
    "        test_set = np.take(X, test_index, axis)\n",
    "        test_set_labels = np.take(y, test_index, axis)\n",
    "        model.fit(training_set,training_set_labels)\n",
    "        m_pred = model.predict(test_set)\n",
    "        sublist.append([\n",
    "                        round(accuracy_score(test_set_labels, m_pred),4),\n",
    "                        round(f1_score(test_set_labels, m_pred, average='weighted'),4),\n",
    "                        round(precision_score(test_set_labels, m_pred,zero_division=0),4),\n",
    "                        round(recall_score(test_set_labels, m_pred),4),\n",
    "                        round(roc_auc_score(test_set_labels, m_pred),4),\n",
    "                        round(time.time()-start,4)\n",
    "                        ])\n",
    "        print(\" -\", time.time()-start,\"seconds\")\n",
    "    print(\"\")\n",
    "    return sublist\n",
    "\n",
    "def auto_tune(setup, model, X_train, y_train, X_test, worker=-1):\n",
    "    auto_tuner = RandomizedSearchCV(model, setup, refit=True, cv=3, verbose=0, n_jobs=worker, error_score=0, random_state=1)\n",
    "    auto_tuner.fit(X_train,y_train)\n",
    "    auto_tuner.predict(X_test)\n",
    "    return auto_tuner.best_params_\n",
    "    \n",
    "def model_test(X, y , X_train, X_test, y_train, y_test, model, model_label):\n",
    "    start = time.time()\n",
    "    print(model_label, end=\" - \")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"Completed! - {time.time()-start:.4f}s\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred),\"\\n\")\n",
    "    #K-Folds prior to tuning.\n",
    "    for s in kfolds(X,y,hgbt):\n",
    "        print(s)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84dd0461",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default HGBT - Completed! - 19.6802s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      8913\n",
      "           1       1.00      1.00      1.00      9065\n",
      "\n",
      "    accuracy                           1.00     17978\n",
      "   macro avg       1.00      1.00      1.00     17978\n",
      "weighted avg       1.00      1.00      1.00     17978\n",
      " \n",
      "\n",
      "Fold: 0 - 21.194347620010376 seconds\n",
      "Fold: 1 - 18.837021589279175 seconds\n",
      "Fold: 2 - 18.24234890937805 seconds\n",
      "Fold: 3 - 18.54241967201233 seconds\n",
      "Fold: 4 - 18.60320234298706 seconds\n",
      "\n",
      "['accuracy', 'f1_score', 'precision', 'recall', 'roc-auc', 'time']\n",
      "[0.9964, 0.9964, 0.9973, 0.9955, 0.9964, 21.1943]\n",
      "[0.9964, 0.9964, 0.9977, 0.9952, 0.9964, 18.837]\n",
      "[0.9975, 0.9975, 0.9985, 0.9965, 0.9975, 18.2423]\n",
      "[0.9959, 0.9959, 0.997, 0.9948, 0.9959, 18.5424]\n",
      "[0.9961, 0.9961, 0.9985, 0.9937, 0.9961, 18.6032]\n"
     ]
    }
   ],
   "source": [
    "#Default HGBT\n",
    "hgbt = HistGradientBoostingClassifier(loss='log_loss', learning_rate=0.1, max_iter=300, max_leaf_nodes=31, max_depth=None, \n",
    "                                      min_samples_leaf=20, l2_regularization=0.0, max_bins=255, categorical_features=None, \n",
    "                                      monotonic_cst=None, interaction_cst=None, warm_start=False, early_stopping='auto', scoring='loss', \n",
    "                                      validation_fraction=0.1, n_iter_no_change=10, tol=1e-07, verbose=0, random_state=1, class_weight=None)\n",
    "hgbt = model_test(X, y , X_train, X_test, y_train, y_test, hgbt, \"Default HGBT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca0d087e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 0.020 GB of training data: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ejose\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.140 s\n",
      "Binning 0.002 GB of validation data: 0.007 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/5000] 1 tree, 239 leaves, max depth = 19, train loss: 0.60456, val loss: 0.60593, in 0.379s\n",
      "[2/5000] 1 tree, 282 leaves, max depth = 20, train loss: 0.53159, val loss: 0.53422, in 0.408s\n",
      "[3/5000] 1 tree, 334 leaves, max depth = 22, train loss: 0.47032, val loss: 0.47461, in 0.490s\n",
      "[4/5000] 1 tree, 352 leaves, max depth = 29, train loss: 0.41879, val loss: 0.42403, in 0.491s\n",
      "[5/5000] 1 tree, 386 leaves, max depth = 30, train loss: 0.37423, val loss: 0.38036, in 0.580s\n",
      "[6/5000] 1 tree, 409 leaves, max depth = 27, train loss: 0.33519, val loss: 0.34214, in 0.566s\n",
      "[7/5000] 1 tree, 428 leaves, max depth = 32, train loss: 0.30122, val loss: 0.30859, in 0.620s\n",
      "[8/5000] 1 tree, 472 leaves, max depth = 39, train loss: 0.27147, val loss: 0.27935, in 0.652s\n",
      "[9/5000] 1 tree, 491 leaves, max depth = 46, train loss: 0.24539, val loss: 0.25347, in 0.685s\n",
      "[10/5000] 1 tree, 492 leaves, max depth = 47, train loss: 0.22205, val loss: 0.23040, in 0.653s\n",
      "[11/5000] 1 tree, 484 leaves, max depth = 52, train loss: 0.20147, val loss: 0.21004, in 0.749s\n",
      "[12/5000] 1 tree, 500 leaves, max depth = 47, train loss: 0.18294, val loss: 0.19178, in 0.810s\n",
      "[13/5000] 1 tree, 505 leaves, max depth = 56, train loss: 0.16636, val loss: 0.17579, in 0.741s\n",
      "[14/5000] 1 tree, 516 leaves, max depth = 66, train loss: 0.15157, val loss: 0.16120, in 0.703s\n",
      "[15/5000] 1 tree, 521 leaves, max depth = 57, train loss: 0.13812, val loss: 0.14815, in 0.728s\n",
      "[16/5000] 1 tree, 521 leaves, max depth = 57, train loss: 0.12610, val loss: 0.13639, in 0.724s\n",
      "[17/5000] 1 tree, 527 leaves, max depth = 65, train loss: 0.11510, val loss: 0.12572, in 0.801s\n",
      "[18/5000] 1 tree, 533 leaves, max depth = 65, train loss: 0.10527, val loss: 0.11601, in 0.732s\n",
      "[19/5000] 1 tree, 530 leaves, max depth = 59, train loss: 0.09656, val loss: 0.10729, in 0.771s\n",
      "[20/5000] 1 tree, 523 leaves, max depth = 64, train loss: 0.08856, val loss: 0.09916, in 0.710s\n",
      "[21/5000] 1 tree, 527 leaves, max depth = 55, train loss: 0.08125, val loss: 0.09180, in 0.753s\n",
      "[22/5000] 1 tree, 533 leaves, max depth = 62, train loss: 0.07486, val loss: 0.08548, in 0.775s\n",
      "[23/5000] 1 tree, 532 leaves, max depth = 60, train loss: 0.06882, val loss: 0.07933, in 0.842s\n",
      "[24/5000] 1 tree, 533 leaves, max depth = 65, train loss: 0.06346, val loss: 0.07400, in 0.714s\n",
      "[25/5000] 1 tree, 514 leaves, max depth = 63, train loss: 0.05853, val loss: 0.06895, in 0.713s\n",
      "[26/5000] 1 tree, 524 leaves, max depth = 61, train loss: 0.05400, val loss: 0.06433, in 0.784s\n",
      "[27/5000] 1 tree, 531 leaves, max depth = 56, train loss: 0.04993, val loss: 0.06017, in 0.825s\n",
      "[28/5000] 1 tree, 529 leaves, max depth = 57, train loss: 0.04623, val loss: 0.05641, in 0.747s\n",
      "[29/5000] 1 tree, 530 leaves, max depth = 59, train loss: 0.04292, val loss: 0.05303, in 0.776s\n",
      "[30/5000] 1 tree, 531 leaves, max depth = 60, train loss: 0.03989, val loss: 0.04994, in 0.785s\n",
      "[31/5000] 1 tree, 517 leaves, max depth = 57, train loss: 0.03711, val loss: 0.04712, in 0.901s\n",
      "[32/5000] 1 tree, 526 leaves, max depth = 58, train loss: 0.03464, val loss: 0.04475, in 0.885s\n",
      "[33/5000] 1 tree, 543 leaves, max depth = 59, train loss: 0.03230, val loss: 0.04237, in 0.808s\n",
      "[34/5000] 1 tree, 523 leaves, max depth = 56, train loss: 0.03014, val loss: 0.04014, in 0.773s\n",
      "[35/5000] 1 tree, 518 leaves, max depth = 58, train loss: 0.02818, val loss: 0.03815, in 0.788s\n",
      "[36/5000] 1 tree, 533 leaves, max depth = 59, train loss: 0.02640, val loss: 0.03636, in 0.726s\n",
      "[37/5000] 1 tree, 525 leaves, max depth = 56, train loss: 0.02475, val loss: 0.03480, in 0.794s\n",
      "[38/5000] 1 tree, 530 leaves, max depth = 53, train loss: 0.02327, val loss: 0.03325, in 0.775s\n",
      "[39/5000] 1 tree, 525 leaves, max depth = 52, train loss: 0.02190, val loss: 0.03199, in 0.858s\n",
      "[40/5000] 1 tree, 530 leaves, max depth = 52, train loss: 0.02066, val loss: 0.03073, in 0.783s\n",
      "[41/5000] 1 tree, 525 leaves, max depth = 56, train loss: 0.01954, val loss: 0.02955, in 0.787s\n",
      "[42/5000] 1 tree, 525 leaves, max depth = 55, train loss: 0.01852, val loss: 0.02860, in 0.968s\n",
      "[43/5000] 1 tree, 524 leaves, max depth = 60, train loss: 0.01757, val loss: 0.02768, in 0.954s\n",
      "[44/5000] 1 tree, 518 leaves, max depth = 44, train loss: 0.01673, val loss: 0.02685, in 0.766s\n",
      "[45/5000] 1 tree, 526 leaves, max depth = 46, train loss: 0.01593, val loss: 0.02604, in 0.783s\n",
      "[46/5000] 1 tree, 529 leaves, max depth = 43, train loss: 0.01522, val loss: 0.02525, in 0.744s\n",
      "[47/5000] 1 tree, 518 leaves, max depth = 45, train loss: 0.01456, val loss: 0.02469, in 0.746s\n",
      "[48/5000] 1 tree, 522 leaves, max depth = 44, train loss: 0.01396, val loss: 0.02415, in 0.717s\n",
      "[49/5000] 1 tree, 520 leaves, max depth = 45, train loss: 0.01340, val loss: 0.02356, in 0.740s\n",
      "[50/5000] 1 tree, 524 leaves, max depth = 42, train loss: 0.01290, val loss: 0.02307, in 0.733s\n",
      "[51/5000] 1 tree, 524 leaves, max depth = 42, train loss: 0.01244, val loss: 0.02257, in 0.759s\n",
      "[52/5000] 1 tree, 522 leaves, max depth = 43, train loss: 0.01202, val loss: 0.02205, in 0.710s\n",
      "[53/5000] 1 tree, 509 leaves, max depth = 43, train loss: 0.01164, val loss: 0.02173, in 0.721s\n",
      "[54/5000] 1 tree, 513 leaves, max depth = 44, train loss: 0.01129, val loss: 0.02147, in 0.700s\n",
      "[55/5000] 1 tree, 524 leaves, max depth = 36, train loss: 0.01097, val loss: 0.02131, in 0.761s\n",
      "[56/5000] 1 tree, 522 leaves, max depth = 41, train loss: 0.01068, val loss: 0.02102, in 0.693s\n",
      "[57/5000] 1 tree, 521 leaves, max depth = 39, train loss: 0.01041, val loss: 0.02088, in 0.741s\n",
      "[58/5000] 1 tree, 515 leaves, max depth = 41, train loss: 0.01017, val loss: 0.02059, in 0.707s\n",
      "[59/5000] 1 tree, 523 leaves, max depth = 37, train loss: 0.00995, val loss: 0.02050, in 0.809s\n",
      "[60/5000] 1 tree, 512 leaves, max depth = 39, train loss: 0.00974, val loss: 0.02016, in 0.777s\n",
      "[61/5000] 1 tree, 522 leaves, max depth = 39, train loss: 0.00955, val loss: 0.01992, in 0.738s\n",
      "[62/5000] 1 tree, 507 leaves, max depth = 38, train loss: 0.00938, val loss: 0.01970, in 0.684s\n",
      "[63/5000] 1 tree, 513 leaves, max depth = 39, train loss: 0.00922, val loss: 0.01950, in 0.731s\n",
      "[64/5000] 1 tree, 505 leaves, max depth = 39, train loss: 0.00908, val loss: 0.01939, in 0.696s\n",
      "[65/5000] 1 tree, 510 leaves, max depth = 42, train loss: 0.00895, val loss: 0.01929, in 0.753s\n",
      "[66/5000] 1 tree, 499 leaves, max depth = 42, train loss: 0.00883, val loss: 0.01921, in 0.708s\n",
      "[67/5000] 1 tree, 499 leaves, max depth = 45, train loss: 0.00872, val loss: 0.01897, in 0.732s\n",
      "[68/5000] 1 tree, 507 leaves, max depth = 57, train loss: 0.00862, val loss: 0.01884, in 0.760s\n",
      "[69/5000] 1 tree, 517 leaves, max depth = 48, train loss: 0.00853, val loss: 0.01885, in 0.806s\n",
      "[70/5000] 1 tree, 508 leaves, max depth = 48, train loss: 0.00845, val loss: 0.01871, in 0.739s\n",
      "[71/5000] 1 tree, 511 leaves, max depth = 51, train loss: 0.00837, val loss: 0.01870, in 0.730s\n",
      "[72/5000] 1 tree, 517 leaves, max depth = 49, train loss: 0.00830, val loss: 0.01882, in 0.696s\n",
      "[73/5000] 1 tree, 520 leaves, max depth = 66, train loss: 0.00824, val loss: 0.01880, in 0.756s\n",
      "[74/5000] 1 tree, 517 leaves, max depth = 61, train loss: 0.00818, val loss: 0.01880, in 0.708s\n",
      "[75/5000] 1 tree, 520 leaves, max depth = 62, train loss: 0.00812, val loss: 0.01883, in 0.757s\n",
      "[76/5000] 1 tree, 508 leaves, max depth = 57, train loss: 0.00807, val loss: 0.01879, in 0.730s\n",
      "[77/5000] 1 tree, 510 leaves, max depth = 54, train loss: 0.00803, val loss: 0.01881, in 0.717s\n",
      "[78/5000] 1 tree, 500 leaves, max depth = 63, train loss: 0.00799, val loss: 0.01874, in 0.703s\n",
      "[79/5000] 1 tree, 501 leaves, max depth = 52, train loss: 0.00795, val loss: 0.01852, in 0.694s\n",
      "[80/5000] 1 tree, 504 leaves, max depth = 59, train loss: 0.00792, val loss: 0.01859, in 0.672s\n",
      "[81/5000] 1 tree, 510 leaves, max depth = 65, train loss: 0.00788, val loss: 0.01859, in 0.739s\n",
      "[82/5000] 1 tree, 508 leaves, max depth = 51, train loss: 0.00785, val loss: 0.01858, in 0.678s\n",
      "[83/5000] 1 tree, 502 leaves, max depth = 53, train loss: 0.00783, val loss: 0.01870, in 0.726s\n",
      "[84/5000] 1 tree, 508 leaves, max depth = 57, train loss: 0.00780, val loss: 0.01872, in 0.681s\n",
      "[85/5000] 1 tree, 496 leaves, max depth = 58, train loss: 0.00778, val loss: 0.01863, in 0.716s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86/5000] 1 tree, 501 leaves, max depth = 61, train loss: 0.00776, val loss: 0.01872, in 0.775s\n",
      "[87/5000] 1 tree, 501 leaves, max depth = 49, train loss: 0.00774, val loss: 0.01873, in 0.725s\n",
      "[88/5000] 1 tree, 484 leaves, max depth = 58, train loss: 0.00773, val loss: 0.01880, in 0.671s\n",
      "[89/5000] 1 tree, 489 leaves, max depth = 46, train loss: 0.00771, val loss: 0.01879, in 0.719s\n",
      "Fit 89 trees in 65.279 s, (44709 total leaves)\n",
      "Time spent computing histograms: 46.084s\n",
      "Time spent finding best splits:  4.261s\n",
      "Time spent applying splits:      7.297s\n",
      "Time spent predicting:           0.051s\n",
      "Binning 0.020 GB of training data: 0.157 s\n",
      "Binning 0.002 GB of validation data: 0.013 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/5000] 1 tree, 246 leaves, max depth = 28, train loss: 0.60490, val loss: 0.60561, in 0.440s\n",
      "[2/5000] 1 tree, 269 leaves, max depth = 29, train loss: 0.53236, val loss: 0.53376, in 0.446s\n",
      "[3/5000] 1 tree, 307 leaves, max depth = 29, train loss: 0.47127, val loss: 0.47310, in 0.456s\n",
      "[4/5000] 1 tree, 345 leaves, max depth = 29, train loss: 0.41920, val loss: 0.42211, in 0.508s\n",
      "[5/5000] 1 tree, 394 leaves, max depth = 28, train loss: 0.37473, val loss: 0.37839, in 0.617s\n",
      "[6/5000] 1 tree, 413 leaves, max depth = 32, train loss: 0.33587, val loss: 0.34012, in 0.679s\n",
      "[7/5000] 1 tree, 434 leaves, max depth = 31, train loss: 0.30200, val loss: 0.30707, in 0.642s\n",
      "[8/5000] 1 tree, 442 leaves, max depth = 36, train loss: 0.27228, val loss: 0.27795, in 0.607s\n",
      "[9/5000] 1 tree, 461 leaves, max depth = 38, train loss: 0.24577, val loss: 0.25160, in 0.686s\n",
      "[10/5000] 1 tree, 485 leaves, max depth = 44, train loss: 0.22239, val loss: 0.22860, in 0.694s\n",
      "[11/5000] 1 tree, 475 leaves, max depth = 46, train loss: 0.20175, val loss: 0.20839, in 0.683s\n",
      "[12/5000] 1 tree, 478 leaves, max depth = 47, train loss: 0.18309, val loss: 0.18973, in 0.676s\n",
      "[13/5000] 1 tree, 495 leaves, max depth = 45, train loss: 0.16670, val loss: 0.17336, in 0.738s\n",
      "[14/5000] 1 tree, 499 leaves, max depth = 49, train loss: 0.15184, val loss: 0.15849, in 0.724s\n",
      "[15/5000] 1 tree, 493 leaves, max depth = 46, train loss: 0.13829, val loss: 0.14477, in 0.700s\n",
      "[16/5000] 1 tree, 490 leaves, max depth = 47, train loss: 0.12619, val loss: 0.13259, in 0.693s\n",
      "[17/5000] 1 tree, 520 leaves, max depth = 62, train loss: 0.11549, val loss: 0.12165, in 0.747s\n",
      "[18/5000] 1 tree, 519 leaves, max depth = 56, train loss: 0.10574, val loss: 0.11190, in 0.711s\n",
      "[19/5000] 1 tree, 505 leaves, max depth = 53, train loss: 0.09689, val loss: 0.10312, in 0.728s\n",
      "[20/5000] 1 tree, 514 leaves, max depth = 53, train loss: 0.08896, val loss: 0.09510, in 0.694s\n",
      "[21/5000] 1 tree, 521 leaves, max depth = 57, train loss: 0.08179, val loss: 0.08782, in 0.760s\n",
      "[22/5000] 1 tree, 531 leaves, max depth = 50, train loss: 0.07520, val loss: 0.08124, in 0.901s\n",
      "[23/5000] 1 tree, 536 leaves, max depth = 49, train loss: 0.06926, val loss: 0.07534, in 1.004s\n",
      "[24/5000] 1 tree, 554 leaves, max depth = 51, train loss: 0.06385, val loss: 0.07008, in 1.270s\n",
      "[25/5000] 1 tree, 528 leaves, max depth = 53, train loss: 0.05895, val loss: 0.06532, in 0.886s\n",
      "[26/5000] 1 tree, 529 leaves, max depth = 54, train loss: 0.05443, val loss: 0.06069, in 0.841s\n",
      "[27/5000] 1 tree, 533 leaves, max depth = 57, train loss: 0.05026, val loss: 0.05655, in 0.854s\n",
      "[28/5000] 1 tree, 523 leaves, max depth = 65, train loss: 0.04651, val loss: 0.05268, in 0.756s\n",
      "[29/5000] 1 tree, 542 leaves, max depth = 57, train loss: 0.04304, val loss: 0.04918, in 0.898s\n",
      "[30/5000] 1 tree, 525 leaves, max depth = 63, train loss: 0.03992, val loss: 0.04610, in 0.806s\n",
      "[31/5000] 1 tree, 535 leaves, max depth = 64, train loss: 0.03704, val loss: 0.04320, in 0.786s\n",
      "[32/5000] 1 tree, 531 leaves, max depth = 68, train loss: 0.03442, val loss: 0.04068, in 0.801s\n",
      "[33/5000] 1 tree, 529 leaves, max depth = 64, train loss: 0.03204, val loss: 0.03840, in 0.771s\n",
      "[34/5000] 1 tree, 529 leaves, max depth = 56, train loss: 0.02985, val loss: 0.03622, in 0.854s\n",
      "[35/5000] 1 tree, 523 leaves, max depth = 55, train loss: 0.02788, val loss: 0.03420, in 0.764s\n",
      "[36/5000] 1 tree, 521 leaves, max depth = 56, train loss: 0.02608, val loss: 0.03227, in 0.725s\n",
      "[37/5000] 1 tree, 525 leaves, max depth = 59, train loss: 0.02442, val loss: 0.03062, in 0.760s\n",
      "[38/5000] 1 tree, 527 leaves, max depth = 44, train loss: 0.02293, val loss: 0.02907, in 0.820s\n",
      "[39/5000] 1 tree, 523 leaves, max depth = 54, train loss: 0.02156, val loss: 0.02769, in 0.834s\n",
      "[40/5000] 1 tree, 529 leaves, max depth = 50, train loss: 0.02033, val loss: 0.02654, in 0.807s\n",
      "[41/5000] 1 tree, 529 leaves, max depth = 55, train loss: 0.01920, val loss: 0.02531, in 0.871s\n",
      "[42/5000] 1 tree, 516 leaves, max depth = 52, train loss: 0.01819, val loss: 0.02428, in 0.805s\n",
      "[43/5000] 1 tree, 511 leaves, max depth = 54, train loss: 0.01725, val loss: 0.02322, in 0.809s\n",
      "[44/5000] 1 tree, 515 leaves, max depth = 53, train loss: 0.01641, val loss: 0.02246, in 0.786s\n",
      "[45/5000] 1 tree, 524 leaves, max depth = 42, train loss: 0.01562, val loss: 0.02165, in 0.857s\n",
      "[46/5000] 1 tree, 511 leaves, max depth = 57, train loss: 0.01491, val loss: 0.02097, in 0.906s\n",
      "[47/5000] 1 tree, 525 leaves, max depth = 53, train loss: 0.01427, val loss: 0.02044, in 0.758s\n",
      "[48/5000] 1 tree, 525 leaves, max depth = 41, train loss: 0.01367, val loss: 0.01984, in 0.692s\n",
      "[49/5000] 1 tree, 516 leaves, max depth = 50, train loss: 0.01313, val loss: 0.01935, in 0.769s\n",
      "[50/5000] 1 tree, 512 leaves, max depth = 58, train loss: 0.01264, val loss: 0.01882, in 0.728s\n",
      "[51/5000] 1 tree, 518 leaves, max depth = 47, train loss: 0.01219, val loss: 0.01835, in 0.771s\n",
      "[52/5000] 1 tree, 513 leaves, max depth = 40, train loss: 0.01177, val loss: 0.01791, in 0.716s\n",
      "[53/5000] 1 tree, 522 leaves, max depth = 46, train loss: 0.01139, val loss: 0.01747, in 0.792s\n",
      "[54/5000] 1 tree, 519 leaves, max depth = 46, train loss: 0.01104, val loss: 0.01710, in 0.754s\n",
      "[55/5000] 1 tree, 502 leaves, max depth = 50, train loss: 0.01072, val loss: 0.01677, in 0.738s\n",
      "[56/5000] 1 tree, 508 leaves, max depth = 44, train loss: 0.01044, val loss: 0.01658, in 0.728s\n",
      "[57/5000] 1 tree, 505 leaves, max depth = 45, train loss: 0.01017, val loss: 0.01620, in 0.733s\n",
      "[58/5000] 1 tree, 505 leaves, max depth = 40, train loss: 0.00992, val loss: 0.01602, in 0.690s\n",
      "[59/5000] 1 tree, 526 leaves, max depth = 31, train loss: 0.00970, val loss: 0.01565, in 0.758s\n",
      "[60/5000] 1 tree, 535 leaves, max depth = 32, train loss: 0.00950, val loss: 0.01543, in 0.721s\n",
      "[61/5000] 1 tree, 523 leaves, max depth = 37, train loss: 0.00931, val loss: 0.01527, in 0.749s\n",
      "[62/5000] 1 tree, 522 leaves, max depth = 32, train loss: 0.00914, val loss: 0.01501, in 0.720s\n",
      "[63/5000] 1 tree, 519 leaves, max depth = 32, train loss: 0.00899, val loss: 0.01487, in 0.741s\n",
      "[64/5000] 1 tree, 516 leaves, max depth = 42, train loss: 0.00884, val loss: 0.01464, in 0.713s\n",
      "[65/5000] 1 tree, 514 leaves, max depth = 46, train loss: 0.00871, val loss: 0.01448, in 0.765s\n",
      "[66/5000] 1 tree, 519 leaves, max depth = 40, train loss: 0.00860, val loss: 0.01436, in 0.716s\n",
      "[67/5000] 1 tree, 520 leaves, max depth = 52, train loss: 0.00848, val loss: 0.01423, in 0.742s\n",
      "[68/5000] 1 tree, 527 leaves, max depth = 49, train loss: 0.00838, val loss: 0.01410, in 0.732s\n",
      "[69/5000] 1 tree, 517 leaves, max depth = 48, train loss: 0.00829, val loss: 0.01396, in 0.743s\n",
      "[70/5000] 1 tree, 522 leaves, max depth = 28, train loss: 0.00821, val loss: 0.01374, in 0.716s\n",
      "[71/5000] 1 tree, 517 leaves, max depth = 28, train loss: 0.00813, val loss: 0.01366, in 0.740s\n",
      "[72/5000] 1 tree, 506 leaves, max depth = 46, train loss: 0.00806, val loss: 0.01366, in 0.731s\n",
      "[73/5000] 1 tree, 504 leaves, max depth = 42, train loss: 0.00800, val loss: 0.01370, in 0.724s\n",
      "[74/5000] 1 tree, 518 leaves, max depth = 31, train loss: 0.00794, val loss: 0.01360, in 0.734s\n",
      "[75/5000] 1 tree, 514 leaves, max depth = 65, train loss: 0.00788, val loss: 0.01362, in 0.749s\n",
      "[76/5000] 1 tree, 509 leaves, max depth = 53, train loss: 0.00783, val loss: 0.01360, in 0.709s\n",
      "[77/5000] 1 tree, 517 leaves, max depth = 57, train loss: 0.00779, val loss: 0.01364, in 0.733s\n",
      "[78/5000] 1 tree, 511 leaves, max depth = 56, train loss: 0.00775, val loss: 0.01371, in 0.717s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79/5000] 1 tree, 505 leaves, max depth = 53, train loss: 0.00771, val loss: 0.01374, in 0.713s\n",
      "[80/5000] 1 tree, 508 leaves, max depth = 49, train loss: 0.00767, val loss: 0.01372, in 0.707s\n",
      "[81/5000] 1 tree, 501 leaves, max depth = 59, train loss: 0.00764, val loss: 0.01363, in 0.721s\n",
      "[82/5000] 1 tree, 504 leaves, max depth = 51, train loss: 0.00761, val loss: 0.01365, in 0.691s\n",
      "[83/5000] 1 tree, 504 leaves, max depth = 57, train loss: 0.00758, val loss: 0.01360, in 0.732s\n",
      "[84/5000] 1 tree, 499 leaves, max depth = 55, train loss: 0.00756, val loss: 0.01367, in 0.676s\n",
      "[85/5000] 1 tree, 501 leaves, max depth = 59, train loss: 0.00753, val loss: 0.01356, in 0.718s\n",
      "[86/5000] 1 tree, 505 leaves, max depth = 58, train loss: 0.00751, val loss: 0.01364, in 0.705s\n",
      "[87/5000] 1 tree, 502 leaves, max depth = 57, train loss: 0.00749, val loss: 0.01366, in 0.736s\n",
      "[88/5000] 1 tree, 503 leaves, max depth = 63, train loss: 0.00748, val loss: 0.01378, in 0.702s\n",
      "[89/5000] 1 tree, 499 leaves, max depth = 68, train loss: 0.00746, val loss: 0.01384, in 0.720s\n",
      "[90/5000] 1 tree, 496 leaves, max depth = 62, train loss: 0.00745, val loss: 0.01395, in 0.704s\n",
      "[91/5000] 1 tree, 489 leaves, max depth = 47, train loss: 0.00743, val loss: 0.01391, in 0.703s\n",
      "[92/5000] 1 tree, 490 leaves, max depth = 63, train loss: 0.00742, val loss: 0.01406, in 0.746s\n",
      "[93/5000] 1 tree, 488 leaves, max depth = 54, train loss: 0.00741, val loss: 0.01405, in 0.722s\n",
      "[94/5000] 1 tree, 493 leaves, max depth = 55, train loss: 0.00740, val loss: 0.01416, in 0.675s\n",
      "[95/5000] 1 tree, 483 leaves, max depth = 56, train loss: 0.00739, val loss: 0.01422, in 0.712s\n",
      "Fit 95 trees in 70.672 s, (47480 total leaves)\n",
      "Time spent computing histograms: 49.825s\n",
      "Time spent finding best splits:  4.913s\n",
      "Time spent applying splits:      7.711s\n",
      "Time spent predicting:           0.062s\n",
      "Binning 0.020 GB of training data: 0.152 s\n",
      "Binning 0.002 GB of validation data: 0.009 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/5000] 1 tree, 236 leaves, max depth = 19, train loss: 0.60509, val loss: 0.60534, in 0.403s\n",
      "[2/5000] 1 tree, 266 leaves, max depth = 23, train loss: 0.53230, val loss: 0.53357, in 0.427s\n",
      "[3/5000] 1 tree, 311 leaves, max depth = 23, train loss: 0.47196, val loss: 0.47417, in 0.515s\n",
      "[4/5000] 1 tree, 350 leaves, max depth = 26, train loss: 0.41983, val loss: 0.42224, in 0.545s\n",
      "[5/5000] 1 tree, 362 leaves, max depth = 28, train loss: 0.37491, val loss: 0.37771, in 0.589s\n",
      "[6/5000] 1 tree, 404 leaves, max depth = 38, train loss: 0.33628, val loss: 0.33968, in 0.604s\n",
      "[7/5000] 1 tree, 438 leaves, max depth = 41, train loss: 0.30236, val loss: 0.30609, in 0.680s\n",
      "[8/5000] 1 tree, 448 leaves, max depth = 39, train loss: 0.27251, val loss: 0.27659, in 0.628s\n",
      "[9/5000] 1 tree, 475 leaves, max depth = 50, train loss: 0.24624, val loss: 0.25039, in 0.717s\n",
      "[10/5000] 1 tree, 476 leaves, max depth = 55, train loss: 0.22259, val loss: 0.22720, in 0.675s\n",
      "[11/5000] 1 tree, 493 leaves, max depth = 60, train loss: 0.20166, val loss: 0.20670, in 0.748s\n",
      "[12/5000] 1 tree, 493 leaves, max depth = 59, train loss: 0.18314, val loss: 0.18819, in 0.722s\n",
      "[13/5000] 1 tree, 518 leaves, max depth = 61, train loss: 0.16648, val loss: 0.17145, in 0.886s\n",
      "[14/5000] 1 tree, 511 leaves, max depth = 65, train loss: 0.15143, val loss: 0.15667, in 0.803s\n",
      "[15/5000] 1 tree, 516 leaves, max depth = 61, train loss: 0.13822, val loss: 0.14369, in 0.832s\n",
      "[16/5000] 1 tree, 518 leaves, max depth = 61, train loss: 0.12641, val loss: 0.13191, in 0.791s\n",
      "[17/5000] 1 tree, 522 leaves, max depth = 60, train loss: 0.11562, val loss: 0.12135, in 0.756s\n",
      "[18/5000] 1 tree, 515 leaves, max depth = 60, train loss: 0.10589, val loss: 0.11169, in 0.714s\n",
      "[19/5000] 1 tree, 525 leaves, max depth = 55, train loss: 0.09708, val loss: 0.10317, in 0.778s\n",
      "[20/5000] 1 tree, 526 leaves, max depth = 60, train loss: 0.08908, val loss: 0.09547, in 0.710s\n",
      "[21/5000] 1 tree, 523 leaves, max depth = 61, train loss: 0.08180, val loss: 0.08858, in 0.761s\n",
      "[22/5000] 1 tree, 525 leaves, max depth = 59, train loss: 0.07523, val loss: 0.08188, in 0.704s\n",
      "[23/5000] 1 tree, 535 leaves, max depth = 66, train loss: 0.06926, val loss: 0.07593, in 0.785s\n",
      "[24/5000] 1 tree, 521 leaves, max depth = 62, train loss: 0.06383, val loss: 0.07042, in 0.702s\n",
      "[25/5000] 1 tree, 530 leaves, max depth = 64, train loss: 0.05890, val loss: 0.06563, in 0.758s\n",
      "[26/5000] 1 tree, 539 leaves, max depth = 57, train loss: 0.05443, val loss: 0.06119, in 0.745s\n",
      "[27/5000] 1 tree, 534 leaves, max depth = 55, train loss: 0.05029, val loss: 0.05703, in 0.869s\n",
      "[28/5000] 1 tree, 536 leaves, max depth = 55, train loss: 0.04654, val loss: 0.05327, in 0.754s\n",
      "[29/5000] 1 tree, 531 leaves, max depth = 63, train loss: 0.04311, val loss: 0.04979, in 0.779s\n",
      "[30/5000] 1 tree, 533 leaves, max depth = 60, train loss: 0.04003, val loss: 0.04657, in 0.737s\n",
      "[31/5000] 1 tree, 527 leaves, max depth = 59, train loss: 0.03714, val loss: 0.04351, in 0.777s\n",
      "[32/5000] 1 tree, 535 leaves, max depth = 59, train loss: 0.03454, val loss: 0.04081, in 0.725s\n",
      "[33/5000] 1 tree, 526 leaves, max depth = 57, train loss: 0.03213, val loss: 0.03832, in 0.773s\n",
      "[34/5000] 1 tree, 535 leaves, max depth = 63, train loss: 0.02993, val loss: 0.03607, in 0.763s\n",
      "[35/5000] 1 tree, 543 leaves, max depth = 62, train loss: 0.02795, val loss: 0.03400, in 0.786s\n",
      "[36/5000] 1 tree, 532 leaves, max depth = 61, train loss: 0.02616, val loss: 0.03214, in 0.733s\n",
      "[37/5000] 1 tree, 531 leaves, max depth = 60, train loss: 0.02450, val loss: 0.03051, in 0.787s\n",
      "[38/5000] 1 tree, 531 leaves, max depth = 59, train loss: 0.02298, val loss: 0.02903, in 0.752s\n",
      "[39/5000] 1 tree, 541 leaves, max depth = 60, train loss: 0.02162, val loss: 0.02751, in 0.809s\n",
      "[40/5000] 1 tree, 534 leaves, max depth = 63, train loss: 0.02040, val loss: 0.02623, in 0.750s\n",
      "[41/5000] 1 tree, 540 leaves, max depth = 63, train loss: 0.01927, val loss: 0.02496, in 0.784s\n",
      "[42/5000] 1 tree, 525 leaves, max depth = 53, train loss: 0.01822, val loss: 0.02387, in 0.728s\n",
      "[43/5000] 1 tree, 528 leaves, max depth = 60, train loss: 0.01726, val loss: 0.02283, in 0.788s\n",
      "[44/5000] 1 tree, 524 leaves, max depth = 51, train loss: 0.01640, val loss: 0.02200, in 0.751s\n",
      "[45/5000] 1 tree, 526 leaves, max depth = 55, train loss: 0.01561, val loss: 0.02115, in 0.770s\n",
      "[46/5000] 1 tree, 538 leaves, max depth = 68, train loss: 0.01488, val loss: 0.02035, in 0.737s\n",
      "[47/5000] 1 tree, 535 leaves, max depth = 36, train loss: 0.01421, val loss: 0.01964, in 0.791s\n",
      "[48/5000] 1 tree, 531 leaves, max depth = 37, train loss: 0.01360, val loss: 0.01898, in 0.748s\n",
      "[49/5000] 1 tree, 534 leaves, max depth = 38, train loss: 0.01305, val loss: 0.01841, in 0.781s\n",
      "[50/5000] 1 tree, 528 leaves, max depth = 43, train loss: 0.01253, val loss: 0.01786, in 0.736s\n",
      "[51/5000] 1 tree, 529 leaves, max depth = 47, train loss: 0.01206, val loss: 0.01736, in 0.805s\n",
      "[52/5000] 1 tree, 533 leaves, max depth = 47, train loss: 0.01163, val loss: 0.01696, in 0.838s\n",
      "[53/5000] 1 tree, 524 leaves, max depth = 48, train loss: 0.01123, val loss: 0.01648, in 0.791s\n",
      "[54/5000] 1 tree, 512 leaves, max depth = 48, train loss: 0.01088, val loss: 0.01599, in 0.730s\n",
      "[55/5000] 1 tree, 522 leaves, max depth = 47, train loss: 0.01055, val loss: 0.01563, in 0.799s\n",
      "[56/5000] 1 tree, 520 leaves, max depth = 46, train loss: 0.01026, val loss: 0.01533, in 0.722s\n",
      "[57/5000] 1 tree, 515 leaves, max depth = 45, train loss: 0.00998, val loss: 0.01503, in 0.760s\n",
      "[58/5000] 1 tree, 526 leaves, max depth = 45, train loss: 0.00973, val loss: 0.01477, in 0.720s\n",
      "[59/5000] 1 tree, 521 leaves, max depth = 35, train loss: 0.00950, val loss: 0.01451, in 0.783s\n",
      "[60/5000] 1 tree, 527 leaves, max depth = 42, train loss: 0.00930, val loss: 0.01428, in 0.752s\n",
      "[61/5000] 1 tree, 526 leaves, max depth = 52, train loss: 0.00911, val loss: 0.01408, in 0.802s\n",
      "[62/5000] 1 tree, 521 leaves, max depth = 40, train loss: 0.00893, val loss: 0.01391, in 0.735s\n",
      "[63/5000] 1 tree, 521 leaves, max depth = 38, train loss: 0.00877, val loss: 0.01360, in 0.774s\n",
      "[64/5000] 1 tree, 530 leaves, max depth = 46, train loss: 0.00863, val loss: 0.01340, in 0.751s\n",
      "[65/5000] 1 tree, 522 leaves, max depth = 47, train loss: 0.00849, val loss: 0.01333, in 0.866s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66/5000] 1 tree, 517 leaves, max depth = 43, train loss: 0.00837, val loss: 0.01328, in 0.706s\n",
      "[67/5000] 1 tree, 516 leaves, max depth = 40, train loss: 0.00825, val loss: 0.01310, in 0.740s\n",
      "[68/5000] 1 tree, 525 leaves, max depth = 50, train loss: 0.00815, val loss: 0.01309, in 0.735s\n",
      "[69/5000] 1 tree, 518 leaves, max depth = 68, train loss: 0.00806, val loss: 0.01301, in 0.757s\n",
      "[70/5000] 1 tree, 516 leaves, max depth = 52, train loss: 0.00797, val loss: 0.01295, in 0.738s\n",
      "[71/5000] 1 tree, 517 leaves, max depth = 53, train loss: 0.00789, val loss: 0.01280, in 0.744s\n",
      "[72/5000] 1 tree, 509 leaves, max depth = 57, train loss: 0.00782, val loss: 0.01277, in 0.737s\n",
      "[73/5000] 1 tree, 511 leaves, max depth = 54, train loss: 0.00775, val loss: 0.01266, in 0.825s\n",
      "[74/5000] 1 tree, 520 leaves, max depth = 56, train loss: 0.00769, val loss: 0.01257, in 0.855s\n",
      "[75/5000] 1 tree, 506 leaves, max depth = 52, train loss: 0.00764, val loss: 0.01246, in 0.833s\n",
      "[76/5000] 1 tree, 513 leaves, max depth = 61, train loss: 0.00759, val loss: 0.01237, in 0.743s\n",
      "[77/5000] 1 tree, 504 leaves, max depth = 65, train loss: 0.00754, val loss: 0.01243, in 0.744s\n",
      "[78/5000] 1 tree, 512 leaves, max depth = 54, train loss: 0.00750, val loss: 0.01248, in 0.708s\n",
      "[79/5000] 1 tree, 514 leaves, max depth = 59, train loss: 0.00746, val loss: 0.01235, in 0.723s\n",
      "[80/5000] 1 tree, 513 leaves, max depth = 61, train loss: 0.00742, val loss: 0.01225, in 0.720s\n",
      "[81/5000] 1 tree, 496 leaves, max depth = 66, train loss: 0.00739, val loss: 0.01228, in 0.761s\n",
      "[82/5000] 1 tree, 508 leaves, max depth = 69, train loss: 0.00736, val loss: 0.01233, in 0.731s\n",
      "[83/5000] 1 tree, 494 leaves, max depth = 62, train loss: 0.00733, val loss: 0.01228, in 0.716s\n",
      "[84/5000] 1 tree, 498 leaves, max depth = 55, train loss: 0.00731, val loss: 0.01240, in 0.692s\n",
      "[85/5000] 1 tree, 504 leaves, max depth = 57, train loss: 0.00729, val loss: 0.01243, in 0.736s\n",
      "[86/5000] 1 tree, 500 leaves, max depth = 62, train loss: 0.00727, val loss: 0.01240, in 0.708s\n",
      "[87/5000] 1 tree, 501 leaves, max depth = 64, train loss: 0.00725, val loss: 0.01228, in 0.744s\n",
      "[88/5000] 1 tree, 485 leaves, max depth = 56, train loss: 0.00723, val loss: 0.01223, in 0.710s\n",
      "[89/5000] 1 tree, 500 leaves, max depth = 58, train loss: 0.00721, val loss: 0.01207, in 0.719s\n",
      "[90/5000] 1 tree, 494 leaves, max depth = 55, train loss: 0.00720, val loss: 0.01201, in 0.758s\n",
      "[91/5000] 1 tree, 482 leaves, max depth = 59, train loss: 0.00718, val loss: 0.01189, in 0.767s\n",
      "[92/5000] 1 tree, 488 leaves, max depth = 63, train loss: 0.00717, val loss: 0.01181, in 0.714s\n",
      "[93/5000] 1 tree, 479 leaves, max depth = 56, train loss: 0.00716, val loss: 0.01176, in 0.738s\n",
      "[94/5000] 1 tree, 474 leaves, max depth = 64, train loss: 0.00715, val loss: 0.01179, in 0.693s\n",
      "[95/5000] 1 tree, 469 leaves, max depth = 63, train loss: 0.00714, val loss: 0.01176, in 0.722s\n",
      "[96/5000] 1 tree, 467 leaves, max depth = 58, train loss: 0.00713, val loss: 0.01170, in 0.770s\n",
      "[97/5000] 1 tree, 464 leaves, max depth = 60, train loss: 0.00712, val loss: 0.01170, in 0.800s\n",
      "[98/5000] 1 tree, 456 leaves, max depth = 62, train loss: 0.00712, val loss: 0.01174, in 0.777s\n",
      "[99/5000] 1 tree, 453 leaves, max depth = 55, train loss: 0.00711, val loss: 0.01183, in 0.753s\n",
      "[100/5000] 1 tree, 440 leaves, max depth = 71, train loss: 0.00710, val loss: 0.01186, in 0.739s\n",
      "[101/5000] 1 tree, 441 leaves, max depth = 63, train loss: 0.00710, val loss: 0.01197, in 0.718s\n",
      "[102/5000] 1 tree, 447 leaves, max depth = 58, train loss: 0.00709, val loss: 0.01202, in 0.652s\n",
      "[103/5000] 1 tree, 428 leaves, max depth = 62, train loss: 0.00709, val loss: 0.01208, in 0.675s\n",
      "[104/5000] 1 tree, 426 leaves, max depth = 71, train loss: 0.00708, val loss: 0.01210, in 0.661s\n",
      "[105/5000] 1 tree, 415 leaves, max depth = 64, train loss: 0.00708, val loss: 0.01212, in 0.644s\n",
      "[106/5000] 1 tree, 415 leaves, max depth = 64, train loss: 0.00707, val loss: 0.01223, in 0.662s\n",
      "Fit 106 trees in 78.350 s, (52638 total leaves)\n",
      "Time spent computing histograms: 56.662s\n",
      "Time spent finding best splits:  4.649s\n",
      "Time spent applying splits:      7.923s\n",
      "Time spent predicting:           0.064s\n",
      "Binning 0.030 GB of training data: 0.251 s\n",
      "Binning 0.003 GB of validation data: 0.013 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/5000] 1 tree, 292 leaves, max depth = 21, train loss: 0.60300, val loss: 0.60440, in 0.534s\n",
      "[2/5000] 1 tree, 337 leaves, max depth = 23, train loss: 0.52880, val loss: 0.53173, in 0.527s\n",
      "[3/5000] 1 tree, 416 leaves, max depth = 39, train loss: 0.46709, val loss: 0.47068, in 0.682s\n",
      "[4/5000] 1 tree, 454 leaves, max depth = 36, train loss: 0.41444, val loss: 0.41836, in 0.677s\n",
      "[5/5000] 1 tree, 490 leaves, max depth = 35, train loss: 0.36918, val loss: 0.37383, in 0.758s\n",
      "[6/5000] 1 tree, 544 leaves, max depth = 31, train loss: 0.32970, val loss: 0.33470, in 0.796s\n",
      "[7/5000] 1 tree, 573 leaves, max depth = 34, train loss: 0.29541, val loss: 0.30089, in 0.900s\n",
      "[8/5000] 1 tree, 595 leaves, max depth = 42, train loss: 0.26542, val loss: 0.27099, in 0.899s\n",
      "[9/5000] 1 tree, 601 leaves, max depth = 41, train loss: 0.23901, val loss: 0.24458, in 0.933s\n",
      "[10/5000] 1 tree, 606 leaves, max depth = 38, train loss: 0.21592, val loss: 0.22166, in 0.896s\n",
      "[11/5000] 1 tree, 612 leaves, max depth = 40, train loss: 0.19513, val loss: 0.20109, in 1.037s\n",
      "[12/5000] 1 tree, 624 leaves, max depth = 52, train loss: 0.17652, val loss: 0.18272, in 0.946s\n",
      "[13/5000] 1 tree, 638 leaves, max depth = 47, train loss: 0.16002, val loss: 0.16653, in 0.959s\n",
      "[14/5000] 1 tree, 645 leaves, max depth = 49, train loss: 0.14549, val loss: 0.15219, in 0.956s\n",
      "[15/5000] 1 tree, 647 leaves, max depth = 61, train loss: 0.13232, val loss: 0.13914, in 0.996s\n",
      "[16/5000] 1 tree, 659 leaves, max depth = 54, train loss: 0.12053, val loss: 0.12762, in 0.967s\n",
      "[17/5000] 1 tree, 664 leaves, max depth = 59, train loss: 0.10993, val loss: 0.11720, in 1.037s\n",
      "[18/5000] 1 tree, 662 leaves, max depth = 53, train loss: 0.10045, val loss: 0.10790, in 1.031s\n",
      "[19/5000] 1 tree, 666 leaves, max depth = 58, train loss: 0.09194, val loss: 0.09937, in 1.136s\n",
      "[20/5000] 1 tree, 675 leaves, max depth = 52, train loss: 0.08421, val loss: 0.09189, in 1.066s\n",
      "[21/5000] 1 tree, 682 leaves, max depth = 59, train loss: 0.07727, val loss: 0.08511, in 1.025s\n",
      "[22/5000] 1 tree, 667 leaves, max depth = 52, train loss: 0.07098, val loss: 0.07892, in 0.933s\n",
      "[23/5000] 1 tree, 678 leaves, max depth = 55, train loss: 0.06531, val loss: 0.07329, in 1.006s\n",
      "[24/5000] 1 tree, 685 leaves, max depth = 55, train loss: 0.06019, val loss: 0.06816, in 0.968s\n",
      "[25/5000] 1 tree, 686 leaves, max depth = 50, train loss: 0.05553, val loss: 0.06342, in 1.027s\n",
      "[26/5000] 1 tree, 682 leaves, max depth = 51, train loss: 0.05124, val loss: 0.05900, in 0.930s\n",
      "[27/5000] 1 tree, 692 leaves, max depth = 55, train loss: 0.04741, val loss: 0.05511, in 1.032s\n",
      "[28/5000] 1 tree, 694 leaves, max depth = 58, train loss: 0.04387, val loss: 0.05136, in 0.961s\n",
      "[29/5000] 1 tree, 689 leaves, max depth = 56, train loss: 0.04065, val loss: 0.04816, in 1.012s\n",
      "[30/5000] 1 tree, 684 leaves, max depth = 59, train loss: 0.03771, val loss: 0.04514, in 0.914s\n",
      "[31/5000] 1 tree, 686 leaves, max depth = 59, train loss: 0.03504, val loss: 0.04241, in 1.031s\n",
      "[32/5000] 1 tree, 678 leaves, max depth = 53, train loss: 0.03258, val loss: 0.03984, in 0.981s\n",
      "[33/5000] 1 tree, 688 leaves, max depth = 53, train loss: 0.03033, val loss: 0.03741, in 1.108s\n",
      "[34/5000] 1 tree, 683 leaves, max depth = 53, train loss: 0.02829, val loss: 0.03520, in 0.956s\n",
      "[35/5000] 1 tree, 687 leaves, max depth = 53, train loss: 0.02646, val loss: 0.03329, in 1.015s\n",
      "[36/5000] 1 tree, 688 leaves, max depth = 55, train loss: 0.02478, val loss: 0.03159, in 0.962s\n",
      "[37/5000] 1 tree, 699 leaves, max depth = 52, train loss: 0.02325, val loss: 0.03010, in 1.006s\n",
      "[38/5000] 1 tree, 679 leaves, max depth = 52, train loss: 0.02184, val loss: 0.02871, in 0.964s\n",
      "[39/5000] 1 tree, 696 leaves, max depth = 49, train loss: 0.02056, val loss: 0.02738, in 1.016s\n",
      "[40/5000] 1 tree, 701 leaves, max depth = 52, train loss: 0.01940, val loss: 0.02616, in 0.972s\n",
      "[41/5000] 1 tree, 697 leaves, max depth = 50, train loss: 0.01835, val loss: 0.02498, in 1.025s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42/5000] 1 tree, 691 leaves, max depth = 49, train loss: 0.01736, val loss: 0.02402, in 0.955s\n",
      "[43/5000] 1 tree, 680 leaves, max depth = 47, train loss: 0.01650, val loss: 0.02315, in 1.004s\n",
      "[44/5000] 1 tree, 686 leaves, max depth = 53, train loss: 0.01568, val loss: 0.02228, in 0.971s\n",
      "[45/5000] 1 tree, 690 leaves, max depth = 52, train loss: 0.01495, val loss: 0.02161, in 1.001s\n",
      "[46/5000] 1 tree, 693 leaves, max depth = 52, train loss: 0.01428, val loss: 0.02092, in 0.964s\n",
      "[47/5000] 1 tree, 697 leaves, max depth = 47, train loss: 0.01367, val loss: 0.02027, in 1.016s\n",
      "[48/5000] 1 tree, 681 leaves, max depth = 45, train loss: 0.01312, val loss: 0.01960, in 0.940s\n",
      "[49/5000] 1 tree, 701 leaves, max depth = 47, train loss: 0.01261, val loss: 0.01902, in 1.038s\n",
      "[50/5000] 1 tree, 687 leaves, max depth = 43, train loss: 0.01215, val loss: 0.01860, in 0.956s\n",
      "[51/5000] 1 tree, 686 leaves, max depth = 52, train loss: 0.01173, val loss: 0.01814, in 1.012s\n",
      "[52/5000] 1 tree, 689 leaves, max depth = 51, train loss: 0.01135, val loss: 0.01781, in 0.985s\n",
      "[53/5000] 1 tree, 684 leaves, max depth = 51, train loss: 0.01100, val loss: 0.01730, in 1.024s\n",
      "[54/5000] 1 tree, 689 leaves, max depth = 43, train loss: 0.01069, val loss: 0.01695, in 0.984s\n",
      "[55/5000] 1 tree, 685 leaves, max depth = 48, train loss: 0.01040, val loss: 0.01663, in 0.992s\n",
      "[56/5000] 1 tree, 702 leaves, max depth = 48, train loss: 0.01014, val loss: 0.01624, in 0.969s\n",
      "[57/5000] 1 tree, 688 leaves, max depth = 44, train loss: 0.00990, val loss: 0.01586, in 1.017s\n",
      "[58/5000] 1 tree, 688 leaves, max depth = 43, train loss: 0.00968, val loss: 0.01552, in 0.951s\n",
      "[59/5000] 1 tree, 685 leaves, max depth = 43, train loss: 0.00948, val loss: 0.01526, in 1.028s\n",
      "[60/5000] 1 tree, 693 leaves, max depth = 45, train loss: 0.00930, val loss: 0.01502, in 0.954s\n",
      "[61/5000] 1 tree, 686 leaves, max depth = 45, train loss: 0.00913, val loss: 0.01490, in 1.030s\n",
      "[62/5000] 1 tree, 701 leaves, max depth = 42, train loss: 0.00898, val loss: 0.01463, in 0.963s\n",
      "[63/5000] 1 tree, 697 leaves, max depth = 48, train loss: 0.00884, val loss: 0.01437, in 1.015s\n",
      "[64/5000] 1 tree, 693 leaves, max depth = 44, train loss: 0.00871, val loss: 0.01425, in 0.935s\n",
      "[65/5000] 1 tree, 696 leaves, max depth = 44, train loss: 0.00860, val loss: 0.01421, in 1.063s\n",
      "[66/5000] 1 tree, 688 leaves, max depth = 49, train loss: 0.00849, val loss: 0.01413, in 0.955s\n",
      "[67/5000] 1 tree, 693 leaves, max depth = 44, train loss: 0.00840, val loss: 0.01404, in 1.023s\n",
      "[68/5000] 1 tree, 698 leaves, max depth = 51, train loss: 0.00831, val loss: 0.01395, in 0.970s\n",
      "[69/5000] 1 tree, 695 leaves, max depth = 52, train loss: 0.00824, val loss: 0.01384, in 1.011s\n",
      "[70/5000] 1 tree, 689 leaves, max depth = 62, train loss: 0.00816, val loss: 0.01383, in 0.965s\n",
      "[71/5000] 1 tree, 689 leaves, max depth = 44, train loss: 0.00810, val loss: 0.01378, in 1.092s\n",
      "[72/5000] 1 tree, 690 leaves, max depth = 48, train loss: 0.00804, val loss: 0.01372, in 0.968s\n",
      "[73/5000] 1 tree, 684 leaves, max depth = 42, train loss: 0.00798, val loss: 0.01365, in 1.011s\n",
      "[74/5000] 1 tree, 688 leaves, max depth = 58, train loss: 0.00793, val loss: 0.01370, in 0.971s\n",
      "[75/5000] 1 tree, 688 leaves, max depth = 61, train loss: 0.00789, val loss: 0.01367, in 0.995s\n",
      "[76/5000] 1 tree, 688 leaves, max depth = 54, train loss: 0.00785, val loss: 0.01369, in 0.969s\n",
      "[77/5000] 1 tree, 674 leaves, max depth = 57, train loss: 0.00781, val loss: 0.01369, in 0.971s\n",
      "[78/5000] 1 tree, 680 leaves, max depth = 52, train loss: 0.00777, val loss: 0.01363, in 0.948s\n",
      "[79/5000] 1 tree, 680 leaves, max depth = 65, train loss: 0.00774, val loss: 0.01356, in 0.992s\n",
      "[80/5000] 1 tree, 676 leaves, max depth = 55, train loss: 0.00771, val loss: 0.01345, in 0.947s\n",
      "[81/5000] 1 tree, 674 leaves, max depth = 62, train loss: 0.00768, val loss: 0.01338, in 1.000s\n",
      "[82/5000] 1 tree, 669 leaves, max depth = 66, train loss: 0.00766, val loss: 0.01328, in 0.920s\n",
      "[83/5000] 1 tree, 676 leaves, max depth = 38, train loss: 0.00764, val loss: 0.01330, in 1.008s\n",
      "[84/5000] 1 tree, 671 leaves, max depth = 55, train loss: 0.00762, val loss: 0.01333, in 0.968s\n",
      "[85/5000] 1 tree, 670 leaves, max depth = 57, train loss: 0.00760, val loss: 0.01336, in 1.019s\n",
      "[86/5000] 1 tree, 675 leaves, max depth = 63, train loss: 0.00758, val loss: 0.01333, in 0.923s\n",
      "[87/5000] 1 tree, 670 leaves, max depth = 66, train loss: 0.00757, val loss: 0.01334, in 0.990s\n",
      "[88/5000] 1 tree, 669 leaves, max depth = 67, train loss: 0.00755, val loss: 0.01329, in 0.950s\n",
      "[89/5000] 1 tree, 663 leaves, max depth = 65, train loss: 0.00754, val loss: 0.01330, in 0.988s\n",
      "[90/5000] 1 tree, 667 leaves, max depth = 70, train loss: 0.00753, val loss: 0.01326, in 0.955s\n",
      "[91/5000] 1 tree, 665 leaves, max depth = 65, train loss: 0.00752, val loss: 0.01328, in 0.982s\n",
      "[92/5000] 1 tree, 674 leaves, max depth = 52, train loss: 0.00751, val loss: 0.01326, in 0.938s\n",
      "[93/5000] 1 tree, 650 leaves, max depth = 51, train loss: 0.00750, val loss: 0.01327, in 0.950s\n",
      "[94/5000] 1 tree, 661 leaves, max depth = 67, train loss: 0.00749, val loss: 0.01332, in 0.971s\n",
      "[95/5000] 1 tree, 644 leaves, max depth = 58, train loss: 0.00748, val loss: 0.01337, in 0.936s\n",
      "[96/5000] 1 tree, 639 leaves, max depth = 55, train loss: 0.00747, val loss: 0.01339, in 0.924s\n",
      "[97/5000] 1 tree, 624 leaves, max depth = 60, train loss: 0.00747, val loss: 0.01342, in 0.967s\n",
      "[98/5000] 1 tree, 618 leaves, max depth = 66, train loss: 0.00746, val loss: 0.01330, in 0.916s\n",
      "[99/5000] 1 tree, 587 leaves, max depth = 69, train loss: 0.00746, val loss: 0.01323, in 0.932s\n",
      "[100/5000] 1 tree, 592 leaves, max depth = 58, train loss: 0.00745, val loss: 0.01313, in 0.875s\n",
      "[101/5000] 1 tree, 595 leaves, max depth = 74, train loss: 0.00745, val loss: 0.01311, in 0.916s\n",
      "[102/5000] 1 tree, 558 leaves, max depth = 67, train loss: 0.00744, val loss: 0.01315, in 0.944s\n",
      "[103/5000] 1 tree, 559 leaves, max depth = 81, train loss: 0.00744, val loss: 0.01306, in 0.880s\n",
      "[104/5000] 1 tree, 543 leaves, max depth = 73, train loss: 0.00744, val loss: 0.01304, in 0.806s\n",
      "[105/5000] 1 tree, 526 leaves, max depth = 68, train loss: 0.00743, val loss: 0.01294, in 0.830s\n",
      "[106/5000] 1 tree, 520 leaves, max depth = 79, train loss: 0.00743, val loss: 0.01305, in 0.803s\n",
      "[107/5000] 1 tree, 487 leaves, max depth = 70, train loss: 0.00743, val loss: 0.01308, in 0.790s\n",
      "[108/5000] 1 tree, 485 leaves, max depth = 75, train loss: 0.00743, val loss: 0.01305, in 0.751s\n",
      "[109/5000] 1 tree, 463 leaves, max depth = 70, train loss: 0.00742, val loss: 0.01317, in 0.757s\n",
      "[110/5000] 1 tree, 433 leaves, max depth = 66, train loss: 0.00742, val loss: 0.01317, in 0.684s\n",
      "[111/5000] 1 tree, 403 leaves, max depth = 70, train loss: 0.00742, val loss: 0.01318, in 0.713s\n",
      "[112/5000] 1 tree, 386 leaves, max depth = 60, train loss: 0.00742, val loss: 0.01308, in 0.641s\n",
      "[113/5000] 1 tree, 368 leaves, max depth = 54, train loss: 0.00742, val loss: 0.01312, in 0.670s\n",
      "[114/5000] 1 tree, 356 leaves, max depth = 64, train loss: 0.00742, val loss: 0.01321, in 0.575s\n",
      "[115/5000] 1 tree, 345 leaves, max depth = 67, train loss: 0.00741, val loss: 0.01321, in 0.604s\n",
      "Fit 115 trees in 108.020 s, (72774 total leaves)\n",
      "Time spent computing histograms: 78.249s\n",
      "Time spent finding best splits:  6.584s\n",
      "Time spent applying splits:      10.803s\n",
      "Time spent predicting:           0.085s\n",
      "HGBT Auto Tune Best Params:\n",
      " {'warm_start': True, 'verbose': 1, 'validation_fraction': 0.1, 'tol': 1e-07, 'scoring': 'loss', 'random_state': 1, 'n_iter_no_change': 10, 'monotonic_cst': None, 'min_samples_leaf': 20, 'max_leaf_nodes': None, 'max_iter': 5000, 'max_depth': None, 'max_bins': 255, 'loss': 'log_loss', 'learning_rate': 0.1, 'l2_regularization': 0.0, 'interaction_cst': None, 'early_stopping': 'auto', 'class_weight': None, 'categorical_features': None}\n",
      "Elapsed time: 323.5381374359131\n"
     ]
    }
   ],
   "source": [
    "#RandomizedSearchCV Tuning\n",
    "#As it is only an example, the it will not really 'tune' per se as it will eat time.\n",
    "#However, it still shows how to execute/implement tuning using RandomizedSearchCV.\n",
    "params = [\n",
    "    {\n",
    "        'loss':['log_loss'],\n",
    "        'learning_rate':[0.1], #realistically, learning_rate of 1.0 is not ideal\n",
    "        'max_iter':[5000],\n",
    "        'max_leaf_nodes':[None], \n",
    "        'max_depth':[None], \n",
    "        'min_samples_leaf':[20], \n",
    "        'l2_regularization':[0.0], \n",
    "        'max_bins':[255], \n",
    "        'categorical_features':[None], \n",
    "        'monotonic_cst':[None], \n",
    "        'interaction_cst':[None], \n",
    "        'warm_start':[True], \n",
    "        'early_stopping':['auto'], \n",
    "        'scoring':['loss'], \n",
    "        'validation_fraction':[0.1], \n",
    "        'n_iter_no_change':[10], \n",
    "        'tol':[1e-07], \n",
    "        'verbose':[1], \n",
    "        'random_state':[1],\n",
    "        'class_weight':[None]\n",
    "    }\n",
    "]\n",
    "\n",
    "start = time.time()\n",
    "hgbt_t = auto_tune(params, HistGradientBoostingClassifier(), X_train, y_train, X_test, worker=1)\n",
    "print(\"HGBT Auto Tune Best Params:\\n\", hgbt_t)\n",
    "print(\"Elapsed time:\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f83c4e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned HGBT - Binning 0.030 GB of training data: 0.302 s\n",
      "Binning 0.003 GB of validation data: 0.015 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/5000] 1 tree, 292 leaves, max depth = 21, train loss: 0.60300, val loss: 0.60440, in 0.528s\n",
      "[2/5000] 1 tree, 337 leaves, max depth = 23, train loss: 0.52880, val loss: 0.53173, in 0.511s\n",
      "[3/5000] 1 tree, 416 leaves, max depth = 39, train loss: 0.46709, val loss: 0.47068, in 0.695s\n",
      "[4/5000] 1 tree, 454 leaves, max depth = 36, train loss: 0.41444, val loss: 0.41836, in 0.660s\n",
      "[5/5000] 1 tree, 490 leaves, max depth = 35, train loss: 0.36918, val loss: 0.37383, in 0.750s\n",
      "[6/5000] 1 tree, 544 leaves, max depth = 31, train loss: 0.32970, val loss: 0.33470, in 0.773s\n",
      "[7/5000] 1 tree, 573 leaves, max depth = 34, train loss: 0.29541, val loss: 0.30089, in 0.870s\n",
      "[8/5000] 1 tree, 595 leaves, max depth = 42, train loss: 0.26542, val loss: 0.27099, in 0.868s\n",
      "[9/5000] 1 tree, 601 leaves, max depth = 41, train loss: 0.23901, val loss: 0.24458, in 0.874s\n",
      "[10/5000] 1 tree, 606 leaves, max depth = 38, train loss: 0.21592, val loss: 0.22166, in 0.847s\n",
      "[11/5000] 1 tree, 612 leaves, max depth = 40, train loss: 0.19513, val loss: 0.20109, in 0.898s\n",
      "[12/5000] 1 tree, 624 leaves, max depth = 52, train loss: 0.17652, val loss: 0.18272, in 0.884s\n",
      "[13/5000] 1 tree, 638 leaves, max depth = 47, train loss: 0.16002, val loss: 0.16653, in 0.937s\n",
      "[14/5000] 1 tree, 645 leaves, max depth = 49, train loss: 0.14549, val loss: 0.15219, in 0.935s\n",
      "[15/5000] 1 tree, 647 leaves, max depth = 61, train loss: 0.13232, val loss: 0.13914, in 0.948s\n",
      "[16/5000] 1 tree, 659 leaves, max depth = 54, train loss: 0.12053, val loss: 0.12762, in 0.925s\n",
      "[17/5000] 1 tree, 664 leaves, max depth = 59, train loss: 0.10993, val loss: 0.11720, in 0.990s\n",
      "[18/5000] 1 tree, 662 leaves, max depth = 53, train loss: 0.10045, val loss: 0.10790, in 0.921s\n",
      "[19/5000] 1 tree, 666 leaves, max depth = 58, train loss: 0.09194, val loss: 0.09937, in 0.954s\n",
      "[20/5000] 1 tree, 675 leaves, max depth = 52, train loss: 0.08421, val loss: 0.09189, in 0.956s\n",
      "[21/5000] 1 tree, 682 leaves, max depth = 59, train loss: 0.07727, val loss: 0.08511, in 0.976s\n",
      "[22/5000] 1 tree, 667 leaves, max depth = 52, train loss: 0.07098, val loss: 0.07892, in 0.941s\n",
      "[23/5000] 1 tree, 678 leaves, max depth = 55, train loss: 0.06531, val loss: 0.07329, in 0.972s\n",
      "[24/5000] 1 tree, 685 leaves, max depth = 55, train loss: 0.06019, val loss: 0.06816, in 0.955s\n",
      "[25/5000] 1 tree, 686 leaves, max depth = 50, train loss: 0.05553, val loss: 0.06342, in 1.107s\n",
      "[26/5000] 1 tree, 682 leaves, max depth = 51, train loss: 0.05124, val loss: 0.05900, in 0.953s\n",
      "[27/5000] 1 tree, 692 leaves, max depth = 55, train loss: 0.04741, val loss: 0.05511, in 1.034s\n",
      "[28/5000] 1 tree, 694 leaves, max depth = 58, train loss: 0.04387, val loss: 0.05136, in 0.951s\n",
      "[29/5000] 1 tree, 689 leaves, max depth = 56, train loss: 0.04065, val loss: 0.04816, in 1.011s\n",
      "[30/5000] 1 tree, 684 leaves, max depth = 59, train loss: 0.03771, val loss: 0.04514, in 0.943s\n",
      "[31/5000] 1 tree, 686 leaves, max depth = 59, train loss: 0.03504, val loss: 0.04241, in 1.041s\n",
      "[32/5000] 1 tree, 678 leaves, max depth = 53, train loss: 0.03258, val loss: 0.03984, in 0.936s\n",
      "[33/5000] 1 tree, 688 leaves, max depth = 53, train loss: 0.03033, val loss: 0.03741, in 1.014s\n",
      "[34/5000] 1 tree, 683 leaves, max depth = 53, train loss: 0.02829, val loss: 0.03520, in 0.946s\n",
      "[35/5000] 1 tree, 687 leaves, max depth = 53, train loss: 0.02646, val loss: 0.03329, in 1.023s\n",
      "[36/5000] 1 tree, 688 leaves, max depth = 55, train loss: 0.02478, val loss: 0.03159, in 0.967s\n",
      "[37/5000] 1 tree, 699 leaves, max depth = 52, train loss: 0.02325, val loss: 0.03010, in 1.012s\n",
      "[38/5000] 1 tree, 679 leaves, max depth = 52, train loss: 0.02184, val loss: 0.02871, in 0.976s\n",
      "[39/5000] 1 tree, 696 leaves, max depth = 49, train loss: 0.02056, val loss: 0.02738, in 1.040s\n",
      "[40/5000] 1 tree, 701 leaves, max depth = 52, train loss: 0.01940, val loss: 0.02616, in 0.962s\n",
      "[41/5000] 1 tree, 697 leaves, max depth = 50, train loss: 0.01835, val loss: 0.02498, in 1.046s\n",
      "[42/5000] 1 tree, 691 leaves, max depth = 49, train loss: 0.01736, val loss: 0.02402, in 1.019s\n",
      "[43/5000] 1 tree, 680 leaves, max depth = 47, train loss: 0.01650, val loss: 0.02315, in 0.994s\n",
      "[44/5000] 1 tree, 686 leaves, max depth = 53, train loss: 0.01568, val loss: 0.02228, in 0.959s\n",
      "[45/5000] 1 tree, 690 leaves, max depth = 52, train loss: 0.01495, val loss: 0.02161, in 1.098s\n",
      "[46/5000] 1 tree, 693 leaves, max depth = 52, train loss: 0.01428, val loss: 0.02092, in 0.983s\n",
      "[47/5000] 1 tree, 697 leaves, max depth = 47, train loss: 0.01367, val loss: 0.02027, in 1.019s\n",
      "[48/5000] 1 tree, 681 leaves, max depth = 45, train loss: 0.01312, val loss: 0.01960, in 0.941s\n",
      "[49/5000] 1 tree, 701 leaves, max depth = 47, train loss: 0.01261, val loss: 0.01902, in 1.025s\n",
      "[50/5000] 1 tree, 687 leaves, max depth = 43, train loss: 0.01215, val loss: 0.01860, in 0.967s\n",
      "[51/5000] 1 tree, 686 leaves, max depth = 52, train loss: 0.01173, val loss: 0.01814, in 1.000s\n",
      "[52/5000] 1 tree, 689 leaves, max depth = 51, train loss: 0.01135, val loss: 0.01781, in 0.967s\n",
      "[53/5000] 1 tree, 684 leaves, max depth = 51, train loss: 0.01100, val loss: 0.01730, in 0.997s\n",
      "[54/5000] 1 tree, 689 leaves, max depth = 43, train loss: 0.01069, val loss: 0.01695, in 0.973s\n",
      "[55/5000] 1 tree, 685 leaves, max depth = 48, train loss: 0.01040, val loss: 0.01663, in 0.984s\n",
      "[56/5000] 1 tree, 702 leaves, max depth = 48, train loss: 0.01014, val loss: 0.01624, in 0.986s\n",
      "[57/5000] 1 tree, 688 leaves, max depth = 44, train loss: 0.00990, val loss: 0.01586, in 1.011s\n",
      "[58/5000] 1 tree, 688 leaves, max depth = 43, train loss: 0.00968, val loss: 0.01552, in 1.024s\n",
      "[59/5000] 1 tree, 685 leaves, max depth = 43, train loss: 0.00948, val loss: 0.01526, in 1.040s\n",
      "[60/5000] 1 tree, 693 leaves, max depth = 45, train loss: 0.00930, val loss: 0.01502, in 0.969s\n",
      "[61/5000] 1 tree, 686 leaves, max depth = 45, train loss: 0.00913, val loss: 0.01490, in 1.013s\n",
      "[62/5000] 1 tree, 701 leaves, max depth = 42, train loss: 0.00898, val loss: 0.01463, in 0.962s\n",
      "[63/5000] 1 tree, 697 leaves, max depth = 48, train loss: 0.00884, val loss: 0.01437, in 1.011s\n",
      "[64/5000] 1 tree, 693 leaves, max depth = 44, train loss: 0.00871, val loss: 0.01425, in 0.951s\n",
      "[65/5000] 1 tree, 696 leaves, max depth = 44, train loss: 0.00860, val loss: 0.01421, in 1.176s\n",
      "[66/5000] 1 tree, 688 leaves, max depth = 49, train loss: 0.00849, val loss: 0.01413, in 1.208s\n",
      "[67/5000] 1 tree, 693 leaves, max depth = 44, train loss: 0.00840, val loss: 0.01404, in 1.133s\n",
      "[68/5000] 1 tree, 698 leaves, max depth = 51, train loss: 0.00831, val loss: 0.01395, in 1.126s\n",
      "[69/5000] 1 tree, 695 leaves, max depth = 52, train loss: 0.00824, val loss: 0.01384, in 1.238s\n",
      "[70/5000] 1 tree, 689 leaves, max depth = 62, train loss: 0.00816, val loss: 0.01383, in 1.004s\n",
      "[71/5000] 1 tree, 689 leaves, max depth = 44, train loss: 0.00810, val loss: 0.01378, in 1.055s\n",
      "[72/5000] 1 tree, 690 leaves, max depth = 48, train loss: 0.00804, val loss: 0.01372, in 0.967s\n",
      "[73/5000] 1 tree, 684 leaves, max depth = 42, train loss: 0.00798, val loss: 0.01365, in 1.143s\n",
      "[74/5000] 1 tree, 688 leaves, max depth = 58, train loss: 0.00793, val loss: 0.01370, in 1.125s\n",
      "[75/5000] 1 tree, 688 leaves, max depth = 61, train loss: 0.00789, val loss: 0.01367, in 1.053s\n",
      "[76/5000] 1 tree, 688 leaves, max depth = 54, train loss: 0.00785, val loss: 0.01369, in 0.983s\n",
      "[77/5000] 1 tree, 674 leaves, max depth = 57, train loss: 0.00781, val loss: 0.01369, in 0.998s\n",
      "[78/5000] 1 tree, 680 leaves, max depth = 52, train loss: 0.00777, val loss: 0.01363, in 0.946s\n",
      "[79/5000] 1 tree, 680 leaves, max depth = 65, train loss: 0.00774, val loss: 0.01356, in 1.072s\n",
      "[80/5000] 1 tree, 676 leaves, max depth = 55, train loss: 0.00771, val loss: 0.01345, in 0.967s\n",
      "[81/5000] 1 tree, 674 leaves, max depth = 62, train loss: 0.00768, val loss: 0.01338, in 1.000s\n",
      "[82/5000] 1 tree, 669 leaves, max depth = 66, train loss: 0.00766, val loss: 0.01328, in 1.139s\n",
      "[83/5000] 1 tree, 676 leaves, max depth = 38, train loss: 0.00764, val loss: 0.01330, in 1.144s\n",
      "[84/5000] 1 tree, 671 leaves, max depth = 55, train loss: 0.00762, val loss: 0.01333, in 1.050s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85/5000] 1 tree, 670 leaves, max depth = 57, train loss: 0.00760, val loss: 0.01336, in 1.518s\n",
      "[86/5000] 1 tree, 675 leaves, max depth = 63, train loss: 0.00758, val loss: 0.01333, in 0.972s\n",
      "[87/5000] 1 tree, 670 leaves, max depth = 66, train loss: 0.00757, val loss: 0.01334, in 0.983s\n",
      "[88/5000] 1 tree, 669 leaves, max depth = 67, train loss: 0.00755, val loss: 0.01329, in 0.970s\n",
      "[89/5000] 1 tree, 663 leaves, max depth = 65, train loss: 0.00754, val loss: 0.01330, in 0.993s\n",
      "[90/5000] 1 tree, 667 leaves, max depth = 70, train loss: 0.00753, val loss: 0.01326, in 0.935s\n",
      "[91/5000] 1 tree, 665 leaves, max depth = 65, train loss: 0.00752, val loss: 0.01328, in 0.986s\n",
      "[92/5000] 1 tree, 674 leaves, max depth = 52, train loss: 0.00751, val loss: 0.01326, in 0.927s\n",
      "[93/5000] 1 tree, 650 leaves, max depth = 51, train loss: 0.00750, val loss: 0.01327, in 0.967s\n",
      "[94/5000] 1 tree, 661 leaves, max depth = 67, train loss: 0.00749, val loss: 0.01332, in 0.950s\n",
      "[95/5000] 1 tree, 644 leaves, max depth = 58, train loss: 0.00748, val loss: 0.01337, in 0.952s\n",
      "[96/5000] 1 tree, 639 leaves, max depth = 55, train loss: 0.00747, val loss: 0.01339, in 0.910s\n",
      "[97/5000] 1 tree, 624 leaves, max depth = 60, train loss: 0.00747, val loss: 0.01342, in 0.960s\n",
      "[98/5000] 1 tree, 618 leaves, max depth = 66, train loss: 0.00746, val loss: 0.01330, in 0.890s\n",
      "[99/5000] 1 tree, 587 leaves, max depth = 69, train loss: 0.00746, val loss: 0.01323, in 0.927s\n",
      "[100/5000] 1 tree, 592 leaves, max depth = 58, train loss: 0.00745, val loss: 0.01313, in 0.883s\n",
      "[101/5000] 1 tree, 595 leaves, max depth = 74, train loss: 0.00745, val loss: 0.01311, in 0.936s\n",
      "[102/5000] 1 tree, 558 leaves, max depth = 67, train loss: 0.00744, val loss: 0.01315, in 0.850s\n",
      "[103/5000] 1 tree, 559 leaves, max depth = 81, train loss: 0.00744, val loss: 0.01306, in 0.889s\n",
      "[104/5000] 1 tree, 543 leaves, max depth = 73, train loss: 0.00744, val loss: 0.01304, in 0.830s\n",
      "[105/5000] 1 tree, 526 leaves, max depth = 68, train loss: 0.00743, val loss: 0.01294, in 0.865s\n",
      "[106/5000] 1 tree, 520 leaves, max depth = 79, train loss: 0.00743, val loss: 0.01305, in 0.827s\n",
      "[107/5000] 1 tree, 487 leaves, max depth = 70, train loss: 0.00743, val loss: 0.01308, in 0.819s\n",
      "[108/5000] 1 tree, 485 leaves, max depth = 75, train loss: 0.00743, val loss: 0.01305, in 0.929s\n",
      "[109/5000] 1 tree, 463 leaves, max depth = 70, train loss: 0.00742, val loss: 0.01317, in 0.919s\n",
      "[110/5000] 1 tree, 433 leaves, max depth = 66, train loss: 0.00742, val loss: 0.01317, in 0.778s\n",
      "[111/5000] 1 tree, 403 leaves, max depth = 70, train loss: 0.00742, val loss: 0.01318, in 0.811s\n",
      "[112/5000] 1 tree, 386 leaves, max depth = 60, train loss: 0.00742, val loss: 0.01308, in 0.815s\n",
      "[113/5000] 1 tree, 368 leaves, max depth = 54, train loss: 0.00742, val loss: 0.01312, in 0.735s\n",
      "[114/5000] 1 tree, 356 leaves, max depth = 64, train loss: 0.00742, val loss: 0.01321, in 0.652s\n",
      "[115/5000] 1 tree, 345 leaves, max depth = 67, train loss: 0.00741, val loss: 0.01321, in 0.662s\n",
      "Fit 115 trees in 110.456 s, (72774 total leaves)\n",
      "Time spent computing histograms: 79.272s\n",
      "Time spent finding best splits:  7.041s\n",
      "Time spent applying splits:      11.466s\n",
      "Time spent predicting:           0.089s\n",
      "Completed! - 110.4749s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      8913\n",
      "           1       1.00      1.00      1.00      9065\n",
      "\n",
      "    accuracy                           1.00     17978\n",
      "   macro avg       1.00      1.00      1.00     17978\n",
      "weighted avg       1.00      1.00      1.00     17978\n",
      " \n",
      "\n",
      "Fold: 0 - 19.130717754364014 seconds\n",
      "Fold: 1 - 17.779451608657837 seconds\n",
      "Fold: 2 - 18.75146198272705 seconds\n",
      "Fold: 3 - 19.3904287815094 seconds\n",
      "Fold: 4 - 19.16198706626892 seconds\n",
      "\n",
      "['accuracy', 'f1_score', 'precision', 'recall', 'roc-auc', 'time']\n",
      "[0.9964, 0.9964, 0.9973, 0.9955, 0.9964, 19.1307]\n",
      "[0.9964, 0.9964, 0.9977, 0.9952, 0.9964, 17.7795]\n",
      "[0.9975, 0.9975, 0.9985, 0.9965, 0.9975, 18.7515]\n",
      "[0.9959, 0.9959, 0.997, 0.9948, 0.9959, 19.3904]\n",
      "[0.9961, 0.9961, 0.9985, 0.9937, 0.9961, 19.162]\n"
     ]
    }
   ],
   "source": [
    "#Tuned HGBT\n",
    "hgbt_t = HistGradientBoostingClassifier(**hgbt_t)\n",
    "\n",
    "hgbt_t = model_test(X, y , X_train, X_test, y_train, y_test, hgbt_t, \"Tuned HGBT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edaa669",
   "metadata": {},
   "source": [
    "## 3.5 Model Robustness Test\n",
    "\n",
    "*Using the recently tuned and trained model as the model and the test_split as the input*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cc02afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_split value_counts:\n",
      "1    12834\n",
      "0      329\n",
      "Name: malware, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"test_split value_counts:\")\n",
    "print(test_split['malware'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f490468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGBT Default Model Robustness Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.71      0.74       329\n",
      "           1       0.99      1.00      0.99     12834\n",
      "\n",
      "    accuracy                           0.99     13163\n",
      "   macro avg       0.89      0.85      0.87     13163\n",
      "weighted avg       0.99      0.99      0.99     13163\n",
      " \n",
      "\n",
      "HGBT Tuned Model Robustness Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.71      0.74       329\n",
      "           1       0.99      1.00      0.99     12834\n",
      "\n",
      "    accuracy                           0.99     13163\n",
      "   macro avg       0.89      0.85      0.87     13163\n",
      "weighted avg       0.99      0.99      0.99     13163\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#These two are effectively X_test and y_test\n",
    "X = test_split.iloc[:,1:101]\n",
    "y = test_split.iloc[:,101]\n",
    "\n",
    "y_pred = hgbt_t.predict(X)\n",
    "print(\"HGBT Default Model Robustness Test\")\n",
    "print(classification_report(y, y_pred),\"\\n\")\n",
    "\n",
    "y_pred = hgbt_t.predict(X)\n",
    "print(\"HGBT Tuned Model Robustness Test\")\n",
    "print(classification_report(y, y_pred),\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
